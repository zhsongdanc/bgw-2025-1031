***\*1.Java集合包源码\****

***\*2.Thread源码分析\****

***\*3.volatile关键字的原理\****

***\*4.Java内存模型JMM\****

***\*5.JMM如何处理并发中的原子性可见性有序性\****

***\*6.volatile如何保证可见性\****

***\*7.volatile的原理(Lock前缀指令 + 内存屏障)\****

***\*8.双重检查单\*******\*例模式的volatile优化\****

***\*9.synchronized关键字的原理\****

***\*10.wait()与notify()的底层原理\****

***\*11.Atomic原子类中的CAS无锁化原理\****

***\*12.LongAdder的分段CAS优化多线程自旋\****

***\*
\****

精通JDK集合和JUC并发包，研究过相关集合源码、锁原理、AQS源码、线程池源码等，深入阅读过Disruptor源码，对高并发的处理有一定的理解。



***\*1.Java集合包源码\****

***\*(1)ArrayList源码总结\****

如果不会频繁插入元素，导致频繁的移动元素位置、List扩容，而主要用于遍历集合或通过索引随机读取元素，那么可以用ArrayList。



如果会频繁插入元素到List中，那么尽量还是不要用ArrayList，因为很可能会造成大量的元素移动 + 数组扩容 + 元素拷贝。



remove()和add(index, element)方法都会导致数组的拷贝(System.arraycopy())，因此性能都不是太高。所以基于ArrayList来进行随机位置的插入和删除，性能不会太高。



add()和add(index, element)方法都可能会导致数组需要扩容(ensureCapacityInternal())。由于数组长度是固定的，默认初始大小是10。如果往数组里添加数据，可能会导致数组不停扩容，影响性能。



set()和get()方法可以基于数组实现随机位置的直接定位，性能很高。



ArrayList每次添加元素都判断是否需要扩容，进行数组扩容时，会先扩容一半，然后再进行数组拷贝。

**
**

***\**\*使用数组进行性能优化的案例：\*\**\***在NioEventLoop的openSelector()方法中，Netty会通过反射对Selector底层的数据结构进行优化(Hash Set => 数组)。FastThreadLocal在定位数据时可以直接根据数组下标index进行定位，时间复杂度为O(1)，所以在数据较多时也不会存在Hash冲突。在进行数组扩容时只需要把数组容量扩容2倍，然后再把原数据拷贝到新数组。ThreadLocal在定位数据时是根据哈希算法进行定位的，在数据较多时容易发生Hash冲突。发生Hash冲突时采用线性探测法解决Hash冲突要不断向下寻找，效率较低。在进行数组扩容时由于采用了哈希算法，所以在数组扩容后需要再做一轮rehash。***\*
\****

**
**

***\*(2)LinkedList源码总结\****

LinkedList，底层是基于链表来实现的。LinkedList的优点就是非常适合频繁插入各种元素。LinkedList的缺点就是不太适合获取某个随机位置的元素。ArrayList和LinkedList区别就是数组和链表的区别。***\*
\****

**
**

***\**\*ArrayList的使用场景：\*\**\***一般会使用ArrayList来代表一个集合。只要别频繁插入大量元素即可，遍历或者随机查都可以。***\*
\****

**
**

***\**\*LinkedList的使用场景：\*\**\***适合频繁在list中插入和删除元素，LinkedList可以当队列来使用。如果要在内存中实现一个内存队列，那么可以使用LinkedList。

**
**

***\**\*LinkedList获取元素的原理：\*\**\***对于ArrayList而言，如果要获取某个随机位置的元素，则其get(int index)方法会直接通过数组的index定位到该元素，性能超高。对LinkedList而言，如果要获取某个随机位置的元素，则其get(int index)方法需调用node(index)这个方法，进行链表的遍历。也就是会比较index和size >> 1(链表元素一半)的大小，如果在前半部分就从头开始遍历，如果在后半部分就从尾开始遍历。

**
**

***\**\*LinkedList删除元素的原理：\*\**\***如果往LinkedList里插入大量数据(队头、队尾、队列中间)，由于基于链表实现，不会出现大量的元素移动，也不会出现数组扩容。但在中间插入元素的性能没有在队头和队尾插入元素的性能好，因为需要遍历到指定的位置，然后才能完成元素的插入。所以基于链表的LinkedList很适合做队列，缺点是随机位置获取一个元素会导致遍历。如果元素很多，遍历的性能可能比较差。***\*
\****

**
**

***\*(3)ArrayList和LinkedList的区别\****

一般的回答：ArrayList是数组实现的，LinkedList是链表实现的。深入的回答：结合ArrayList的源码，介绍add、remove、get、set方法的实现原理。介绍ArrayList数组扩容、元素移动的原理、以及优缺点是什么。LinkedList则是基于双向链表实现的，可以介绍一下它的数据结构。介绍LinkedList一些常见操作的原理、node是怎么变化的、以及优缺点，以及在哪个项目哪个业务场景下用过ArrayList和LinkedList。***\*
\****

**
**

***\*(4)栈的数据结构总结\****

栈是由Vector和Stack这两个类来实现的。Stack代表了栈这种数据结构，它是继承自Vector的。Vector是一种类似ArrayList的数据结构(基于数组实现)，是有序的集合。Stack是一种基于数组来实现的栈数据结构。栈是先进后出，队列是先进先出。栈的使用一般通过push()和pop()，将元素压入栈底和从栈顶弹出元素。



栈的push()方法会将元素压入栈底。Stack.push()方法和ArrayList.add()方法的的实现源码几乎是一样的：就是放在数组按顺序排列的位置上。ArrayList默认每次扩容成原来的1.5倍，Vector默认每次扩容成原来的2倍。Vector将元素压入栈底时，elementData[elementCount++] = element。



Stack.pop()方法和Stack.peek()方法会从栈顶弹出一个元素。也就是最后一个压入栈的元素，会通过Stack.pop()方法从栈顶弹出。首先使用elementData[size - 1]获取最后一个元素，返回给用户。然后通过removeElementAt(size - 1)方法删除最后一个元素。***\*
\****

**
**

***\*(5)HashMap的源码之数组 + 链表 + 红黑树\****

***\**\*HashMap设置元素和获取元素的流程：\*\**\***如果要对一个HashMap执行map.put(1, "张三")，map.put(2, "李四")。首先对一个key进行hashCode()运算，获取该key的哈希值。然后常规的做法是用这个哈希值对数组的长度进行取模。接着根据取模的结果，将key-value对放在数组中的某个元素上。如果要对一个HashMap执行map.get(1)，同理会先根据key获取哈希值。然后根据哈希值对数组长度取模，这样就知道key对应的value在哪里。***\*
\****

**
**

***\**\*HashMap设置元素时出现哈希冲突的处理：\*\**\***在JDK 1.8以前，使用的是数组 + 链表来进行处理。如果出现大量哈希冲突，那么遍历长链表寻找key-value对时的复杂度是O(n)。在JDK 1.8以后，使用的是数组 + 链表 + 红黑树来进行处理。如果链表长度超过8，会自动将链表转换为红黑树，那么查找key-value对时的复杂度是O(logn)。



HashMap的数组的默认初始大小是16，ArrayList的数组的默认初始大小是10。HashMap的默认负载因子0.75，如果数组里的元素个数达到了数组大小(16) * 负载因子(0.75)，也就是数组元素达到12个时就会进行数组扩容。HashMap的扩容阈值 = 数组容量 * 负载因子。如果size达到threshold，那么HashMap就会进行数组扩容。



Node是HashMap的内部类，它代表了一个key-value对。一个Node结点里包含了：key的哈希值、key、value、一个next指针。这个next指针会指向下一个Node，也就是单向链表中的下一个结点。通过这个next指针就可以形成一个链表，来解决哈希冲突。Node[]数组就是HashMap里的核心数据结构的数组。数组的元素是Node类型的，天然就可以挂成一个单向链表，因为Node里面会有一个next指针。

**
**

***\**\*HashMap降低哈希冲突概率的Hash算法：\*\**\***HashMap里的Hash算法是经过优化的、高性能的。HashMap的hash()方法会对key执行具体的Hash算法来获取一个Hash值。首先通过key.hashCode()获取key的HashCode，然后通过h >>> 16对HashCode进行右移16位，也就是把32位的二进制数字的所有bit往右侧移动16位，最后将右移16位的结果和HashCode进行异或运算。实际上就是将32位的key.hashCode()的高16位和低16位进行异或运算，这样可以降低哈希冲突的概率。



为什么要将32位的key.hashCode()的高16位和低16位进行异或运算？因为首先HashMap的数组的默认初始大小是16，然后在get()方法会用这个异或运算的结果值定位数组的index时，默认情况下就会将数组大小16和异或运算的结果值进行位与运算。当然随着数组扩容，之后可能用32和异或运算的结果值进行位与运算。当使用数组大小16和异或运算的结果值进行位与运算时：由于HashCode是32位，所以运算结果最多只能利用HashCode低16位。因此为了尽量利用到HashCode的32位，降低哈希冲突的概率，才对HashCode进行高低16位的异或处理。否则如果直接使用HashCode的低16位进行位与运算，则冲突更多。而当使用扩容后的数组大小32和异或运算的结果值进行位与运算时，即使不对HashCode进行高低16位异或，也能利用到32位的HashCode。



在HashMap的hash()方法里，把HashCode的高低16位进行异或运算，可保证异或运算结果同时保留HashCode的高16位和低16位的特征。于是在get()方法中通过位运算定位数组index时，即使只有低16位参与运算，HashCode的高低16位特征也参与到运算中。相比于直接使用HashCode的低16位去定位数组index，能减少哈希冲突。



HashMap的构造方法只是对HashMap的负载因子和扩容阈值进行赋值，而具体的HashMap的数组初始化则是在执行put()方法时处理的。假设一开始是通过HashMap的无参构造函数创建一个HashMap对象，然后第一次执行该HashMap对象的put()方法时HashMap的数组为空。于是在执行"tab = resize()"代码来对HashMap数组的初始化时，会初始化数组大小为默认16，负载因子为默认0.75，扩容阈值为12。也就是会初始化一个大小为16的、元素为Node的数组。接着执行"hash & (n - 1)"代码，其中n是16，所以变成"15 & hash"，转成10进制就是3。所以哈希寻址算法并不是直接用hash值对数组大小取模来实现的。因为取模的操作性能不高，而位运算的性能很高，一般会通过位与操作来实现取模的效果。***\*
\****

**
**

***\**\*JDK1.8对HashMap的一个优化：\*\**\***数组刚开始的初始值以及未来每次扩容的值，都是2的n次方。只要保证数组大小是2的n次方，就可以保证："(数组大小 - 1) & hash"与"hash % 数组大小"的结果一样。注意：a对2的n次方取模等价于a和2的n次方-1的结果进行位与。通过hash & (n - 1)，就能将任意一个hash值定位到数组的某个index里。直接取模的性能相对较低，所以这是HashMap提升性能的一个优化点，这也是HashMap底层原理里的重要部分。***\*
\****

**
**

***\**\*哈希冲突时的链表处理：\*\**\***两个key的hash值不同，但通过哈希寻址算法定位到数组的同一个index。此时就会出现典型的哈希冲突，默认情况下会使用单向链表来处理。其中的分支代码"if ((p = tab[i = (n - 1) & hash]) == null)"的意思是：如果通过哈希寻址算法定位到的下标为i的数组元素为空(即tab[i]为空)，那么就可以直接将一个新创建的Node对象放到数组的tab[i]这个位置。如果通过哈希寻址算法定位到的数组位置已有Node元素，那么会判断是否为相同的key，如果是相同的key则进行value覆盖。如果不是相同的key，那么通过"p instanceof TreeNode)"，判断数组的tab[i]元素是否是一颗红黑树。如果通过哈希寻址算法定位到的数组位置已有Node元素，且判断出不是相同的key，且数组的tab[i]元素也不是一颗红黑树，那么则说明数组的tab[i]元素是一个链表，于是通过"p.next = newNode()"这行代码将新元素串入到链表尾部。最后判断当前链表的长度是否已经大于等于8。如果是，则通过调用treeifyBin()方法将这个链表转换成一个红黑树。***\*
\****

**
**

***\**\*JDK 1.8对HashMap的一个优化：\*\**\***如果链表的长度达到8，那么就会将链表转换为红黑树。如果对红黑树进行get()操作，那么时间复杂度会变成O(logn)。红黑树查找的O(logn)远比链表查找的O(n)低，性能得到大幅提升。***\*
\****

**
**

***\**\*HashMap扩容的原理非常简单：\*\**\***2倍扩容 + rehash。每个key-value对，都会基于key的hash值重新寻址找到新数组的新位置。原来数组的长度是16，现在新数组的长度是32。原来所有的key的hash对16取模是一个位置，比如index = 5。但是如果对32取模，可能就是index = 11，位置可能变化。JDK 1.8为提升rehash性能，不再使用key的hash值对新数组大小取模。而使用位与操作实现哈希寻址，因为直接取模的性能比较低。***\*
\****

**
**

***\*(6)HashMap的底层原理总结\****

***\*说明一：哈希算法\****

为什么要对key的HashCode进行高低位的异或运算？因为可以降低数组大小为16时的哈希冲突的概率。(h = key.hashCode()) ^ (h >>> 16);

**
**

***\*说明二：哈希寻址\****

为什么是hash值和数组.length - 1进行与运算？因为位与的性能比取模要高：tab[(n - 1) & hash];

**
**

***\*说明三：哈希冲突处理\****

首先将元素存到单向链表中，当单向链表的元素超8个时，再将单向链表转双向链表再转红黑树。

**
**

***\*说明四：数组扩容机制\****

每次按原数组大小2倍扩容，并按hash & (n - 1)进行重新寻址(rehash)。重新寻址时，会判断hash & (n - 1)的二进制结果是否多出一个bit的1。如果是，那么重新寻址后的位置就是index + oldCap。如果否，那么重新寻址后的位置还是原来的index。通过这个方式，避免rehash时使用低效的每个hash值对新数组大小进行取模。***\*
\****

**
**

***\*(7)迭代器应对多线程并发修改的Fail-Fast机制\****

***\**\*Java集合在迭代时的Fail-Fast机制：\*\**\***也就是一个线程在迭代遍历集合，另一个线程在修改集合时：迭代的线程会快速报异常：ConcurrentModificationException。这个异常就是并发修改的异常，这种机制就叫做Fail-Fast机制。***\*
\****

**
**

***\**\*通过modCount实现Java集合在迭代时的Fail-Fast机制：\*\**\***modCount就是用来实现Fail-Fast机制的，各个集合里其实都有这个modCount的概念。只要这个集合被修改了，那么就会对modCount++。modCount就是修改次数之意，只要修改一次集合，那么就会更新modCount。这些修改集合的方法有：add()、remove()、set()等。



Java集合包下的类都是非线程安全的，都设计了针对并发修改集合的处理，也就是用modCount来实现Fail-Fast机制。比如在迭代一个ArrayList前，已经插入4个元素，此时modCount = 4。获取和初始化一个迭代器时，其expectedModCount会设为modCount，迭代器每次迭代时都会比较expectedModCount和modCount是否相等。如果不相等，抛出并发修改冲突异常ConcurrentModificationException。比如HashMap在迭代开始时，会将modCount赋值给mc。迭代完成后，会判断mc是否等于modCount。如果不相等，抛出并发修改冲突异常ConcurrentModificationException。

**
**

***\*2.Thread源码分析\****

***\*(1)线程的运行状态\****

New(新建状态)：调用new Thread()时的状态。Runnable(运行状态)：调用start()方法启动线程后的状态。Blocked(阻塞状态)：线程执行synchronized代码但未抢到锁的状态。Waiting(等待状态)：调用Object.wait()等方法时的状态。Timed_Waiting(等待超时状态)：调用sleep(timeout)时的状态。Terminated(终止状态)：线程的run()方法执行完后的状态。***\*
\****

**
**

***\*(2)如何减少线程上下文切换\****

方法一：减少线程数

方法二：采用无锁设计解决线程竞争问题

方法三：采用CAS做自旋操作***\*
\****

**
**

***\*(3)创建和启动一个线程的主要方法\****

继承Thread类、实现Runnable接口、使用ExecutorService线程池、使用Callable/Future实现带有返回值的多线程。***\*
\****

**
**

***\*(4)以daemon模式运行微服务的存活监控线程\****

***\**\*非daemon线程(工作线程)：\*\**\***一般来说，工作线程就是非daemon线程，后台线程是daemon线程。默认创建的线程就是非daemon的，称之为工作线程。如果main()方法启动后要开启几个无限循环工作的线程来处理一些请求，比如类似于Web服务器的，那么这些无限循环工作的线程就是工作线程。main方法执行完后，因工作线程一直在运行，所以JVM进程不会退出。***\*
\****

**
**

***\**\*daemon线程：\*\**\***daemon线程指的是，当工作线程都停止时，而自动退出的线程。比如main线程都执行完了，那么daemon线程会跟着JVM进程一起退出。daemon线程不会像工作线程一样阻止JVM进程退出。***\*
\****

**
**

***\**\*设置监控微服务存活的线程为daemon线程：\*\**\***比如当微服务注册中心负责接收请求的核心工作线程，由于某些原因停止后，那么这个微服务注册中心必须停止。但如果监控微服务存活状态的线程一直在while循环中运行着，那么会导致微服务注册中心无法停止，因为JVM进程没法结束。所以针对这种情况，一般会将后台运行的线程设置为daemon线程。如果JVM只剩daemon线程，就会销毁所有daemon线程，并退出JVM进程。所以应该将监控微服务存活状态的线程设置为daemon线程，这样如果微服务注册中心的工作线程挂了，监控存活状态的线程也能被销毁。***\*
\****

**
**

***\*(5)Thread线程初始化要点总结\****

一.由父线程来创建子线程

二.线程的ThreadGroup默认与父线程的相同

三.线程的daemon状态默认与父线程的相同

四.线程的优先级默认是父线程的优先级

五.线程的名称默认是Thread-0格式的名称

六.线程的ID是全局递增的，从1开始

七.线程的上下文类加载器默认与父线程的相同***\*
\****

**
**

***\*(6)Thread线程的启动源码\****

注意：不能对一个线程多次调用和执行start()方法。因为一个线程一旦被执行，那么它的threadStatus属性就一定会变成非0值。如果多次执行一个线程的start()方法，会抛出IllegalThreadStateException。



Thread对象的start0()方法会结合底层的代码机制来启动一个线程。底层执行Thread.start0()方法时，会执行Thread.run()或Runnable.run()方法。如果创建的Thread对象重写了run()方法，那么就会执行被重写的run()方法，否则就会执行Thread对象的run()方法。如果创建Thread对象时传入了Runnable对象，且没有重写Thread.run()方法，那么在执行Thread的run()方法时，就执行传入的Runnable对象的run()方法。***\*
\****

**
**

***\**\*Thread线程的启动总结：\*\**\***不能多次调用start()方法，因为启动后threadStatus就会变成非0值。启动线程后，这个线程才会加入线程初始化时指定的线程组中。启动一个线程实际上是通过start0()这个native方法来启动的，线程启动后会执行被重写的run()方法或传入的Runnable的run()方法。***\*
\****

**
**

***\*(7)yield()方法可切换当前线程执行其他线程\****

如果担心某个线程一直长时间霸占CPU，导致其他线程很少得到机会来执行。此时可以使用Thread.yield()方法让当前线程先别执行，让其他线程来先执行。在Disruptor框架中就使用了Thread.yield()方法实现切换线程自旋的等待策略。***\*
\****

**
**

***\*(8)join()方法实现服务注册线程的阻塞式运行\****

***\**\*没有使用join()时：\*\**\***在main线程里开启了其他线程，那么main线程会和其他线程并发运行。也就是一会儿执行main线程的代码，一会儿执行其他线程的代码。

**
**

***\**\*使用join()时：\*\**\***在main线程里开启一个线程A，main线程如果执行了线程A的join()方法，那么就会导致main线程被阻塞，main线程会等待线程A执行完毕才会继续。

**
**

***\*(9)Thread的interrupt()方法\****

Thread.interrupt()方法会修改线程底层的interrupt标志位为true，Thread.isInterrupted()方法可以获取这个interrupt标志位的值。



所以可以通过Thread.isInterrupted()方法来判断是否要继续运行线程，而不是执行Thread.interrupt()方法，直接就会中断线程不让线程运行。具体就是在while循环的条件中通过isInterrupted()方法获取interrupt标志位，如果线程没有被中断，那么interrupt标志位就为false，while循环正常执行。如果线程被中断，那么isInterrupted()就会返回true，于是就终止while循环。



如果线程正在被wait()方法、join()方法、sleep()方法、以及IO操作阻塞着，那么执行该线程的interrupt()方法会抛出InterruptedException等异常。



Thread.interrupt()方法的一个常见用法就是打断一个线程的休眠。***\*
\****

**
**

***\*(10)interrupt()方法实现优雅关闭心跳线程\****

在一个分布式系统里，通常会有核心的工作线程，并会设计一个shutdown()方法关闭这个系统。在这个shutdown()方法会设置工作线程是否需要运行的标志位为false，设置完标志位为false之后，会对工作线程都执行其interrupt()方法。



由于工作线程可能会在不断地运行while循环，而每循环一次会进入休眠状态。所以如果想尽快停止系统，那么可用interrupt()方法打断工作线程的休眠，同时通过判断工作线程需要运行的标志位是否为false来立即退出线程。只有所有的工作线程都结束了，JVM进程才会自动退出。比如在微服务注册中心的register-client中新增一个isRunning标志位，通过isRunning标志位来判断服务实例是否还需要运行来终止心跳线程。***\*
\****

**
**

***\*3.volatile关键字的原理\****

***\*(1)volatile关键字的使用场景\****

如果多个线程共用一个共享变量，有的线程写、有的线程读，那么可能会导致有的线程没法及时读到其他线程修改的变量值。volatile关键字可让某线程修改变量后，其他线程立即看到该变量的修改值。***\*
\****

**
**

***\*(2)volatile关键字的理解路径\****

一.CPU缓存模型

二.Java内存模型JMM

三.原子性、可见性、有序性

四.volatile的作用

五.volatile的底层原理

六.volatile案例***\*
\****

**
**

***\*(3)主内存和CPU的缓存模型\****

CPU如果频繁读写主内存，那么就会导致CPU的计算性能比较差。所以现代的计算机，一般都会在CPU和内存之间加几层高速缓存。这样每个CPU就可以直接操作自己对应的高速缓存，从而不需要直接和主内存进行频繁的通信，保证了CPU的计算效率。

**
**

***\*(4)CPU高速缓存的数据不一致问题\****

主内存的数据会被加载到CPU高速缓存里，CPU后续会读写自己的高速缓存。但多线程并发运行时，就可能引发各个CPU高速缓存里的数据不一致问题。



上面volatile的例子，在没有volatile修饰flag的时候：负责执行线程0的CPU一开始会将主内存的flag值读到其高速缓存。之后在执行线程0的指令时，便会在该CPU的高速缓存里读取flag的值。此时该CPU无法感知负责执行线程1的其他CPU对其高速缓存的flag的修改。因为负责执行线程1的CPU可能没有将其修改的缓存值及时刷新回到主内存，或者负责执行线程0的CPU可能没有主动更新主内存的最新值到其高速缓存中。



所以CPU的缓存模型，在多线程并发运行时可能存在数据不一致的问题。也就是各个CPU的高速缓存和主内存没有及时同步，同一个数据在各CPU可能都不一样，从而导致数据的不一致。***\*
\****

**
**

***\*(5)总线锁和缓存锁机制\****

***\**\*什么是总线\*\**\******\**\*：\*\**\***所谓总线，就是CPU与内存和输入输出设备传输信息的公共通道。当CPU和内存进行数据交互时，必须经过总线来传输。

**
**

***\**\*总线锁：\*\**\***所谓总线锁，就是如果某个CPU要修改主内存的某数据，那么就往总线发出一个Lock[#信号](javascript:;)。这个信号能够确保主内存只有该CPU可以访问，其他CPU的请求会被阻塞。这就使得同一时刻只有一个CPU能够访问主内存，从而解决缓存不一致问题。所以总线锁可以理解为：当一个CPU往总线发出一个Lock[#信号](javascript:;)时，其他CPU的的请求将会被阻塞，于是该CPU就能独占主内存(共享内存)了。总线锁把CPU和内存之间的通信锁住了，从而使得锁定期间，其他CPU不能操作其他内存地址的数据。所以总线锁虽然解决了缓存不一致的问题，但却大幅降低了CPU的利用率，于是CPU使用缓存锁来替代总线锁。

**
**

***\**\*缓存锁：\*\**\***如果当前CPU访问的数据已经缓存在其他CPU的高速缓存中，那么当前CPU便不会在总线上发出一个Lock[#信号](javascript:;)，而是采用MESI缓存一致性协议来保证多个CPU的缓存一致性。***\*
\****

**
**

***\*(6)MESI缓存一致性协议\****

该协议要求每个CPU都可以监听到总线上的数据事件并做出相应的处理。当某个CPU向总线发出请求时，其他CPU便能监听到总线收到的请求，从而可以根据当前缓存行的状态和监听的请求类型来更新缓存行状态。这其实也就是所谓的CPU嗅探机制。



这样MESI缓存一致性协议就能保证，在CPU的缓存模型下，就不会出现多线程并发读写变量，各CPU没有办法及时感知到的问题，也就是解决了CPU缓存的一致性问题。

**
**

***\*(7)CPU、高速缓存、内存之间的关系总结\****

***\**\*高速缓存可解决CPU与内存的速度矛盾：\*\**\***为了解决CPU与内存速度之间的矛盾，引入了高速缓存作为内存与CPU之间的缓冲。这里的高速缓存其实就是三级缓存。每个CPU可能有多个物理核，每个物理核会有多个逻辑核。每个CPU都有自己的三级缓存，每个物理核都有自己的一二级缓存。***\*
\****

**
**

***\**\*每个CPU都有自己的高速缓存：\*\**\***每个CPU都有自己的高速缓存，而它们又共享同一个主内存。当多个CPU的运算任务都涉及到同一块主内存区域时，将可能导致各自的高速缓存的数据不一致。为了解决这种缓存数据不一致，引入了如MESI这些缓存一致性协议(嗅探)。***\*
\****

**
**

***\**\*CPU的乱序执行优化：\*\**\***为了使CPU内部的运算单元能尽量被充分利用，CPU可能会对输入代码进行乱序执行优化。与CPU的乱序执行优化类似，JVM的即时编译中也有指令重排序优化。

**
**

***\*4.Java内存模型JMM\****

***\*(1)Java内存模型JMM简介\****

Java内存模型是用来屏蔽各种硬件和操作系统的内存访问差异的，以实现让Java程序在各种平台下都能达到一致的内存访问效果。Java内存模型JMM的具体内容包括如下：



一.主内存与工作内存的关系

二.主内存与工作内存之间的交互

三.对于volatile变量的特殊规则

四.针对long和double型变量的特殊规则

五.原子性、可见性和有序性

六.Happens-Before原则(先行发生原则)

**
**

***\*(2)主内存与工作内存的关系\****

一.JMM规定所有共享变量都存储在主内存

这里的共享变量只包括实例字段、静态字段和构成数组对象的元素。但不包括局部变量与方法参数，因为这些是线程私有的，不会共享。



二.每个线程都有自己的工作内存

这里的工作内存可以理解为各个CPU上的高速缓存，线程的工作内存中保存了被该线程使用的变量的主内存副本。线程对变量的读写操作必须在工作内存中进行，不能直接读写主内存的数据。线程之间无法直接访问对方工作内存中的变量，线程之间变量值的传递均需要通过主内存来完成。



三.主内存和工作内存分别存堆栈数据

主内存对应于Java堆中对象的实例数据，工作内存对应于虚拟机栈中的部分区域。对象包括对象头、实例数据、对象填充。

**
**

***\*(3)主内存与工作内存的交互\****

JMM定义了8种操作来完成：一个变量如何从主内存拷贝到工作内存，如何从工作内存同步回主内存。这8种操作都是原子的、不可再分的。



一.read：读取主内存的变量值并传输到线程的工作内存中，配合load使用

二.load：把read操作从主内存读取到的变量值写入工作内存的变量副本中

三.use：从工作内存中读取出数据，然后交给执行引擎进行计算

四.assign：将执行引擎计算好的值赋值给工作内存中的变量

五.store：把工作内存中的变量值传输回主内存中，配合write使用

六.write：把store操作从工作内存中得到的变量值写入主内存的变量中

七.lock：作用于主内存中的变量，用来标识变量被某个线程独占

八.unlock：作用于主内存中的变量，用来释放锁定状态



在Java内存模型下，多线程并发运行依然存在CPU高速缓存不一致问题。线程1修改了flag之后write回主内存，线程2也还是没法感知到flag已修改。

**
**

***\*(4)对于volatile变量的特殊规则\****

volatile变量对所有线程是立即可见的，对volatile变量的所有写操作都能立刻反映到其他线程之中，volatile变量在各个线程的工作内存中是不存在一致性问题的。从物理存储角度看，各线程的工作内存的volatile变量也可存在不一致的情况。但由于各线程每次使用volatile变量前都刷新，执行引擎看不到不一致的情况。因此可以认为不存在不一致的问题。volatile变量是禁止指令重排序优化的。

**
**

***\*(5)针对long和double型变量的特殊规则\****

long合double的非原子性协定：允许虚拟机将没有被volatile修饰的64位数据的读写操作，划分为两次32位的操作来进行。即允许虚拟机自行选择是否要保证64位数据类型的：read、load、store和write四个操作的原子性。



如果有多个线程共享一个并未声明位volatile的long或double类型的变量，且同时对它们进行读取和修改，那么某些线程可能会读取到一个既不是原值，也不是其他线程修改值的"半个变量"的值。这种情况很罕见，64位的JVM不会出现，但32位的JVM有可能出现。

**
**

***\*5.JMM如何处理并发中的原子性可见性有序性\****

***\*(1)并发过程中可能产生的三类问题\****

一.原子性问题

对于一行代码"n++"，只要多个线程并发执行，都不保证该操作是原子性的。如果保证了该自增操作的原子性，那么下图线程1的i++为1，线程2的i++为2。



二.可见性问题

线程1修改完了主内存的某个变量值，线程2一直读取的是CPU高速缓存中的该变量值，线程1修改完的该变量值对线程2不可见。



三.有序性问题

有序性指的是程序按照代码的先后顺序执行。而编译器为了优化性能，有时候会改变程序中语句的先后顺序。即编译器和指令器有时为了提高代码执行效率，会将指令进行重排序。比如"a=1;b=2;"，编译器优化后可能就变成了"b=7;a=6"。此时编译器虽然调整了语句的顺序，但不会影响最终结果。

**
**

***\*(2)JMM如何处理原子性\****

由JMM来直接保证的原子性变量操作包括：read、load、assign、use、store和write六个。基本数据类型的访问、读写都是原子性的，例外就是long和double的非原子性协定。



JMM还提供了lock和unlock操作来满足更大范围的原子性保证。这是通过字节码指令monitorenter和monitorexit来隐式地使用这两个操作的，这两个字节码指令反映到Java代码中就是同步块(synchronized修饰的代码)。

**
**

***\*(3)JMM如何处理可见性\****

可见性就是指当一个线程修改了共享变量的值，其他线程能立即得知该修改。JMM是通过在变量被修改后将新值同步回主内存，在变量被读取前从主内存刷新变量值的方式来实现可见性的，无论普通变量还是volatile变量都是如此。



普通变量和volatile变量的区别是：volatile保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。也就是volatile保证了多线程操作时变量的可见性，而普通变量则不能保证。



除了volatile关键字，synchronized和final两个关键字也能实现可见性。synchronized的可见性是通过如下这条规则获得的：对一个变量执行unlock操作之前，必须把变量先同步回主内存中。final的可见性是指被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把引用this传递出去，那么其他线程就能看见final字段的值。

**
**

***\*(4)JMM如何处理有序性\****

volatile和synchronized关键字可保证多线程操作的有序性：volatile的有序性是由于volatile变量禁止了指令重排序优化。synchronized的有序性则是通过如下这条规则来获得的：一个变量在同一时刻只允许一个线程对其进行lock操作。



通过Happens-Before规则来保证多线程操作的有序性：Happens-Before规则指定了哪些操作是不能进行重排序的。

**
**

***\*6.volatile如何保证可见性\****

***\*(1)volatile型变量的特殊规则\****

volatile变量对所有线程都是立即可见的：对volatile变量的所有写操作都能立刻反映到其他线程之中，volatile变量在各个线程的工作内存中是不存在数据不一致性的问题。从物理存储的角度看，各个线程的工作内存中，volatile变量也可能存在不一致。但由于各个工作线程在每次使用volatile变量之前都要先刷新其值，于是执行引擎便看不到不一致的情况，因此可以认为不存在不一致的问题。



volatile变量是禁止指令重排序优化的：指令重排序是指CPU将多条指令，不按程序规定的顺序，分开发送给各个相应的电路单元进行处理。可见，volatile型变量的特殊规则就规定了volatile变量对所有线程立即可见。

**
**

***\*(2)volatile如何保证可见性\****

普通变量和volatile变量的区别是：volatile保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。也就是volatile保证了多线程操作时变量的可见性，而普通变量则不能保证。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QCR4pPntW93R83HksKdQoVCdLp0P5Faj2RO8Mg1lVAFTx6yIh6KkY33OlpgRA4OhCthYoiaVppEHzg/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果flag变量是加了volatile关键字，那么当线程1通过assign操作将flag = 1写回工作内存时，会立即执行store和write操作将flag = 1同步到主内存。同时还会让线程2的工作内存中的flag变量的缓存过期，这样当线程2后续从工作内存里读取flag变量的值时，发现缓存已经过期就会重新从主内存中加载flag = 1的值。所以通过volatile关键字可以实现这样的效果：当一个线程修改了变量值，其他线程可以马上感知这个变量值。

**
**

***\*(3)volatile不能保证原子性的字节码解释\****

比如对volatile变量n进行自增的方法虽然只有一行代码，但用javap反编译可知由4条字节码指令构成。当get_field指令把n的值取到操作栈顶时，volatile保证了n的值此时是最新的。但线程1执行iconst_1、iadd这些指令时，线程2可能已经把n的值改变了。于是此时线程1的操作栈顶的n值，就变成了过期数据，所以线程1执行put_field指令后就会把较小的n值同步回主内存中。



严格来说，volatile并不是轻量级的锁或者是轻量级同步机制。因为对于n++这样的基本操作，加了volatile关键字也无法保证原子性。而锁和同步机制，如synchonized或者lock是可以保证原子性的。

**
**

***\*(4)volatile如何保证有序性\*******\**\*
\*\**\***

***\**\*Happens-Before规则的volatile变量规则：\*\**\***程序中的代码如果满足上面这8条规则，就一定会保证指令的顺序。但是如果没满足上面的8条规则，那么就可能会出现指令重排。如对一个volatile变量的写操作先行发生于后面对这个volatile变量的读操作。

**
**

***\**\*volatile型变量的特殊规则：\*\**\***volatile型变量会禁止指令重排序优化。在有序性问题的例子一中，使用volatile修饰flag能禁止重排序避免逻辑异常。在有序性问题的例子二中，使用volatile修饰instance能禁止重排序避免异常。

**
**

***\*7.volatile的原理(Lock前缀指令 + 内存屏障)\****

***\*(1)Lock前缀指令 + MESI实现可见性\****

如果对volatile关键字修饰的变量执行写操作，那么JVM就会向CPU发送一条Lock前缀指令，将这个变量所在的缓存行数据写回到主内存中。同时根据MESI缓存一致性协议，各个CPU会通过嗅探在总线上传播的数据，来检查该变量的缓存值是否过期。如果发现过期，CPU就会将该变量所在的缓存行设置成无效状态。后续当这个CPU要读取该变量时，就会从主内存中加载最新的数据。



所以Lock前缀指令 + MESI缓存一致性协议实现了volatile型变量的可见性。Lock前缀指令会引起将volatile型变量所在的缓存行数据写回到主内存，MESI缓存一致性协议可让CPU检查出哪些缓存被修改，同时令缓存失效。

**
**

***\*(2)通过内存屏障实现禁止指令重排序\****

***\**\*通过内存屏障来禁止某些指令重排序：\*\**\***加了volatile关键字的变量，可以保证前后的一些代码不会被指令重排。那么这个是如何做到的呢？volatille是如何保证有序性的呢？为了保证内存可见性，Java编译器会在生成指令序列的适当位置，插入内存屏障指令来禁止特定类型的指令重排序。

**
**

***\**\*JMM的4种内存屏障指令：\*\**\***LoadLoad屏障(Load1;LoadLoad;Load2)，确保Load1数据的装载，先于Load2及所有后续装载指令的装载。也就是Load1对应的代码和Load2对应的代码，是不能指令重排的。StoreStore屏障(Store1;StoreStore;Store2)，确保Store1数据刷新到主内存，先于Store2及所有后续存储指令的存储。LoadStore屏障(Load1;LoadStore;Store2)，确保Load1数据的装载，先于Store2及所有后续的存储指令刷新到主内存。StoreLoad屏障(Store1;StoreLoad;Load2)，确保Store1数据刷新到主内存，先于Load2及所有后续装载指令的装载。

**
**

***\*8.双重检查单例模式的volatile优化\****

在利用双重检查创建单例的代码中，Singleton.getInstance()方法会先判断inst是否为空，如果为空则锁定Singleton.class并再次检查inst是否为空，如果还为空就创建一个Singleton的对象实例。



其中"inst = new Singleton()"会执行三个指令：

指令1：分配对象的内存空间

指令2：初始化对象

指令3：设置inst变量指向指令1分配的内存地址



如果按照正常顺序来执行，那么是不会有问题的。但是由于指令2和指令3不存在依赖关系，所以编译器优化后可能进行重排序。于是执行顺序变为：指令1 -> 指令3 -> 指令2。那么就可能发生：在线程A刚把inst指向对应地址后，线程B获取到执行权。然后线程B便获取到一个没有初始化的对象，从而产生空指针异常。

**
**

***\*9.synchronized关键字的原理\****

***\*(1)JMM是多线程并发安全问题的根本原因\****

多线程并发写一个共享变量会出现问题的根本原因是Java内存模型JMM。



在Java内存模型下，多个线程并发执行时，每个线程(一般对应一个CPU)都会有自己的工作内存，每个线程读写数据时，线程对应的CPU会从主内存获取数据到本地进行缓存。



volatile是无法保证原子性的，因为volatile的底层机制是：Lock前缀指令 + MESI缓存一致性协议。某线程修改变量时会刷主内存，并使其他线程工作内存的该变量缓存过期。

**
**

***\*(2)synchronized的常见使用方法\****

***\**\*加类锁：\*\**\***如果synchronized一个静态方法，就是对这个类的Class对象加锁。如果synchronized(类.class)，也是对这个类的Class对象加锁。同一时间只有一个线程可以访问同一个类的synchronized方法。注意：每个类都会对应一个Class对象。

**
**

***\**\*加对象锁：\*\**\***如果synchronized一个普通的方法，那么就是对当前的对象实例加锁。同一时间只有一个线程可以访问同一个对象实例的synchronized方法。注意：synchronized一个代码片段的常见写法，就是synchronized(this)，意思就是基于当前这个对象实例来进行加锁。



synchronized锁分两种：一是对某个对象加锁，二是对某个类加锁。对类加锁其实也是在对一个对象加锁，只不过是对类的Class对象加锁。类是在JVM启动过程中加载的，每个.class文件被装载后会产生一个Class对象，每个.class文件产生的Class对象在JVM进程中是全局唯一的。static修饰的成员变量和方法，它们的生命周期都是属于类级别的，它们随着类的定义被分配和装载到内存，随着类被卸载而回收。实例对象的生命周期伴随着实例对象的创建而开始，同时伴随着实例对象的回收而结束。因此，类锁和对象锁的最大区别就是：锁的生命周期不同。

**
**

***\*(3)锁信息的存储\****

一个Java对象的存储结构由三部分组成：对象头、实例数据、对齐填充。其中对象头也由三部分组成：Mark Word、Klass Pointer、Length，而Mark Word会记录该对象的HashCode、分代年龄和锁标记位。

**
**

***\*(4)锁的四种状态\****

为了减少获得锁和释放锁带来的性能损耗，JDK 1.6引入了偏向锁和轻量级锁。锁一共有4种状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，这意味着偏向锁升级为轻量级锁后不能降级回偏向锁。

**
**

***\*(5)什么是偏向锁\****

大多数情况下，锁不仅不存在多线程竞争，而且总会由同一线程多次获得。所以，为了让线程获得锁的代价更低，便引入了偏向锁。偏向锁可以认为是在没有多线程竞争的情况下，访问同步块的加锁场景。也就是在单线程执行同步块的情况下，就没有必要使用重量级锁了。为了提升性能，没必要基于操作系统级别的Mutex Lock来实现锁的抢占。



偏向锁的作用是：线程在没有线程竞争的情况下去访问同步块代码时，会先尝试通过偏向锁来抢占访问资格，这个抢占过程是基于CAS来完成的。如果抢占锁成功，则直接修改对象头中的Mark Word信息。也就是修改偏向锁标记为1、锁标记为01，以及存储当前获得锁的线程ID。



偏向的意思是：如果线程X获得了偏向锁，当线程X后续再访问这个同步块时，就会判断出对象头中的线程ID和线程X相等，于是就不需要再次抢占锁了。

**
**

***\*偏向锁的实现流程：\****

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QCR4pPntW93R83HksKdQoVCWZzyt167Y9r95dIlLl0GmA8WcrTOuA7vqFPkbBuGnVEIGrIE6GgnQw/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

***\**\*偏向锁的实现原理：\*\**\***就是使用CAS来设置对象头中的线程ID。如果成功则获得偏向锁，如果失败则升级到轻量级锁。

**
**

***\*(6)什么是轻量级锁\****

***\**\*轻量级锁的介绍：\*\**\***如果没有线程竞争，使用偏向锁能够在不影响性能的前提下获得锁。如果有多个线程并发访问同步块，那么没抢占到锁的线程只能进行阻塞等待。但在使用重量级锁阻塞等待前，还有更好的平衡方案，也就是使用轻量级锁。所谓的轻量级锁，就是没有抢占到锁的线程，进行一定次数的自旋重试。如果线程在重试过程中抢占到了锁，那么这个线程就不需要阻塞了。如果持有锁的线程占用锁的时间比较短，则自旋带来的性能提高会比较明显。如果持有锁的线程占用锁的时间比较长，则自旋就会浪费CPU资源。所以线程通过自旋来重试抢占锁的次数必须要有一个限制。为了优化自旋，JDK还引入了自适应自旋锁。如果在一个锁对象中，通过自旋获得锁很少成功，则JVM会缩短自旋次数。否则，JVM可能会增加自旋次数。

**
**

***\*轻量级锁的实现流程：\****

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QCR4pPntW93R83HksKdQoVCaqZVLyhOCf1wz52ib9KE2bcCu9eZm57dXcxB8E5y7TVQn0Xz1aXiaKEw/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

***\**\*轻量级锁的实现原理：\*\**\***如果偏向锁存在竞争或者偏向锁未开启，那么当线程访问同步块代码时就会通过轻量级锁来抢占锁资源。轻量级锁的原理就是，通过CAS来修改锁对象中指向Lock Record的指针。其中偏向锁存在竞争指的是，还没有线程通过CAS设置对象头的线程ID。此时多个线程会同时执行CAS尝试获取偏向锁，失败的就升级轻量级锁。而偏向锁未开启指的是，已有线程成功通过CAS设置了对象头的线程ID。此时多个线程同时访问同步块代码，就会直接升级轻量级锁。

**
**

***\*(7)什么是重量级锁\****

轻量级锁能够通过一定次数的重试，让每一个没获得锁的线程有可能抢占到锁。但轻量级锁只有在获得锁的线程持有锁的时间比较短的情况下才能提升性能。如果持有锁的线程占用锁的时间较长，那么不能让没抢到锁的线程一直自旋。如果没抢到锁的线程通过一定次数的自旋后，发现仍然没有获得锁。那么就只能升级到重量级锁，来进行阻塞等待了。



重量级锁的本质是：没有获得锁的线程会通过park()方法挂起，接着被获得锁的线程通过unpark()方法唤醒后再次抢占锁，直到抢占成功。重量级锁依赖于底层操作系统的Mutex Lock来实现。使用Mutex Lock时需要从用户态切换到内核态，才能将当前线程挂起，所以性能开销比较大。从偏向锁到轻量级锁再到重量级锁，整个优化过程其实使用了乐观锁的思想。

**
**

***\*(8)锁升级的流程\****

***\**\*锁升级的流程：\*\**\***当一个线程访问使用了synchronized修饰的代码块时，如果当前还没有线程获得偏向锁，则先通过CAS尝试获得偏向锁。如果当前已有线程获得偏向锁，则尝试升级到轻量级锁去抢占锁。轻量级锁就是通过多次CAS(也就是自旋)来完成的。如果线程通过多次自旋仍然无法获得锁，那么就只能升级到重量级锁进行阻塞等待。

**
**

***\**\*偏向锁和轻量级锁的区别：\*\**\***偏向锁只能保证偏向同一个线程，只要有线程获得过偏向锁，那么当其他线程抢占锁时，只能通过轻量级锁来实现，除非触发了重新偏向。如果获得轻量级锁的线程在后续的20次访问中，发现每次访问锁的线程都是同一个，那么就会触发重新偏向。



轻量级锁可以灵活释放。如果线程1抢占了轻量级锁，那么在锁用完并释放后，线程2可以继续通过轻量级锁来抢占锁资源。偏向锁，就是在一段时间内只会由同一个线程来获得和释放锁，加锁的方式是把线程ID保存到锁对象的Mark Word中。



轻量级锁，就是在一段时间内可能会由多个线程来获得和释放锁。存在锁交替竞争的场景，但在同一时刻不会有多个线程同时获得锁。加锁的方式是首先在每个线程的栈帧中分配一个Lock Record，然后把锁对象中的Mark Word拷贝到Lock Record中，最后把锁对象的Mark Word的指针指向Lock Record。

**
**

***\*(9)锁膨胀的流程\****

***\**\*获取重量级锁之前的锁膨胀：\*\**\***如果线程在运行synchronized修饰的同步块代码时，发现锁状态是轻量级锁并且有其他线程抢占了锁资源，那么该线程就会触发锁膨胀升级到重量级锁。在获取重量级锁之前会先实现锁膨胀，锁膨胀时首先会创建一个ObjectMonitor对象，然后把ObjectMonitor对象的指针保存到锁对象的Mark Word中。重量级锁的实现是在ObjectMonitor中完成的，所以锁膨胀的意义就是构建一个ObjectMonitor对象。

**
**

***\**\*ObjectMonitor对象的重要字段：\*\**\***_owner表示保存当前持有锁的线程，_cxq表示没有获得锁的线程队列，_waitset表示被wait()方法阻塞的线程队列，_recursions表示锁被重入的次数。

**
**

***\**\*重量级锁的获取流程：\*\**\***重量级锁的竞争都是在ObjectMonitor对象中完成的。首先判断当前线程是否是重入，如果是则重入次数 + 1。然后通过CAS自旋来判断ObjectMonitor中的_owner字段是否为空。如果为空，则表示重量级锁已经被释放，当前线程可以获得锁。如果不为空，就继续进行自适应自旋重试。最后如果通过自旋竞争锁失败，那么就把当前线程构建成一个ObjectWaiter结点，插入到ObjectMonitor的_cxq队列的队头，然后再调用park()方法阻塞当前线程。

**
**

***\**\*重量级锁的释放流程：\*\**\***首先把ObjectMonitor的_owner字段设置为null，然后从ObjectMonitor的_cxq队列中调用unpark()方法唤醒一个阻塞的线程。被唤醒的线程会重新竞争重量级锁，如果没抢到，则继续阻塞等待。因为synchronized是非公平锁，被唤醒的线程不一定能重新抢占到锁。

**
**

***\*(10)synchronized的lock和unlock操作规定保证原子性 + 可见性 + 有序性\****

synchronized是由monitorenter和monitorexit这两条字节码指令来实现的。可以理解为monitorenter指令对应了重量级锁的获取流程，monitorexit指令对应了重量级锁的释放流程。



这两个字节码指令最终又会在内存间的交互时，使用lock操作和unlock操作。通过lock操作和unlock操作的语义来实现synchronized的原子性。lcok操作前需要从主内存同步最新值到工作内存，unlock操作前会将工作内存上的值刷新回主内存，这样lock操作和unlock操作就实现了synchronized的可见性。此外还规定，同一时刻只有一条线程可以进行lock操作，这样就实现了synchronized的有序性。

**
**

***\*10.wait()与notify()的底层原理\****

这与synchronized的原理(ObjectMonitor对象)相关，ObjectMonitor对象有一个_waitset队列和重入计数器。使用wait()和notify()时必须对同一个对象实例进行加synchronized锁。如果对象实例加锁，那么重入计数器 + 1。如果对象实例释放锁，那么重入计数器 - 1。



执行wait()方法时会释放锁 + 阻塞当前线程 + 把当前线程放入_waitset队列，执行notify()方法时会唤醒_waitset队列里的被阻塞的线程。



wait()与sleep()的区别：两者都会等待，前者释放锁，后者不释放锁。wait()必须要有其他线程调用notify()来唤醒它。wait(timeout)会阻塞一段时间，然后自己唤醒自己，继续争抢锁。wait()与notify()必须与synchornized一起，对同一个对象进行使用。notify()会唤醒阻塞状态的一个线程，notifyall()会唤醒阻塞状态的所有线程。

**
**

***\*11.Atomic原子类中的CAS无锁化原理\****

***\*(1)Atomic原子类与CAS\****

Atomic原子类底层的核心原理就是CAS，属于一种乐观锁。每次修改时就先对比原值，看看有没有其他线程修改过原值。如果没有修改过就可以修改，如果有修改就重新查出最新值来重复这个过程。Atomic原子类通过Unsafe执行CAS操作，Unsafe类仅限JDK内部使用。

**
**

***\*(2)CAS的底层工作原理\****

底层的CAS方法中会传递三个参数。第一个参数V表示要更新的变量，第二个参数E表示期望值，第三个参数U表示更新后的值。更新的方式是：如果V == E，表示预期值和实际值相等，则将V修改成U并返回true；否则修改失败，然后返回false。

**
**

***\*(3)自旋策略与阻塞策略\****

当一个线程拿不到锁时，有两种基本策略。策略一是放弃CPU进入阻塞状态，等待后续被唤醒，再重新被操作系统调度。策略二是不放弃CPU，而是进入空转进行不断重试，也就是自旋。单核CPU只能使用策略一，AtomicInteger就是使用了自旋策略。synchronized则是先自旋几圈，自旋后还获取不到锁再阻塞。

**
**

***\*(4)Atomic原子类基于CAS操作的三大问题\****

***\**\*ABA问题：\*\**\***ABA问题就是如果某个值一开始是A，后来变成了B，然后又变成了A。AtomicStampedReference能原子更新带有版本号的引用类型，解决ABA问题。此外一般用AtomicInteger进行的是不断累加计数，所以ABA问题比较少。

**
**

***\**\*无限循环问题：\*\**\***Atomic原子类设置值的时候会进入一个无限循环，只要不成功就不停循环再次尝试，在高并发修改值时是挺常见的。比如用AtomicInteger定义一个原子变量，高并发下修改时，可能会导致compareAndSet()要循环很多次才设置成功。所以引入了LongAdder来解决，通过分段CAS的思路来解决无限循环问题。

**
**

***\**\*多个变量的原子性问题：\*\**\***一般的AtomicInteger，只能保证一个变量的原子性，但是如果多个变量呢？要保证多个变量的原子性，可以使用AtomicReference来封装自定义对象。将多个变量放在一个对象里，通过对象的引用来实现多个变量的原子性。

**
**

***\*12.LongAdder的分段CAS优化多线程自旋\****

***\*(1)采用分段CAS降低重试频率\****

这种分段的做法类似于JDK7中的ConcurrentHashMap的分段锁。



高并发场景下，value变量其实就是一个热点数据，大量线程竞争一个热点。LongAdder基本思路就是分散热点，将value分散到一个Cell数组中。不同线程会命中数组的不同槽位，各线程只对自己槽位的value进行CAS操作。这样热点就被分散了，冲突概率就变小了。



LongAdder内部有一个base变量和一个Cell[ ]数组。当并发不高的时候都是通过CAS来直接操作base变量的值。如果对base变量的CAS失败，则再针对Cell[ ]数组中的Cell进行CAS操作。如果对Cell[ ]数组中的Cell进行CAS失败，则换一个Cell进行CAS操作。



LongAdder在无竞争情况下，跟AtomicLong是一样的， 对同一个base进行操作。当出现竞争的时候，则采用化整为零分散热点的做法，用空间换时间。通过使用一个Cell[ ]数组，将一个value拆分进这个Cell[ ]数组中。

**
**

***\*(2)通过惰性求值提升自增性能\****

只有在使用longValue()方法获取当前累加值时才会真正去结算计数的数据。LongAdder.longValue()方法其实就是调用LongAdder.sum()方法，LongAdder.sum()方法会将Cell数组中的各元素value和base累加作为返回值。



AtomicLong的incrementAndGet()方法每次都会返回long类型的计数值，每次递增后还会伴随着数据返回，增加了额外的开销。

**
**

***\*(3)LongAdder和AtomicLong对比\****

AtomicLong的实现原理是：基于CAS + 自旋操作，CAS是基于硬件来实现原子性的，可以保障线程安全。AtomicLong的使用场景：低并发下的全局计数器、序列号生成器。AtomicLong的优势是：占用空间小。AtomicLong的缺点是：高并发下性能急剧下降，N个线程同时进行自旋，N - 1个线程会自旋失败、不断重试。



LongAdder设计思想是：空间换时间，分散热点数据value的值。LongAdder的实现原理是：高并发时通过Cell[ ]数组进行分段CAS。LongAdder的使用场景是：高并发下的全局计数器。LongAdder的优势是：减少CAS重试次数、防止伪共享、惰性求值。LongAdder的缺点是：如果使用它的sum()方法时有并发更新，可能数据结果存在误差。

**
**

***\*(5)LongAdder的设计总结\****

***\*设计一：分段CAS机制\****

把一个变量拆成多份变成多个变量，类似JDK 1.7的ConcurrentHashMap的分段锁。具体来说就是把一个Long型变量拆成一个base变量外加多个Cell变量，每个Cell变量包装了一个Long型变量。当多个线程并发累加时，如果并发度低就直接加到base变量上，如果并发度高就分散到Cell变量上。在最后取值时，再把base变量和这些Cell变量进行累加求和运算。LongAddr只能进行累加操作，并且初始值默认为0。LongAccumulator可以自定义一个二元操作符，而且可以传入一个初始值。

**
**

***\*设计二：最终一致性\****

LongAddr的sum()方法并没有对cell[ ]数组加锁，所以存在一边有线程对cell[ ]数组求和、一边有线程修改数组的情况。类似于ConcurrentHashMap的clear()方法，一边清空数据一边放入数据。

**
**

***\*设计三：伪共享与缓存行填充\****

LongAddr在定义Cell时，使用了注解@Contended。这个注解可以用来进行缓存行填充，从而解决伪共享问题。

**
**

***\**\*设计四：数组扩容\*\**\***

Cell[ ]数组的大小始终是2的整数次方，每次扩容都变为原来的2倍。

**
**

***\*(6)伪共享问题说明\****

每个CPU都有自己的缓存，也就是高速缓存。CPU缓存与主内存进行数据交换的基本单位叫缓存行。CPU缓存是由若干个缓存行组成的，缓存行是CPU缓存的最小存储单位。在64位的x86架构中，每个缓存行是64字节，也就是8个Long型的大小。当CPU的缓存失效了需要从主内存刷新数据时，至少需要刷新64字节。



假设主内存的Long型变量X、Y已被CPU1和CPU2分别读入自己的缓存，且Long型变量X、Y在CPU缓存和主内存中都是放在同一行缓存行中的。这样当CPU1修改了变量X，需要失效整个缓存行时，就会往总线发送消息，通知CPU2对应的缓存行失效。所以虽然CPU2并没有修改变量Y，但也需要刷新变量Y所在的缓存行。这就是伪共享问题，缓存行上的不同变量，读CPU受到写CPU的影响。
