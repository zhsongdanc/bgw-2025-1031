***\**\*大纲\*\**\***(33600字)

***\*1.高并发订单系统面临的技术挑战\****

***\*2.消息中间件的作用\****

***\*3.Kafka、RabbitMQ以及RocketMQ的调研对比\****

***\*4.消息中间件路由中心的架构原理\****

***\*5.Broker的主从架构原理\****

***\*6.高可用的消息中间件生产部署架构\****

***\*7.全链路分析为什么消息会丢失\****

***\*8.RocketMQ的事务消息机制实现发送消息零丢失\****

***\*9.RocketMQ事务消息机制的底层实现原理\****

***\*10.是否可以通过同步重试方案来代替事务消息方案来实现发送消息零丢失\****

***\*11.同步刷盘 + Raft协议同步实现发送消息零丢失\****

***\*12.手动提交offset + 故障\**\**\*\*自动\*\**\**\*转移消费消息零丢失\****

***\*13.基于RocketMQ全链路的消息零丢失方案总结\****

***\*14.消息重复 + 处理失败 + 乱序 + 过滤等问题\****

***\*15.RocketMQ的生产实践经验总结\****

***\*16.如何处理RocketMQ的百万消息积压问题\****

***\*17.为RocketMQ增加消息限流功能保证其高可用\****

***\*18.从Kafka迁移到RocketMQ的双写双读方案\****

***\*19.Kafka利用零拷贝和页缓存实现高性能读取\****

***\*20.Kafka集群的分布式存储和高可用\****

***\*21.Kafka的LEO机制和高水位机制\****

***\*22.Kafka的ISR工作\*******\*原理和机制\*******\*
\****

***\*
\****

**简历关键词：深入理解RocketMQ和Kafka的运行原理，并能够根据不同场景选择合适的方案，有效利用消息中间件以提升系统性能。**

***\*
\****

***\*1.\*******\*高并发订单系统面临的技术挑战\****

一.下单时还要发券发红包Push推送等导致性能太差

二.订单退款时经常流程失败导致无法完成退款

三.第三方客户系统对接耦合性太高导致经常出问题

四.大数据团队需要订单数据应该怎么对接

五.秒杀活动时数据库压力太大应该怎么缓解

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBZj8xMq5PRAtFvGSaQ15RkZKvQmibtWD8z9T0OCDvSvHZUSSMj6ZHkZKubUMckAwrial8raDgY1HUQ/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

订单系统面临的常见技术问题如下：

一.下单核心流程环节太多，性能较差

二.订单退款流程可能面临退款失败的风险

三.关闭过期订单时存在扫描大量订单数据的问题

四.跟第三方物流系统耦合，存在性能抖动的问题

五.大数据团队获取订单数据，存在不规范直接查询订单数据库的问题

六.进行秒杀时订单数据库压力过大

**
**

***\*2.消息中间件的作用\****

消息中间件，其实就是一种系统，它自己独立部署。然后让两个系统之间通过发消息和收消息，来进行异步的调用，而不是仅仅局限于同步调用。消息中间件的主要作用有三个：异步化提升性能、降低系统耦合、流量削峰。

**
**

***\*(1)异步化提升性能\****

如果在系统A和系统B之间加一个MQ：系统A干完自己的事情，就20ms，然后发送一条消息到MQ，就5ms，然后就返回结果给用户了。也就是说，用户仅仅等待25ms就收到了结果。然后系统B从MQ里获取消息，接着花费200ms去执行。但是这个200ms就跟用户没关系了，用户早就收到结果了，也不关心你花200ms还是2s去干自己的事。这样对用户来说，就从原来等待220ms返回结果，变成现在只要25ms就可以返回结果了。可见，消息中间件可以用来提升系统性能。

**
**

***\*(2)降低系统耦合\****

如果系统A同步调用系统B，这其实属于系统间的耦合，系统A和系统B互相之间会有影响。如果系统B突然出现故障，会导致系统A在调用系统B的时候感知到这个故障。由于系统B故障了，所以系统A对系统B的调用失败。系统A就得返回异常给用户，以及在后续处理这个异常。



假设在系统A和系统B之间加入一个消息中间件。在这种情况下，系统A对系统B的调用仅仅是发送一个消息到MQ，然后就直接返回给用户了，后面对系统B就不管了。此时系统B如果出现了故障，对系统A根本没影响，系统A也感觉不到。系统B出现故障就成了它自己的事，它要等故障恢复后继续去完成它的工作，此时对系统A没任何影响了。

为什么会有这样的效果呢？因为通过引入MQ，两个系统实现了异步化调用，也就实现了解耦。系统A并没有跟系统B耦合，所以互相之间并没有任何影响。所以消息中间件还能让两个系统解耦，让它们互相之间没有任何影响。

**
**

***\*(3)流量削峰\****

消息中间件还有一个特别好的功能叫流量削峰。假设系统A是不操作数据库的，因此只要多部署几台机器，就可以抗下每秒1万的请求。比如部署个20台机器，就可以轻松抗下每秒上万请求。然后系统B是要操作一台数据库服务器的，这台数据库服务器的上限是每秒接收6000请求。那么系统B无论部署多少台机器都没有用，因为它依赖的数据库最多只能每秒接收6000请求。



比如系统A在1秒内收到了1万请求，然后把这1万请求转发给系统B。由于系统B要操作数据库，每秒最多能处理6000请求，所以此时系统B是抗不下这1万请求的，一定会压垮数据库。因此可以通过引入MQ来解决这个问题，MQ抗高并发的能力远远高于数据库。同样配置下，如果数据库可以抗每秒6000请求，那么MQ可以抗每秒几万请求。



为什么呢？因为数据库复杂，它要能够支持执行复杂的SQL语句，支持事务等复杂的机制，支持对数据进行增删改查等。所以一般数据库单服务器也就支撑每秒几千的请求。但MQ就不一样了，它的核心功能就是让系统A发消息给它，再让系统B从它这里获取消息。这个就简单得多了，所以同等机器配置下，MQ一般都能抗几万并发请求。



所以只要引入MQ，就可以让系统A把每秒1万请求都作为消息直接发送到MQ里，MQ可以轻松抗下这每秒1万请求。接着系统B只要慢慢的从MQ里获取消息然后执行数据库读写操作即可，这个获取消息的速度是系统B自己可以控制的。所以系统B完全可以用一个比较低的速率获取消息然后写入数据库，保证对数据库的QPS不超过它的极限值6000。



这时因为系统A发送消息到MQ很快，系统B从MQ消费消息很慢，所以MQ里自然会积压一些消息。不过不要紧，MQ一般都是基于磁盘来存储消息的，所以适当积压一些消息是可以的。当系统A的高峰过去，每秒可能就恢复到1000请求，此时系统B还是以每秒6000请求的速度获取消息写入数据库，那么自然MQ里积压的消息就会慢慢被消化掉了。



所以这就是使用MQ进行流量削峰的效果：系统A发送过来的每秒1万请求是一个流量洪峰，然后MQ直接给扛下来了，都存储到自己本地磁盘。这个过程就是流量削峰的过程，瞬间把一个洪峰给削下来，让系统B后续慢慢获取消息来处理。

**
**

***\*3.Kafka、RabbitMQ以及RocketM\*******\*Q的调研对比\****

一般国内常用的MQ技术有四种实现：ActiveMQ、Kafka、RabbitMQ、RocketMQ。但是其中ActiveMQ主要是几年以前较多公司使用，现在国内用的公司都很少了。因此下面主要针对Kafka、RabbitMQ、RocketMQ三种技术进行介绍。

**
**

***\*(1)Kafka的优势和劣势\****

首先Kafka的吞吐量几乎是行业里最优秀的，在常规的机器配置下，一台机器的QPS就可以达到每秒十几万，相当的强悍。Kafka性能也很高，基本上发送消息给Kafka是毫秒级的性能。Kafka可用性也很高，Kafka是支持集群部署的，其中部分机器宕机是可以继续运行的。



Kafka的劣势一是：存在数据丢失的问题。因为Kafka收到消息之后会写入一个内存缓冲区里，并没有直接落地到物理磁盘上去。所以要是机器本身故障了，可能会导致内存缓冲区里的数据丢失。



Kafka的劣势二是：功能非常单一。主要是支持发送消息给它，然后从里面消费消息，其他就没有什么额外的高级功能了。所以基于Kafka有限的功能，可能适用的场景并不是很多。



因此综上所述得出Kafka在行业里的一个使用标准：就是把Kafka用在用户行为日志的采集和传输上。比如大数据团队要收集APP上用户的一些行为日志，这种日志就是用Kafka来收集和传输的。因为那种日志适当丢失数据是没有关系的，而且一般量特别大，要求吞吐量要高，一般就是收发消息，不需要太多的高级功能，所以Kafka是非常适合这种场景的。

**
**

***\*(2)RabbitMQ的优势和劣势\****

在RocketMQ出现之前，国内大部分公司都从ActiveMQ切换到RabbitMQ来使用，包括很多一线互联网大厂，而且直到现在都有很多中小型公司在使用RabbitMQ。



RabbitMQ的优势在于可以保证数据不丢失，也能保证高可用性，即集群部署的时候部分机器宕机可以继续运行，然后支持部分高级功能，比如死信队列、消息重试等，这些是它的优点。



RabbitMQ缺点一是：吞吐量是比较低的，QPS一般就是几万的级别。所以如果遇到特别高并发的情况下，支撑起来是有点困难的。而且它进行集群扩展的时候(也就是加机器部署)，还比较麻烦。



RabbitMQ缺点二是：开发语言是erlang，国内很少有精通erlang语言的工程师。因此也没办法去阅读它的源代码，甚至修改它的源代码。



所以现在行业里的一个情况是，很多BAT等一线互联网大厂都切换到使用更加优秀的RocketMQ了。但是很多中小型公司觉得RabbitMQ基本可以满足自己的需求还在继续使用中，因为中小型公司并不需要特别高的吞吐量，RabbitMQ已经足以满足他们的需求了，而且也不需要部署特别大规模的集群，也没必要去阅读和修改RabbitMQ的源码。

**
**

***\*(3)RocketMQ的优势和劣势\****

RocketMQ是阿里开源的消息中间件，非常靠谱，它几乎同时解决了Kafka和RabbitMQ的缺陷。



RocketMQ的吞吐量也同样很高，单机QPS可以达到10万以上，而且可以保证高可用性、可以通过配置保证数据绝对不丢失、可以部署大规模的集群、还可以支持各种高级的功能，比如延迟消息、事务消息、消息回溯、死信队列、消息积压等。同时，RocketMQ是基于Java开发的，符合国内大多数公司的技术栈，工程师很容易就可以阅读它的源码，甚至是修改它的源码。



所以现在国内很多一线互联网大厂都切换为使用RocketMQ了。它们需要RocketMQ的高吞吐量，大规模集群部署能力，以及各种高阶的功能去支撑自己的各种业务场景，同时还可以根据自己的需求定制修改RocketMQ的源码。



RocketMQ是非常适合用在Java业务系统架构中，因为它很高的性能表现、高阶功能的支持，可以解决各种业务问题。当然RocketMQ也有一点美中不足的地方，就是RocketMQ的官方文档相对简单一些。

**
**

***\*4.消息中间件路由中心的架构原理\****

***\*(1)研究RocketMQ的切入点是NameServer\****

下面是RocketMQ最基本的一个架构原理图，可知RocketMQ一共包含了四个核心的部分：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QDOHhNg02437c7koJfDUuBMeSUOwuRr3HDMsA9oh9AjNEts6GD9ugpqdHl3qSicGRJYMTtMjxulRyg/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

第一部分是NameServer：这部分很重要，它要负责管理集群里所有Broker的信息，让使用MQ的系统可以通过它感知到集群里有哪些Broker。



第二部分是Broker集群本身：必须得在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本存储和高可用。



第三部分是向MQ发送消息的系统：这些系统一般称之为生产者，生产者会从NameServer拉取路由信息，然后选择Broker机器建立连接以及发送消息。



第四部分是从MQ获取消息的系统：这些系统一般称之为消费者，到底是Broker主动推送消息给消费者、还是消费者自己从Broker里拉取消息。



NameServer是研究RocketMQ必须优先选择的一个切入点，因为没有NameServer，一切都无从谈起，NameServer是RocketMQ运行的起点。

**
**

***\*(2)NameServer可以部署几台机器\****

要部署RocketMQ，就得先部署NameServer，那么这个NameServer到底可以部署几台机器呢？是一台机器？还是可以部署多台机器？如果部署多台机器，他们之间是怎么协同工作的？



NameServer首先支持部署多台机器，也就是说NameServer是可以集群化部署的。这根据前面RocketMQ的基本架构原理图也可以看出，而NameServer要进行集群化部署最主要的一个原因就是为了高可用。因为NameServer是集群里非常关键的一个角色，它要管理Broker信息，生产者和消费者都要通过它才知道跟哪个Broker通信。如果NameServer只部署一台机器，一旦NameServer宕机了，就会导致RocketMQ集群出现故障。所以通常来说，NameServer一定会多机器部署，实现一个集群，起到高可用的效果。保证任何一台机器宕机，其他机器上的NameServer可以继续对外提供服务。

**
**

***\*(3)Broker会把信息注册到哪个NameServer\****

是否会出现这样的情况：比如一共有10台Broker机器，2台NameServer机器，然后其中5台Broker会把自己的信息注册到1台NameServer上去，另外5台Broker会把自己的信息注册到另外1台NameServer上去。



实际上不是这样的，因为这样会有一个问题。如果1台NameServer上有5台Broker信息，另外1台NameServer上有另外5台Broker信息，那么此时任何一个NameServer宕机，就会导致5个Broker的信息没了。这种做法会导致数据丢失，系统不可用。



所以RocketMQ的每个Broker启动时都得向所有的NameServer进行注册。也就是说，每个NameServer都会有一份集群中所有Broker的信息。

**
**

***\*(4)系统如何从NameServer获取Broker信息\****

扮演生产者和消费者角色的系统，如何从NameServer获取到集群的Broker信息。它们需要知道集群里有哪些Broker，才能根据一定的算法挑选一个Broker去发送消息或者获取消息。有两种办法：



第一种办法是NameServer会主动发送请求给所有的系统，告诉它们Broker信息。但这种办法存在的问题是：NameServer不能提前知道要推送Broker信息给哪些系统。



第二种办法是每个系统自己每隔一段时间定时发送请求到NameServer去拉取最新的集群Broker信息。事实上RocketMQ中的生产者和消费者就是这样，生产者和消费者会主动去NameServer拉取Broker信息。

**
**

***\*(5)NameServer如何感知Broker挂了\****

首先一个Broker启动后会向每个NameServer进行注册，这样每个NameServer都知道集群里有这么一台Broker的存在。然后各个系统也从NameServer拉取到了Broker信息，也知道集群里有这么一台Broker。但是如果这台Broker挂了，NameServer应如何感知到。



要解决这个问题，靠的是Broker跟NameServer之间的心跳机制。Broker会每隔30s给所有的NameServer发送心跳，告诉每个NameServer自己目前还活着。每次NameServer收到一个Broker的心跳，就可以更新一下它的最近一次心跳时间。然后NameServer会每隔10s运行一个任务，去检查一下各个Broker的最近一次心跳时间。如果某个Broker超过120s都没发送心跳，那么就认为这个Broker已经挂掉了。

**
**

***\*(6)系统如何感知Broker挂了\****

如果Broker挂掉了，那么作为生产者和消费者的系统是怎么感知到的？难道需要NameServer发送通知给所有的系统吗？



NameServer并不会主动通知生产者和消费者系统。但如果NameServer没有及时通知这些系统，那么可能出现一种情况：刚开始集群里有10个Broker，各个系统从NameServer那里得知，都以为有10个Broker。结果突然挂了一个Broker，120s没发心跳给NameServer，NameServer是知道现在只有9个Broker了。但此时这些系统是不知道只有9个Broker的，还以为有10个Broker。此时可能某个系统就会发送消息到那个已经挂掉的Broker上去。



这里有两种解决办法：第一，可以考虑不发送消息到那台Broker，改成发送到其他Broker上去。第二，如果必须要发送消息给那台Broker，由于它的Slave是备份，所以可以继续使用，可以考虑和它的Slave进行通信。



生产者会有一套容错机制，往Broker发送消息时会通过是否有响应来感知Broker是否挂了。如果没有响应，就会重新到NameServer拉取最新的路由信息，于是就知道是否有Broker宕机了。



以上便是NameServer的核心工作原理，总结如下：NameServer会进行集群化部署、Broker会向所有NameServer进行注册、30s心跳机制和120s故障感知机制、生产者和消费者的客户端容错机制。

**
**

***\*5.Broker的主从架构原理\****

***\*(1)Master Broker如何将消息同步给Slave Broker\****

为了保证MQ的数据不丢失而且具备一定的高可用性，一般都会将Broker部署成Master-Slave模式，也就是一个Master Broker对应一个Slave Broker。然后Master需要在接收到消息后，将数据同步给Slave，这样一旦Master Broker挂了，Slave上还有一份数据。Slave Broker也会向所有的NameServer进行注册，Slave Broker也会向所有的NameServer每30s发送心跳。



接下来考虑一个问题，Master Broker是如何将消息同步给Slave Broker的？是Master Broker主动推送给Slave Broker、还是Slave Broker发送请求到Master Broker去拉取。RocketMQ的Master-Slave模式采取的方式是：Slave Broker不停地发送请求到Master Broker去拉取消息，也就是通过Pull模式拉取消息。

**
**

***\*(2)RocketMQ是否实现读写分离\****

既然Master Broker主要会接收系统的消息写入，然后同步给Slave Broker，那么其实本质上Slave Broker也应该有一份一模一样的数据。所以作为消费者的系统在获取消息时，是从Master Broker获取还是从Slave Broker获取的？其实都不是：有可能从Master Broker获取消息，也有可能从Slave Broker获取消息。



作为消费者的系统在获取消息时，会先发送请求到Master Broker上去，请求获取一批消息，此时Master Broker会返回一批消息给消费者系统。然后Master Broker在返回消息给消费者系统时，会根据当时Master Broker的负载情况和Slave Broker的同步情况，向消费者系统建议下一次拉取消息时是从Master Broker拉取还是从Slave Broker拉取。



举个例子，如果这个时候Master Broker负载很重，本身要抗10万写并发了。现在消费者系统还要从它这里拉取消息，所以此时该Master Broker就会建议消费者从Slave Broker去拉取消息。另外一个例子，这时Master Broker上都已写入100万条数据了，结果Slave Broker不知道啥原因同步的特别慢，才同步了96万条数据，落后了整整4万条消息的同步。这时候作为消费者系统可能都获取到96万条数据了，那么下次获取只能从Master Broker去拉取消息，因为Slave Broker同步太慢导致没法从它那里获取更新的消息了。



所以这一切都会由Master Broker根据情况来决定，并非实行严格的读写分离。在写入消息时，是选择Master Broker进行写入的。在拉取消息时，有可能从Master Broker获取，也可能从Slave Broker去获取，一切都根据当时的情况来定。

**
**

***\*(3)如果Slave Broke挂了有什么影响\****

如果Slave Broker挂掉了，会对整个系统有一点影响，但是影响不太大。因为消息写入全部是发送到Master Broker的，而消息获取也可以通过Master Broker获取，只不过有一些消息的获取可能是从Slave Broker去获取。



所以如果Slave Broker挂了，那么此时无论消息写入还是消息拉取，还可以继续依赖Master Broker，对整体运行不影响。只不过少了Slave Broker，会导致所有读写压力都集中在Master Broker上。

**
**

***\*(4)如果Master Broker挂了怎么办\****

假设出现了一个故障，Master Broker突然挂了，这时候就会对消息的写入和获取都有一定的影响了。但Slave Broker有Master Broker的备份数据，只不过Slave Broker上的数据可能有部分没来得及从Master Broker同步，此时RocketMQ也不会直接自动将Slave Broker切换为Master Broker。



在RocketMQ 4.5版本之前，都是用Slave Broker同步数据，尽量保证数据不丢失。但是一旦Master故障了，Slave是没法自动切换成Master的。所以在这种情况下，如果Master Broker宕机了，这时就得手动进行一些运维操作：对Slave Broker重新修改配置，重启后调整为Master Broker，这还是有点麻烦的，而且会导致中间一段时间不可用。所以这种Master-Slave模式不是彻底的高可用模式，没法实现自动把Slave切换为Master。

**
**

***\*(5)基于Dledger实现RocketMQ高可用自动切换\****

在RocketMQ 4.5之后，这种情况得到了改变，因为RocketMQ支持了一种新的机制叫Dledger。Dledger是基于Raft协议实现的一个机制，利用Dledger可以实现RocketMQ的高可用自动切换。



简单来说，把Dledger融入RocketMQ后，就可以让一个Master Broker对应多个Slave Broker。也就是说一份数据可以有多份副本，比如一个Master Broker对应两个Slave Broker，Master和Slave会进行数据同步。一旦Master Broker宕机，就可以在多个副本也就是多个Slave中，通过Dledger技术和Raft协议算法进行Leader选举，将其中一个Slave Broker选举为新的Master Broker，然后这个新的Master Broker就可以对外提供服务了。



整个过程也许只要10秒或者几十秒的时间就可以完成。这样就可以实现Master Broker挂掉后，自动从多个Slave Broker中选举出来一个新的Master Broker，继续对外服务，一切都是自动的。

**
**

***\*6.高可用的消息中间件生产部署架构\****

***\*(1)MQ生产部署架构的设计任务\****

基于RocketMQ的核心原理去设计一套RocketMQ的生产部署架构，在这套生产部署架构中，需要着重考虑到高可用的问题，要保证整个系统运行过程中任何一个环节宕机都不能影响系统的整体运行。

**
**

***\*(2)NameServer集群化部署保证高可用性\****

首先，要让NameServer集群化部署，比如可以部署在三台机器上。这样可以充分保证NameServer作为路由中心的可用性，只要还有一个NameServer正常运行，就能保证MQ系统的稳定性。



NameServer的设计采用了Peer-to-Peer模式，也就是可以集群化部署。但里面任何一台NameServer机器都是独立运行的，跟其他机器没有任何通信。



每台NameServer实际上都会有完整的集群路由信息，包括所有的Broker节点信息等。所以只要任何一台NameServer存活下来，就可以保证MQ系统正常运行，不会出现故障。因此NameServer集群化部署是RocketMQ架构部署的第一步。

**
**

***\*(3)基于Dledger的Broker主从架构部署\****

其次，要考虑Broker集群应该如何部署，采用什么方式来部署。如果采用RocketMQ 4.5前的那种Master-Slave架构来部署，能在一定程度上保证数据不丢失，也能保证一定的可用性。但是那种方式最大的问题就是当Master Broker挂了之后，没办法让Slave Broker自动切换为新的Master Broker，需要手工做一些运维操作，修改配置以及重启机器才行，这个非常麻烦。而且在手工运维的期间，会导致系统的不可用。



所以既然RocketMQ 4.5后已经基于Dledger技术实现了可以自动让Slave切换为Master的功能，那么肯定是选择基于Dledger的主备自动切换的功能来进行生产架构的部署。而且Dledger技术是要求至少得是一个Master带两个Slave，由三个Broker组成一个Group，即作为一个分组来运行。一旦Master宕机，就可以从剩余的两个Slave中选举出来一个新的Master对外提供服务。



每个Broker(不论是Master和Slave)都会把自己注册到所有的NameServer上去，然后Master Broker还会把数据同步给两个Slave Broker，保证一份数据在不同机器上有多份副本。

**
**

***\*(4)Broker如何跟NameServer进行通信\****

前面介绍了，由于Broker会每隔30秒发送心跳到所有的NameServer上去，然后每个NameServer会每隔10s检查一次有没有哪个Broker超过120s没发送心跳的，如果有就认为那个Broker已经宕机，从路由信息里要摘除这个Broker。下面介绍Broker和NameServer进行通信的细节：



首先，Broker跟NameServer之间的通信是基于TCP长连接来进行的。在RocketMQ的实现中，采用的是TCP长连接进行通信。也就是说，Broker会跟每个NameServer都建立一个TCP长连接，然后定时通过TCP长连接发送心跳请求过去。各个NameServer会通过跟Broker建立好的长连接不断收到心跳数据包，然后通过定时检查Broker有没有120s都没发送心跳数据包，来判定集群里各个Broker到底是否已经挂掉。

**
**

***\*(5)使用MQ的系统都要多机器集群部署\****

日常中一定会有很多的系统使用RocketMQ，有些系统是作为生产者往MQ发送消息，有些系统是作为消费者从MQ获取消息。当然还有的系统既是生产者又是消费者，所以需要考虑这些系统的部署。



对于这些系统的部署本身不应该在MQ的考虑范围内，但是还是应该给出一个建议。就是无论作为生产者还是消费者的系统，都应该多机器集群化部署，保证它自己本身作为生产者或者消费者的高可用性。



因为一个系统如果就部署在一台机器上，然后作为生产者向MQ发送消息。一旦机器上的生产者系统挂了，那么整个流程就断开了，不能保证高可用性。但是如果在多台机器上部署生产者系统，任何一台机器上的生产者挂了，其他机器上的生产者系统可以继续运行。同理消费者系统也应该是集群化部署的，如果一台机器上的消费者系统挂了，其他机器上的消费者系统也可以继续运行。

**
**

***\*(6)MQ的核心数据模型——Topic\****

生产者和消费者都会往MQ里写入消息和获取消息了，但是有一个问题。MQ中的数据模型是什么，生产者投递出去的消息在逻辑上到底是放到哪里去的，是队列还是别的什么。这个涉及的其实就是MQ中的核心数据模型——Topic。



Topic表达的意思是一个数据集合。举个例子，现在订单系统需要往MQ里发送订单消息。那么此时就应该建一个Topic，它的名字可以叫做：topic_order_info，也就是一个包含了订单信息的数据集合。然后订单系统投递的订单消息，都是进入到这个名为topic_order_info的Topic里面。如果仓储系统要获取订单消息，那么它可以指定从名为topic_order_info的Topic里获取消息，获取出来的都是想要的订单消息。



总之，Topic其实就是一个数据集合，不同类型的数据需要放到不同的Topic里去。要是有一些商品数据要发送到MQ里，就应该创建一个Topic，它的名字可以叫：topic_product_info，代表里面都是商品数据。那些想要从MQ里获取商品数据的系统，就可以从名为topic_product_info的Topic里获取了。



所以简单来说，一个系统如果要往MQ里写入消息或者获取消息，首先得创建一些Topic，作为一个数据集合来存放某种类型的消息，比如订单Topic，商品Topic等。

**
**

***\*(7)Topic作为数据集合如何在Broker集群里存储\****

创建的这些Topic是怎么存储在Broker集群里的？比如有一个订单Topic，可能订单系统每天都会往里面投递几百万条数据，然后这些数据在MQ集群上还得保留好多天。那么最终可能会有几千万的数据量，这还只是一个Topic。如果有很多的Topic，并且里面都有大量的数据，最终加起来的总和也许是一个惊人的数字。此时这么大量的数据本身是不太可能存放在一台机器上的。如果一台机器没法放下那么多的数据，那么就应该通过分布式存储来存放。



我们可以在创建Topic时指定让它里面的数据分散存储在多台Broker机器上。比如一个Topic里有1000万条数据，此时有2台Broker，那么就可以让每台Broker上都放500万条数据。这样就可以把一个Topic代表的数据集合分布式存储在多台机器上了。



而且每个Broker在发送定时的心跳给NameServer时，都会告诉NameServer自己当前的数据情况。比如有哪些Topic的哪些数据在自己这里，这些信息都属于路由信息的一部分。这样每个NameServer都知道集群里有哪些Broker，每个Broker存放了哪些Topic的数据。

**
**

***\*(8)生产者系统如何将消息发送给Broker\****

在发送消息之前，得先有一个Topic，然后在发送消息时需要指定要发送到哪个Topic里面去。接着既然知道要发送的Topic，那么就可以和NameServer建立一个TCP长连接，然后定时从那里拉取最新的路由信息。这些路由信息会包括集群里有哪些Broker、集群里有哪些Topic、每个Topic都存储在哪些Broker上。



这样生产者系统就可以通过路由信息找到自己要投递消息的Topic分布在哪几台Broker上。接着就可以根据负载均衡算法，从里面选择一台Broke机器出来。选择一台Broker后，就可以和这个Broker建立一个TCP长连接，然后通过长连接向Broker发送消息。Broker收到消息后就会存储在自己本地磁盘里去。



这里唯一要注意的就是：生产者一定是投递消息到Master Broker的，然后Master Broker会同步数据给它的Slave Broker，实现一份数据多份副本，保证Master故障时数据不丢失，而且可以自动把Slave切换为Master提供服务。

**
**

***\*(9)消费者如何从Broker上拉取消息\****

消费者系统其实跟生产者系统原理是类似的，它们也会和NameServer建立长连接，然后拉取路由信息。接着找到自己要获取消息的Topic在哪几台Broker上，这样就可以和Broker建立长连接，从里面拉取消息了。



这里唯一要注意的就是：消费者系统可能会从Master Broker拉取消息，也可能从Slave Broker拉取消息，看具体情况。所以，最终的MQ生产部署架构图，如下所示：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QDOHhNg02437c7koJfDUuBMEgwJq8L5fldGL7K5z5jRzLZwtqQPJibZ6sBeh5mQ0Nf3iapicvMfNVJxQ/640?wx_fmt=other&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

***\*(10)高可用、高并发、海量消息、可伸缩的架构\****

上述这套生产架构图实现了完全高可用，因为NameServer随便挂一台都可以。同时也是集群化部署的，每台机器都有完整的路由信息。此外Broker随便挂一台也可以，Slave Broker挂了对集群没太大影响。Master Broker挂了也会基于Dledger技术自动实现Slave切换为Master。甚至生产者系统和消费者系统随便挂一台也可以，因为它们本身就是集群化部署的，其他机器会接管工作。



这个架构可以抗下高并发。因为假设订单系统对订单Topic要发起每秒10万QPS的写入，那么只要订单Topic分散在比如5台Broker上。实际上每台Broker会承载2万QPS写入，也就是说高并发场景下的10万QPS可以分散到多台Broker上抗下来。



然后集群足以存储海量消息。因为所有数据都是分布式存储的，每个Topic的数据都是存储在多台Broker机器上的，用集群里多台Master Broker就足以存储海量的消息。



所以采用多个Master Broker部署的方式，加上Topic分散在多台Broker的机制，可以抗下高并发访问以及海量消息的分布式存储。然后每个Master Broker有两个Slave Broker结合Dledger技术可以实现故障时的Slave-Master自动切换，实现高可用性。最后这套架构还具备伸缩性，如果要抗更高的并发存储更多的数据，可以在集群里加入更多的Broker机器进行线性扩展。

**
**

***\*7.全链路分析为什么消息会丢失\****

***\*(1)订单系统推送消息到RocketMQ可能会丢失消息\****

订单系统在接收到订单支付成功的回调后，会推送一条订单支付成功的消息到RocketMQ。在这个过程中，是有可能会出现丢失消息的情况的。因为订单系统在推送消息到RocketMQ时是通过网络去进行传输的，如果网络发生了抖动，就会导致网络通信失败，于是可能某条消息就没有成功投递给RocketMQ。所以订单系统投递消息到RocketMQ可能会因为网络问题而导致失败。



除此之外，还有其他的原因可能会导致订单系统推送消息到RocketMQ失败。比如RocketMQ确实收到消息了，但是它网络通信模块的代码出现了异常，导致消息没成功处理。比如生产者在写消息到RocketMQ的过程中，刚好遇到了某个Leader Broker故障，其他的Follower Broker正在尝试切换为Leader Broker，这个过程中也可能会有异常。



所以我们在使用任何一个MQ时，无论是RocketMQ、还是RabbitMQ、或者是Kafka，首先都要明确一点：不一定发送消息出去就一定会成功、也有可能会失败，此时代码里可能会抛出异常、也有可能不会抛异常。

**
**

***\*(2)消息到达RocketMQ后也可能会丢失消息\****

即使订单系统成功把消息写入了RocketMQ，那么消息也有可能出现丢失。因为根据RocketMQ的底层原理可以知道：消息写入RocketMQ后，RocketMQ可能仅仅是把这个消息写入到PageCache里，也就是OS管理的一个缓冲区，这本质也属于内存。



当生产者认为已经向RocketMQ成功写入了一条消息，但实际上该消息可能只是仅仅进入OS的PageCache，还没刷入磁盘。此时如果Broker机器宕机，那么OS的PageCache中的数据也就丢失了。



所以根据RocketMQ底层原理，消息数据在进入OS的PageCache时，如果碰上机器宕机，那么内存里的数据必然会丢失。此后机器即便重启了，并启动好Broker进程，那么这条消息数据也没了。

**
**

***\*(3)就算消息进入磁盘了也不是万无一失\****

当Broker把消息写入了OS的PageCache，操作系统会在一段时间后把消息从内存中刷入磁盘文件里。



但是即便写入RocketMQ的一条消息已经进入Broker机器的磁盘文件里了，那么这条消息也是有可能会丢失的。因为如果Broker机器的磁盘出现了故障，比如磁盘坏了，那么上面存储的数据就可能会丢失。

**
**

***\*(4)即使红包系统消费到消息也可能会丢失\****

即使红包系统顺利从RocketMQ里获取到一条消息，那么它也不一定能把现金红包发出去。offset表示的是消息的位置，代表了消息的标识。假设有两条消息，offset分别为1和2。现在红包系统已经获取到了消息1，然后消息1此时就在红包系统的内存里，正准备运行代码去派发现金红包(还没派发)。



由于默认情况下，RocketMQ的消费者会自动提交已经消费的offset。如果此时红包系统在还没处理完根据消息1派发红包的情况下，提交了消息1的offset到Broker，标识已成功处理了消息1。接着恰好红包系统突然重启或者宕机、或者在派发红包时更新数据库失败了。那么此时内存里的消息1必然会丢失，而且红包也还没发出去。

**
**

***\*(5)用户支付完后红包没发送出去的原因汇总\****

原因一：订单系统推送消息到RocketMQ失败了，消息没有被成功发送到RocketMQ上

原因二：消息确实推送到RocketMQ了，但是结果Broker机器故障，把消息弄丢了

原因三：红包系统拿到了消息，但是在红包还没发完的时候把消息弄丢了



如果订单系统推送了消息，结果红包系统连消息都没收到，那么可能消息根本就没发到RocketMQ，或者RocketMQ弄丢了消息。如果红包系统收到了消息，结果红包没派发，那么就是红包系统弄丢了消息。

**
**

***\*8.RocketMQ的事务消息机制实现发送消息零丢失\****

***\*(1)解决消息丢失的第一个问题：推送消息时丢失\****

首先要解决的第一个问题，就是在订单系统推送消息到RocketMQ的过程中，可能会因为常见的网络故障等问题导致推送消息失败。而在RocketMQ中，有一个事务消息机制。凭借这个事务消息机制，就可以确保订单系统推送的消息一定会成功写入RocketMQ，不会出现推送消息失败。

**
**

***\*(2)事务消息机制之发送half消息试探是否正常\****

订单系统收到一个订单支付成功的回调后，需要在自己的订单数据库里做一些增删改操作，比如更新订单状态等。然后发一条消息到RocketMQ，让其他关注这个订单支付成功消息的系统可以从RocketMQ获取消息做对应的处理。



在基于RocketMQ的事务消息机制中，首先会让订单系统发送一条half消息到RocketMQ中。这个half消息本质就是一个订单支付成功的消息，只不过该消息状态是half状态，此时红包系统是看不见该half消息的。然后订单系统会等待接收这个half消息写入成功的响应通知。



可以想象一下，假设订单系统在收到订单支付成功的通知后，直接进行本地的数据库操作，比如更新订单状态为已完成，然后再发送消息给RocketMQ，结果才发现RocketMQ挂了报异常。这时就会导致没法通过消息通知到红包系统去派发红包，那用户一定会发现自己订单支付了，结果红包没收到。



所以订单系统在收到订单支付成功的通知后做的第一件事，不是先让订单系统做一些增删改操作，而是先发一个half消息给RocketMQ以及等待它的成功响应，也就是初步和RocketMQ建立联系和沟通，从而让订单系统确认RocketMQ还活着，RocketMQ也会知道订单系统后续可能想发送一条很关键的不希望丢失的消息给它。

**
**

***\*(3)事务消息机制之如果half消息写入失败\****

如果订单系统发送half消息给RocketMQ失败了，可能因为订单系统报错了、可能因为RocketMQ挂了、或者网络故障了等原因导致half消息都没发送成功，此时订单系统应该执行一系列的回滚操作，比如对订单状态做一个更新，让状态变成"关闭交易"，同时通知支付系统自动进行退款。因为订单虽然支付了，但是包括派发红包、发送优惠券之类的后续操作无法执行，所以此时需要把钱款退还给用户，表示交易失败了。

**
**

***\*(4)事务消息机制之如果half消息写入成功\****

如果订单系统发送half消息给RocketMQ成功了，此时订单系统就应该在自己本地的数据库里执行一些增删改操作。因为一旦half消息写成功，就说明RocketMQ肯定已经收到了这条消息、RocketMQ还活着而且目前生产者可以跟RocketMQ正常沟通，所以订单系统下一步应该执行自己的增删改操作。

**
**

***\*(5)事务消息机制之如果本地事务执行失败\****

如果订单系统更新数据库失败了，比如数据库也出现网络异常、或者数据库挂了，那么订单系统就无法把订单更新为"已完成"这个状态。此时应该让订单系统发送一个rollback请求给RocketMQ，意思是RocketMQ可以把订单系统之前发过来的half消息删除掉。



订单系统发送rollback请求给RocketMQ删除half消息后，订单系统就必须执行回退流程，通知支付系统退款。当然回退流程中可能还要考虑订单系统自己的高可用降级机制：比如数据库无法更新时，订单系统需要在机器本地磁盘文件里写入订单支付失败的记录，然后订单系统会启动一个后台线程在MySQL数据库恢复后再把订单状态更新为"已关闭"。

**
**

***\*(6)事务消息机制之如果本地事务执行成功\****

如果订单系统成功完成了本地事务的操作，比如把订单状态都更新为"已完成"，那么订单系统就可以发送一个commit请求给RocketMQ，要求RocketMQ对之前的half消息进行commit操作，让红包系统可以看见这个订单支付成功消息。



所谓的half消息实际就是订单支付成功的消息，只不过它的状态是half。也就是当消息的状态是half状态时，红包系统是看不见该消息的，没法获取到该消息。必须等到订单系统执行commit请求，该half消息被commit后，红包系统才可以看到和获取该消息进行后续处理。

**
**

***\*(7)事务消息机制之没收到half消息成功的响应\****

如果订单系统把half消息发送给RocketMQ了，RocketMQ也把half消息保存下来了，但是订单系统却没能收到RocketMQ返回的响应，那么此时会发生什么事情？



订单系统没收到响应，可能是由于网络超时问题，也可能是由于其他的异常问题。如果订单系统没能收到RocketMQ返回half消息成功的响应，那么就会误以为发送half消息到RocketMQ失败了，从而执行退款流程，订单状态也会被标记为"已关闭"。



但此时RocketMQ已经存下来一条half消息了，那么对这条half消息应该怎么处理？其实RocketMQ里有一个补偿流程，它会扫描自己处于half状态的消息。如果订单系统一直没有对这个消息执行commit/rollback操作，超过一定时间，RocketMQ就会回调订单系统的一个接口。



RocketMQ会通过该接口询问订单系统：这个half消息到底怎么回事？到底是打算commit这个消息还是要rollback这个消息？这时订单系统的这个接口就会去查一下数据库，看看这个订单当前的状态，如果发现订单状态是“已关闭”，那么订单系统就要发送rollback请求给RocketMQ去删除那个half消息。

**
**

***\*(8)事务消息机制之rollback或commit发送失败\****

***\**\*场景一：\*\**\***如果订单系统收到了half消息写入成功的响应，同时尝试对自己的数据库更新了。然后根据失败或者成功去执行了rollback或者commit请求，发送给RocketMQ了。结果因为网络故障，导致rollback或者commit请求发送失败了。



这时候因为RocketMQ里的消息一直是half状态，所以RocketMQ过了一定的超时时间就会发现这个half消息有问题，于是就会回调订单系统的接口。然后订单系统的接口就可以判断一下订单状态是否为"已完成"，并决定执行commit请求还是rollback请求。因此这个回调就是一个补偿机制：如果订单系统没收到half消息的响应，或者rollback、commit请求没发送成功，RocketMQ都会来询问订单系统如何处理half消息。

**
**

***\**\*场景二：\*\**\***如果订单系统收到了half消息写入成功的响应，同时尝试对自己的数据库更新了。然后根据失败或者成功去执行了rollback或者commit请求，发送给RocketMQ了。但此时RocketMQ却挂掉了，导致rollback或者commit请求发送失败。



这时候就需要等RocketMQ自己重启，重启后它会扫描half状态的消息，然后还是通过补偿机制，回调订单系统的接口。

**
**

***\*(9)RocketMQ事务消息的全流程总结\****

***\**\*情况一：\*\**\***如果RocketMQ有问题或者网络有问题，half消息根本都发不出去。此时half消息肯定是失败的，那么订单系统就不会执行后续的流程。

**
**

***\**\*情况二：\*\**\***如果half消息发送出去了，但是half消息的响应没有收到，然后执行了退款流程。那么RocketMQ会有补偿机制来回调订单系统询问要commit还是rollback，此时订单系统选择rollback删除消息就可以了，不会执行后续流程。



***\**\*情况三：\*\**\***如果订单系统收到half消息的响应了，结果订单系统自己更新数据库失败了。那么它也会进行回滚，不会执行后续流程。

**
**

***\**\*情况四：\*\**\***如果订单系统收到half消息的响应了，然后还更新自己数据库成功了，订单状态是"已完成"。此时必然会发送commit请求给RocketMQ，一旦消息commit了，那么必然保证红包系统可以收到这个消息。而且即使commit请求发送失败了，RocketMQ也会有补偿机制，通过回调订单系统的接口来判断是否重新发送commit请求。



总之，只要订单系统本地事务成功了，那么必然会保证RocketMQ里的half消息被commit，从而让红包系统看到该消息。所以，通过RocketMQ的事务消息机制，可以保证订单系统一旦成功执行了数据库操作，就一定会通知到红包系统派发红包，至少订单系统到RocketMQ之间的消息发送不会出现消息丢失的问题。

**
**

***\*9.RocketMQ事务消息机制的底层实现原理\****

***\*(1)half消息是如何对消费者不可见的\****

前面介绍了RocketMQ事务消息的全流程，在这个流程中，第一步会由订单系统发送一个half消息给RocketMQ。对于这个half消息，红包系统刚开始是看不到它的，没法消费这条消息进行处理。那么这个half消息是如何做到不让红包系统看到的呢？这就涉及到RocketMQ底层采取的一个巧妙的设计了。



假设订单系统发送了一个half状态的订单支付消息到OrderPaySuccessTopic里，然后红包系统也订阅了这个OrderPaySuccessTopic从里面获取消息。根据前面RocketMQ的底层原理可知：向一个Topic写入消息，首先会定位这个Topic的某个MessageQueue，然后会定位一台Broker机器，接着会将消息写入到这个Broker机器的CommitLog文件，同时将消息偏移量写入到该Broker机器上的MessageQueue对应的ConsumeQueue文件。



如果要写入一条half消息到OrderPaySuccessTopic里，需要先定位到这个Topic的一个MessageQueue，然后定位到RocketMQ的一台Broker机器上，接着将half消息写入到该Broker机器上的CommitLog文件，同时消息的offset会写入该到Broker机器上的MessageQueue对应的ConsumeQueue，该ConsumeQueue是属于OrderPaySuccuessTopic的，最后红包系统才能从这个ConsumeQueue里获取到写入的这个half消息。



但实际上红包系统却没法看到这条half消息，原因是RocketMQ一旦发现生产者发送的是一个half消息，那么它就不会把这个half消息的offset写入OrderPaySuccessTopic的ConsumeQueue里，而是会把这条half消息写入到自己内部的RMQ_SYS_TRANS_HALF_TOPIC这个Topic对应的一个ConsumeQueue里。



所以对于事务消息机制下的half消息：RocketMQ是写入内部Topic的ConsumeQueue的，不是写入生产者指定的OrderPaySuccessTopic的ConsumeQueue的。因此红包系统自然就无法从OrderPaySuccessTopic的ConsumeQueue中看到这条half消息了。

**
**

***\*(2)订单系统何时会收到half消息成功的响应\****

在什么情况下订单系统会收到half消息成功的响应呢？简单来说，必须要half消息进入到RocketMQ内部的RMQ_SYS_TRANS_HALF_TOPIC的ConsumeQueue文件了，此时才会认为half消息写入成功，然后才会返回响应给订单系统。所以一旦订单系统收到half消息写入成功的响应，那就代表着这个half消息已经在RocketMQ内部了。

**
**

***\*(3)如果没有执行rollback或commit会怎样\****

如果因为网络故障，订单系统没收到half消息的响应，或者发送的rollback/commit请求失败了，那么RocketMQ会怎么处理呢？其实RocketMQ会有一个定时任务，定时扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息。如果这些消息超过一定时间还是half消息，就会回调订单系统的接口来判断这个half消息是要rollback还是commit。

**
**

***\*(4)处理rollback请求时如何标记消息回滚\****

假设订单系统发送了rollback请求，那么RocketMQ就需要对消息进行回滚。RocketMQ会删除对应的half消息，但并不是在磁盘文件里删除。RocketMQ内部有一个OP_TOPIC，在处理half消息的rollback请求时，会向这个Topic写入一条OP记录，标记这个half消息为rollback状态。如果订单系统一直没有执行commit/rollback，RocketMQ会回调订单系统的接口去判断half消息的状态。但是RocketMQ最多回调15次，如果15次之后订单系统都没法告知half消息的状态，就自动把half消息标记为rollback状态。

**
**

***\*(5)处理commit请求时如何让消息可见\****

假设订单系统发送了commit请求，那么RocketMQ需要让消息可见。RocketMQ在处理half消息的commit请求时，首先会在OP_TOPIC里写入一条OP记录，然后标记这条half消息为commit状态，接着会把RMQ_SYS_TRANS_HALF_TOPIC中的half消息写入到OrderPaySuccessTopic的ConsumeQueue里，这样红包系统才可以看到这条消息并进行消费。

**
**

***\*10.是否可以通过同步重试方案来代替事务消息方案来实现发送消息零丢失\****

***\*(1)是否有简单方法确保消息可以到达RocketMQ\****

生产者在发送消息时，可能会存在消息丢失的情况，也就是可能消息根本就没有进入到RocketMQ就丢了。如果生产者使用事务消息机制去发送消息到RocketMQ，那么一定可以保证消息发送到RocketMQ。但根据事务消息机制的原理，发现其流程有点复杂：需要先发送half消息，之后还得发送rollback或commit请求，要是中间有点什么问题，RocketMQ还得回调生产者的接口。



那么是否真的有必要使用这么复杂的机制去确保消息到达RocketMQ且不会丢失呢？毕竟这么复杂的机制完全有可能导致整体性能比较差，而且吞吐量比较低。是否有更加简单的方法来确保消息一定可以到达RocketMQ呢？

**
**

***\*(2)能不能基于重试机制来确保消息到达RocketMQ\****

生产者发送消息给RocketMQ时，是可以等待RocketMQ返回响应给生产者的。那么在什么样的情况下，RocketMQ会返回响应给生产者呢？事实上，只要RocketMQ将消息写入了自己的本地存储，就可以返回响应给生产者。



所以只要生产者在发送消息到RocketMQ后，同步等待RocketMQ返回的响应，也可以确保消息一定会到达RocketMQ。如果期间有网络异常或者RocketMQ内部异常，生产者肯定会收到异常响应，比如网络错误或者请求超时等。如果生产者收到了异常响应，那么就可以认为消息发送到RocketMQ失败了，然后再次尝试重新发送消息，再次同步等待RocketMQ返回的响应。通过反复重试，最终也可以确保消息一定会到达RocketMQ。



理论上在一些短暂的网络异常场景下，生产者是可以通过不停的重试去保证消息到达RocketMQ的。因为如果短时间网络异常了消息一直没法发送，只要不停重试，那么当网络恢复后，消息就可以发送到RocketMQ。如果反复重试多次都没法把消息投递到RocketMQ，此时就可以直接让订单系统回滚之前的流程。比如发起退款流程，判定本次订单支付交易失败。所以这个简单的同步发送消息 + 反复重试的方案，也可以保证消息成功投递到RocketMQ中。



在基于Kafka作为消息中间件的发送消息零丢失方案中，因为Kafka本身不具备RocketMQ这种事务消息的高级功能，所以一般都会采用同步发送消息 + 反复重试的方案，保证消息成功投递到Kafka中。



但是在类似这个较为复杂的订单业务场景中，仅仅采用同步发送消息 + 反复重试的方案，来确保消息成功投递到RocketMQ中，似乎还是不够。下面分析一下在复杂业务场景下，这种方案会有什么问题。

**
**

***\*(3)先执行订单本地事务还是先发消息到RocketMQ\****

如果订单系统先执行订单本地事务，接着再发送消息到RocketMQ，那么伪代码如下所示：

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
try {    //执行订单本地事务    orderService.finishOrderPay();    //发送消息到MQ去    producer.sendMessage();} catch (Exception e) {    //如果发送消息失败了，进行重试    for (int i=0; i<3; i++) {        //重试发送消息    }    //如果多次重试发送消息后，还是不行，则回滚本地订单事务    orderService.rollbackOrderPay();}
```

上述代码看着天衣无缝，先执行订单本地事务，接着发送消息到RocketMQ。如果订单本地事务执行失败了，则不会继续发送消息到RocketMQ。如果订单事务执行成功，但发送消息失败了，则自动进行几次重试，如果重试一直失败，就回滚订单事务。



但是有一个问题：假设订单系统刚执行完成订单本地事务，结果还没等订单系统发送消息到RocketMQ，订单系统却突然崩溃了。这就会导致订单状态可能已经修改为"已完成"，但是消息却没发送到RocketMQ，这就是这个方案最大的隐患。



如果出现这种场景，那么多次重试发送消息的代码根本没机会执行。而且订单本地事务已经执行成功了，但消息没发送出去，红包系统没机会派发红包。必然导致用户支付成功了，结果看不到自己的红包。

**
**

***\*(4)如果把订单本地事务代码和重试发送RocketMQ消息的代码放到一个事务中\****

伪代码如下所示：

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
//直接在方法上加入事务注解@Transactionalpublic void payOrderSuccess() {    try {        //执行订单本地事务        orderService.finishOrderPay();        //发送消息到MQ去        producer.sendMessage();    } catch (Exception e) {        //如果发送消息失败了，进行重试        for (int i=0; i<3; i++) {            //重试发送消息        }        //如果多次重试发送消息后，还是不行，则回滚本地订单事务        throw new XXXException();        }}
```

上述代码看起来解决了面临的问题，就是在这个方法上加入事务。在这个事务方法中：哪怕执行了orderService.finishOrderPay()，但其实也只是执行一些增删改SQL语句，还没提交订单本地事务。如果发送消息到RocketMQ失败了，而且多次重试还不行，则在抛出异常后会自动回滚订单本地事务。如果刚执行orderService.finishOrderPay()，结果订单系统直接崩溃，此时订单本地事务也会被回滚，因为根本没提交过。



但是这个方案还是非常不理想，原因就出在多次重试的地方。如果用户支付成功了，然后支付系统回调通知订单系统，有一笔订单已经支付成功。这时订单系统卡在多次重试的代码里，可能耗时好几秒种，此时发起回调通知的支付系统早就等不及可能都超时异常了。而且把重试的代码放在这个逻辑里，会导致订单系统的这个接口性能很差。

**
**

***\*(5)订单系统就一定可以依靠本地事务回滚吗\****

如果将订单事务和发送消息到RocketMQ包裹在一个事务代码中，依靠本地事务回滚，那么除了多次重试导致的超时异常问题外，还会有其他问题。伪代码如下所示：

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
//直接在方法上加入事务注解@Transactionalpublic void payOrderSuccess() {    try {        //执行订单本地事务        orderService.finishOrderPay();        //更新Redis缓存         orderService.updateRedisCache();        //更新Elasticsearch数据        orderService.updateEsData();        //发送消息到MQ去        producer.sendMessage();    } catch (Exception e) {        //如果发送消息失败了，进行重试        for (int i=0; i<3; i++) {            //重试发送消息        }        //如果多次重试发送消息后，还是不行，则回滚本地订单事务        throw new XXXException();        }}
```

上述代码中，虽然在方法上加了事务注解，但是代码里还有更新Redis缓存和Elasticsearch数据的代码逻辑。如果已经完成了订单数据库更新、Redis缓存更新、ES数据更新，结果没法送MQ订单系统就崩溃了。虽然此时订单数据库的操作会回滚，但是Redis、Elasticsearch中的数据更新就不会自动回滚了。而且它们也根本没法自动回滚，此时数据还是会不一致。所以，完全寄希望于本地事务自动回滚是不现实的。

**
**

***\*(6)保证业务系统一致性的最佳方案是使用RocketMQ的事务消息机制\****

所以分析完这个同步发送消息 + 反复重试的方案后，会发现该方案落会存在一些问题。

***\**\*问题一：\*\**\***订单事务执行成功，但消息没发送出去

***\**\*问题二：\*\**\***订单事务执行成功，但反复重试发送消息到RocketMQ极为耗时，导致调用该接口的系统超时

***\**\*问题三：\*\**\***利用本地事务回滚，发生异常时只能自动回滚数据库部分的操作



所以真正要保证消息一定投递到RocketMQ，同时保证业务系统之间的数据完全一致，业内最佳方案还是用基于RocketMQ的事务消息机制。因为这个方案落地后，就能保证订单系统的本地事务一旦成功，那么必然会投递消息到RocketMQ。而且整个流程中，订单系统也不会进行长时间的阻塞和重试。



如果half消息发送失败，就直接回滚整个流程。如果half消息发送成功，后续的rollback或者commit发送失败了，订单系统也不需要阻塞在那里反复重试，直接让代码结束即可，因为之后RocketMQ会回调订单系统的接口来判断是rollback还是commit。此外，我们也可以借鉴RocketMQ事务消息机制的实现原理，来实现同时向多个不同业务系统写同一数据时的数据一致性。

**
**

***\*11.同步刷盘 + Raft协议同步实现发送消息零丢失\****

***\*(1)使用事务消息机制就一定不会丢消息吗\****

RocketMQ的事务消息机制是RocketMQ非常核心以及重要的一个功能，该功能可以实现在生产消息的环节不丢失数据，而且最重要的是，可以保证两个业务系统的数据一致性。但是即使在生产消息时用了事务消息机制，也未必就真的可以保证数据不丢失。



假设现在订单系统已经通过事务消息的机制，通过half消息 + commit的方式，在RocketMQ里提交了消息。也就是对于RocketMQ而言，那条消息已经进入到它的存储层了，可以被红包系统看到了。



生产者生产的这条消息在commit之后，会从内部的TRANS_HALF_TOPIC进入生产者的OrderPaySuccessTopic中。但是这条消息此时仅仅是进入生产者指定的Topic而已，仅仅是可以被红包系统看到而已，此时红包系统可能还没来得及去获取这条消息。



然而恰好在此时，这条消息还停留在OS的PageCache中，还没进入到ConsumeQueue磁盘文件里，然后这台Broker机器突然宕机了，OS的PageCache中的数据全部丢失。从而导致这条消息也会丢失，红包系统再也没机会读到这条消息了。

**
**

***\*(2)消息进了磁盘就不会丢了吗\****

即使这条消息已经进入了OrderPaySuccessTopic的ConsumeQueue磁盘文件了，不只是停留在OS的PageCache里了，此时消息也未必一定不会丢失。



即使消息已经进入磁盘文件，但是这个时候红包系统还没来得及消费这条消息，然后此时这台机器的PageCache已经没有了这条消息，同时磁盘突然坏了。这样也一样会导致消息丢失，而且这条消息可能再也找不回来了。

**
**

***\*(3)保证消息写入MQ不代表不丢失\****

所以需要明确一个前提：无论是通过比较简单的同步发送消息 + 反复重试的方案，还是事务消息机制的方案，哪怕已经确保消息成功写入了RocketMQ，此时消息也未必就不会丢失。



因为即使写入RocketMQ成功，这条消息也大概率是还停留在OS的PageCache中，一旦RocketMQ机器宕机，其内存里的数据也都会丢失。甚至哪怕消息已经进入了RocketMQ机器的磁盘文件，一旦磁盘坏了，消息也同样可能会丢失。如果消息丢失了，消费者还没来得及消费，那么该消息就永远没机会被消费了。

**
**

***\*(4)异步刷盘 vs 同步刷盘\****

所以到底怎么去确保消息写入RocketMQ后，RocketMQ自己不会丢失数据呢？解决这个问题的第一个关键点，就是将异步刷盘调整为同步刷盘。



所谓的异步刷盘指的是：消息即使成功写入了RocketMQ，它也只是在机器的OS PageCache中，还没有进入磁盘里，要过一会儿等操作系统自己把PageCache里的数据刷入磁盘文件中。



所以在异步刷盘的模式下，消息写入的吞吐量肯定是极高的，毕竟消息只要进入OS 的PageCache这个内存就可以了。写消息的性能就是写内存的性能，但是这个情况下可能就会有数据丢失的风险。



因此如果一定要确保数据零丢失，可以调整RocketMQ的刷盘策略为同步刷盘。需要调整Broker的配置文件，将flushDiskType参数的值设置为SYNC_FLUSH，flushDiskType参数的默认值是ASYNC_FLUSH，即异步刷盘。



如果调整为同步刷盘后，写入RocketMQ的每条消息，只要RocketMQ返回写入成功，那么消息就一定是已进入磁盘文件。比如发送half消息时，只要RocketMQ返回响应就是half消息发送成功了，那么就说明消息已经进入磁盘文件。



所以如果使用同步刷盘的策略，是可以确保写入RocketMQ的消息一定是已经进入磁盘文件的。

**
**

***\*(5)通过主从架构避免磁盘故障导致数据丢失\****

如何避免磁盘故障导致的数据丢失？其实解决方法很简单，就是对Broker使用主从架构的模式。也就一个Master Broker必须要有一个Slave Broker去同步它的数据。而且Master Broker中的一条消息写入成功，Slave Broker也必须是写入成功，保证数据有多个副本的冗余。



这样一来，一条消息只要写入成功了，那么主从两个Broker上就会都有这条数据。此时如果Master Broker的磁盘坏了，但是Slave Broker上至少还是有数据的，数据不会因为磁盘故障而丢失。



Broker的主从同步架构，一般是基于DLedger技术和Raft协议实现的。所以如果采用了基于DLedger技术和Raft协议的主从同步架构，那么对于所有的消息写入，只要写入成功，就一定会通过Raft协议同步给其他的Broker机器。

**
**

***\*(6)RocketMQ确保数据零丢失的方案总结\****

根据以上分析可知：只要把Broker的刷盘策略调整为同步刷盘，那么就绝对不会因为机器宕机而丢失数据。只要采用了基于DLedger技术和Raft协议的主从架构的Broker集群，那么一条消息写入成功，就意味着多个Broker机器都写入了，此时任何一台机器的磁盘故障，数据也是不会丢失的。这样，只要Broker层面保证写入的数据不丢失，后续就一定可以让消费者消费到这条消息。

**
**

***\*12.手动提交offset + 自动故障转移的消费消息零丢失\****

***\*(1)红包系统拿到了消息就一定会派发红包吗\****

根据前面介绍，现在已经知道：如何确保订单系统发送出去的消息一定会到达RocketMQ，如何确保到达RocketMQ的消息一定不会丢失。只要能做到：对half消息rollback或commit + OS的PageCache同步刷盘 + Broker主从数据同步，那么必然可以保证红包系统可以获取到一条订单支付成功的消息，然后一定可以去尝试把红包派发出去。



但现在的问题在于：即使红包系统拿到了消息，也未必一定可以成功派发红包。因为如果红包系统已经拿到了一条消息，但消息目前还在它的内存里，还没执行派发红包的逻辑。此时它就直接提交了这条消息的offset到Broker上，告知Broker自己已经处理过该消息了。接着红包系统此时才突然崩溃，内存里的消息于是就没了，红包也没派发出去。但Broker已经收到它提交的消息offset了，认为它已经处理完这条消息了。等红包系统重启时，就不会再次消费到这条消息了。



因此需要明确的就是：即使可以保证发送消息到RocketMQ时绝对不会丢失，而且RocketMQ收到消息后一定不会发生丢失，但是红包系统在获取到消息后还是可能会发生丢失。

**
**

***\*(2)Kafka消费者的数据丢失问题\****

RocketMQ中的消费者数据丢失问题，也完全可以套用到Kafka中。Kafka的消费者采用的消费的方式跟RocketMQ是有些不一样的，如果按照Kafka的消费模式，就是会存在数据丢失的风险。



也就是说Kafka消费者可能会出现：拿到一批消息，还没来得及处理，结果就提交offset到Broker去了。之后消费者系统就宕机了，这批消息就再也没机会处理了，因为它重启之后已经不会再次获取提交过offset的消息了。

**
**

***\*(3)RocketMQ消费者的与众不同的地方\****

对于RocketMQ的消费者而言，它有一些与众不同的地方，至少跟Kafka的消费者是有较大不同的。



先来看RocketMQ消费者的代码，如下所示：

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
public class RocketMQConsumer {    public static void start() {        new Thread() {            public void run() {                //这是RocketMQ消费者实例对象                //"credit_group"之类的就是消费者分组                //一般来说积分系统就用"credit_consumer_group"，营销系统就用"marketing_consumer_group"                //以此类推，不同的系统给自己取不同的消费组名字                DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("credit_group");                //这是给消费者设置NameServer的地址                //这样就可以拉取路由信息，知道Topic的数据在哪些Broker上，然后就可以从对应的Broker上拉取数据                consumer.setNamesrvAddr("localhost:9876");                //选择订阅"TopicOrderPaySuccess"的消息                //这样会从这个Topic的Broker机器上拉取订单消息过来                consumer.subscribe("TopicOrderPaySuccess");                                //注册消息监听器来处理拉取到的订单消息                //如果consumer拉取到订单消息，就会回调这个方法进行处理                consumer.registerMessageListener(new MessageListenerConcurrently() {                    @Override                    public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {                        //在这里对获取到的msgs订单消息进行处理                        //比如增加积分、发送优惠券、通知发货等                        return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;                    }                });
                //启动消费组实例                 consumer.start();                System.out.printf("Consumer Started.%n");                                //别让线程退出，就让创建好的consumer不停消费数据                while(true) {                    Thread.sleep(1000);                }            }        }.start();    }}
```

再来看上述代码中的一小块代码：

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
consumer.registerMessageListener(    new MessageListenerConcurrently() {        @Override        public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {            //在这里对获取到的msgs订单消息进行处理             //比如增加积分、发送优惠券、通知发货等            return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;                }    });
```

可以看见，RocketMQ的消费者会注册一个监听器，这个监听器名为MessageListenerConcurrently。

**
**

***\**\*情况一：\*\**\***当消费者获取到一批消息后，就会回调这个监听器函数，让它来处理这一批消息。然后处理完毕后，才会向RocketMQ返回CONSUME_SUCCESS表示消费成功。也就是告诉RocketMQ，这批消息已经处理完毕了。所以对于RocketMQ而言，红包系统首先会在这个监听器的函数中处理一批消息，然后返回消费成功的状态，接着才会去提交这批消息的offset到Broker，此时即使消费者系统崩溃了，消息也不会丢失，因为都已经消费完了。

**
**

***\**\*情况二：\*\**\***如果红包系统获取到一批消息后，还没处理完。也就是还没返回CONSUME_SUCCESS这个状态，还没提交这批消息的offset给Broker。此时红包系统突然挂了，会怎么样？在这种情况下，由于消费者还没有提交这批消息的offset给Broker，所以Broker是不会认为消费者已经处理完这批消息的。此时红包系统的一台机器宕机，Broker会感知到红包系统的一台机器作为一个Consumer挂了。接着Broker就会把宕机机器没处理完的那批消息交给红包系统的其他机器去进行处理。因此在这种情况下，消息也是不会丢失的。

**
**

***\*(4)需要注意的地方是不能异步消费消息\****

所以在默认的消费模式下：必须是处理完一批消息了，才能返回CONSUME_SUCCESS状态，标识消息处理结束，接着才能提交offset到Broker。在这种情况下，是不会丢失消息的。即使一个Consumer宕机，Broker也会把没处理完的消息交给其他Consumer处理。



但是需要注意的一点就是：不能在代码中对消息进行异步处理。如下便是错误的示范，开启了一个子线程去处理这批消息，然后启动线程后就直接返回CONSUME_SUCCESS状态了。

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
consumer.registerMessageListener(    new MessageListenerConcurrently() {        @Override        public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {            //开启一个子线程处理这批消息            new Thread() {                public void run() {                    //在这里对获取到的msgs订单消息进行处理                     //比如增加积分、发送优惠券、通知发货等                }            }.start();            return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;        }    });
```

如果使用上述这种方式来处理消息的话，那么可能就会出现开启的子线程还没处理完消息，消费者已经返回CONSUME_SUCCESS状态了。于是就可能提交这批消息的offset给Broker了，让Broker认为消费者已经处理结束了。如果此时红包系统突然宕机，必然会导致消息丢失。



因此如果要保证消费过程中不丢消息，那么就需要在回调函数里同步处理消息，处理完再让消费者返回CONSUME_SUCCESS状态表明消息已处理完毕。



如果同步处理消息比较耗时，可能会造成消息积压，导致RocketMQ的消费性能大幅下降(消息积压导致读内存变为读磁盘)。那么可以先将消息存入数据库甚至OS PageCache，后续异步处理消息成功后，再手动提交消息的offset给Broker。

**
**

***\*13.基于RocketMQ全链路的消息零丢失方案总结\****

***\*(1)对全链路消息零丢失方案进行总结\****

***\**\*发送消息到RocketMQ的零丢失：\*\**\***方案一：同步发送消息 + 反复重试。方案二：事务消息机制。两者都有保证消息发送零丢失的效果，但是经过分析，事务消息方案整体会更好一些。

**
**

***\*RocketMQ收到消息之后的零丢失：\****开启同步刷盘策略 + 主从架构同步机制。只要让一个Broker收到消息后同步写入磁盘，同时同步复制给其他Broker，然后再返回响应给生产者表示写入成功，那么就可以保证RocketMQ自己不会丢失消息。

**
**

***\*消费消息的零丢失：\****RocketMQ的消费者可以保证处理完消息后，才会提交消息的offset给Broker，所以只要注意避免采用多线程异步处理消息时提前提交offset即可。如果想要保证在一条消息基于RocketMQ流转时绝对不会丢失，那么可以采取上述一整套方案。

**
**

**(2)消息零丢失方案的优势与劣势**

优势就是：可以让系统的数据都是正确的，不会丢失数据。劣势就是：会让消息在流转链路中的性能大幅度下降，让消息生产和消费的吞吐量大幅度下降。

**
**

**(3)消息零丢失方案会导致吞吐量大幅度下降**

在发送消息到RocketMQ的环节中，如果生产者仅仅只是简单的把消息发送到RocketMQ。那么不过就是一次普通的网络请求罢了，生产者发送请求到RocketMQ然后接收返回的响应。这个性能自然很高，吞吐量也是很高的。



如果生产者改成了基于事务消息的机制之后，那么此时实现原理如下图示，会涉及到half消息、commit or rollback、写入内部Topic、回调机制等诸多复杂的环节。生产者光是成功发送一条消息，至少要half + commit两次请求。所以当生产者一旦上了如此复杂的方案之后，势必会导致生产者发送消息的性能大幅度下降，从而导致发送消息到RocketMQ的吞吐量大幅度下降。



当Broker收到消息后，一样会让性能大幅度下降。首先RocketMQ的一台Broker机器收到消息后，会直接把消息刷入磁盘，这个性能就远远低于直接写入OS PageCache的性能。写入OS的PageCache相当于是写内存，可能仅仅需要0.1ms，但是写入磁盘文件可能就需要10ms。接着这台Broker还需要把消息复制给其他Broker完成多副本的冗余。这个过程涉及到两台Broker机器之间的网络通信 + 另外一台Broker机器需要写数据到自己本地磁盘，同样会比较慢。在Broker完成了上述两个步骤后，接着才能返回响应告诉生产者这次消息写入已经成功。



由此可见，写入一条消息需要强制同步刷磁盘，而且还需要同步复制消息给其他Broker机器。这两个步骤可能就让原本只要10ms完成的变成100ms完成了。所以也势必会导致性能和吞吐量大幅下降。



当消费者拿到消息之后，比如开启一个子线程去处理这批消息，然后就直接返回CONSUME_SUCCESS状态，接着就可以去处理下一批消息了。如果这样的话，该消费者消费消息的速度会很快，吞吐量也会很高。但为了保证数据不丢失，消费者必须在处理完一批消息后再返回CONSUME_SUCCESS状态。那么消费者处理消息的速度就会降低，吞吐量自然会下降。

**
**

**(4)消息零丢失方案到底适合什么场景**

所以如果系统一定要使用消息零丢失方案，那么必然导致从头到尾的性能下降以及吞吐量下降，因此一般不要轻易在一个业务里使用如此重的一套方案。一般来说，与金钱、交易以及核心数据相关的系统和核心链路，可以使用这套消息零丢失方案。



对于非常核心的场景和少数核心链路的系统，才会建议使用这套复杂的消息零丢失方案。而对于其他大部分非核心的场景和系统，其实即使丢失一些数据，也不会导致太大的问题。此时可以不采取这套方案，或者可以在某些地方做一些简化。



比如可以把事务消息方案退化成同步发送消息 + 反复重试的方案。如果发送消息失败，就重试几次，但是大部分时候可能不需要重试，那么也不会轻易的丢失消息的。最多在该方案里，可能会出现一些数据不一致的问题，因为生产者可能在发送消息前宕机导致没法重试但本地事务已执行。



比如可以把Broker的刷盘策略改为异步刷盘 + 但使用一套主从架构。这样即使一台机器挂了，OS PageCache里的数据丢失了，其他机器上还有数据。不过大部分时候Broker不会随便宕机，那么异步刷盘策略下性能还是很高的。

**
**

***\*14.消息重复 + 处理失败 + 乱序 + 过滤等问题\****

***\*(1)引入幂等性机制来保证数据不会重复\****

一般来说，可以往RocketMQ里重复发送一样的消息。因为RocketMQ里有多条重复消息，不会对系统的核心数据直接造成影响。但关键要保证的是，消费者从RocketMQ获取消息进行处理时，消息不能被重复处理。



为了保证消息的幂等性，优先推荐的还是业务判断法。直接根据数据库存储中的记录来判断这个消息是否处理过，如果处理过了，那就别再次处理了。因为已经知道，基于Redis的消息发送状态的方案，在一些极端情况下还是没法100%保证幂等性的。

**
**

***\*(2)消息处理失败场景下的方案处理\****

如果消费者依赖的一些系统可能有故障，比如数据库宕机、缓存宕机等，此时消费者没办法完成消息的处理，那么可以通过一些返回状态去让消息进入RocketMQ自带的重试队列。同时如果反复重试还是不行，可以让消息进入RocketMQ自带的死信队列，后续再单独针对死信队列中的消息进行消费处理。

**
**

***\*(3)消息乱序问题的处理方案\****

在订单数据库的同步过程中，产生消息乱序问题的根本原因是：属于同一个订单的binlog进入了不同的MessageQueue，从而导致一个订单的binlog被不同机器上的Consumer获取来处理，最终导致这一个订单的binlog被乱序执行。



所以要解决消息乱序的问题，方法就是让同一个订单的binlog进入同一个MessageQueue里。具体的做法就是往RocketMQ发送binlog时根据订单ID对MessageQueue的数量进行取模来选择一个MessageQueue。



比如有一个订单ID是10，它有2条binlog，对这两条binlog，用订单ID=10对MessageQueue的数量进行取模。如果MessageQueue一共有8个，那么此时订单ID=10对8取模就是2。也就是说，凡是订单ID=10的binlog，都应该进入位置为2的MessageQueue中。通过这个方法，就可以让一个订单的binlog都按顺序进入同一个MessageQueue中。



由于一个Consumer可以处理多个MessageQueue的消息，但是一个MessageQueue只能交给同一个Consumer来处理，所以同一个订单的binlog有序地进入同一个MessageQueue后，会有序地交给同一个Consumer来处理。



这样一个大数据系统就可以获取到同一个订单的有序的binlog，然后根据binlog有序地把数据还原到自己的存储中去。当然，大数据系统在处理消息时，需要注意是否使用了多线程来打乱消息处理的顺序。



此外，为了保证消息有序，如果消息处理失败，就必须返回SUSPEND_CURRENT_QUEUE_A_MOMENT状态，该状态的意思是先等一会儿再继续处理这批消息。而不能把这批消息放入重试队列，然后直接处理下一批消息。

**
**

***\**\*总结：\*\**\***如果消费者一定要求消息是有序的，那么必须要让：同一个订单的binlog都进入同一个MessageQueue中 + Cannel获取和发送binlog时要有序 + binlog写入MessageQueue时要有序 + 消费者处理消息时要有序 + 消息处理失败时不能进入重试队列而要暂停等待继续处理。

**
**

***\*(4)基于RocketMQ的数据过滤机制提升处理效率\****

首先一个数据库中可能会包含很多表的数据。比如订单数据库，它里面除了订单信息表以外，可能还包含很多其他的表。所以在进行数据库binlog同步的时候，很可能是把一个数据库里所有表的binlog都推送到RocketMQ里去的。因此在RocketMQ的某个Topic中，可能混杂了订单数据库里几个甚至十几个表的binlog数据，不一定只包含大数据系统想要的表的binlog数据。



假设大数据系统仅仅关注订单数据库中的表A的binlog，并不关注其他表的binlog。那么大数据系统可能需要在获取到所有表的binlog后，对每条binlog都判断一下是否是表A的binlog。



如果不是表A的binlog，那么就直接丢弃不要处理。如果是表A的binlog，才会去进行处理。但是这样的话，必然会导致大数据系统处理很多不关注的表的binlog，从而浪费时间并降低处理消息的效率。



针对这个问题，可以采用RocketMQ的数据过滤机制，让大数据系统只关注它要的表的binlog数据。首先在发送消息时，给每条消息设置Tag和属性，然后在消费消息时，根据每条消息的Tag值来过滤，或者根据每条消息的属性值来过滤。



**总结：**在使用RocketMQ时，如果RocketMQ里混杂了大量的数据，可能Consumer只对其中一部分数据感兴趣，那么就可以在Consumer端使用Tag等数据过滤语法，过滤出消费者自己感兴趣的数据来消费。

**
**

***\*15.RocketMQ的生产实践经验总结\****

***\*(1)灵活运用Tags来过滤数据\****

在生产项目中，建议合理地规划Topic和里面的Tags。一个Topic代表了一类业务数据，对于这类业务数据，如果希望继续划分一些类别，那么可以在发送消息时设置Tags。比如常见的外卖平台有A、B、C几种，现在某系统要发送外卖订单消息到MQ，那么不同类型的外卖订单消息可以设置不同的Tags。然后消费外卖订单消息的系统，如果只需要某一种外卖类型的订单消息，就可以根据Tags来进行筛选。

**
**

***\*(2)基于消息Key来定位消息是否丢失\****

在消息零丢失方案中，需要解决的是消息是否丢失的问题。那么如果消息真的丢失了，应该如何进行排查？可不可以在RocketMQ里查一下：某条消息是否真的丢失了？其实可以基于消息Key来实现。



比如通过设置一个消息的Key为订单ID，这样这个消息就具备一个Key了。接着这个消息被发送到Broker时，会基于Key构建Hash索引，这个Hash索引就存放在IndexFile索引文件里。然后后续就可以通过RocketMQ提供的命令根据Key来查询这个消息。

**
**

***\*(3)消息零丢失方案的补充\****

在消息零丢失的方案中其实还有一个问题：就是RocketMQ集群彻底故障了，此时集群都不可用了，那么应该怎么办？对于一些金融级的系统，或者与钱相关的支付系统、广告系统等，都必须要有高可用保障机制。如果RocketMQ集群彻底崩溃了，那么生产者就应该把消息写入到本地磁盘文件或者数据库进行持久化，等RocketMQ集群恢复后再将持久化的消息投递到RocketMQ里。

**
**

***\*(4)提高消费者的吞吐量\****

如果消费者消费消息比较慢，那么可能会造成RocketMQ中的消息积压，甚至导致Broker读消息从内存读变为磁盘读，严重影响读性能。所以需要注意提高消费者的吞吐量，尽量避免RocketMQ中的消息积压。常见的做法如下：

**
**

***\**\*部署更多的Consumer机器：\*\**\***部署更多的Consumer机器时，Topic的MessageQueue也要对应增加。因为如果Consumer机器有5台，而MessageQueue只有4个，那意味着有一个Consumer机器是获取不到消息的，一个MessageQueue只会给一台Consumer机器去消费。

**
**

***\**\*增加Consumer的线程数量：\*\**\***可以设置Consumer的参数consumeThreadMin和consumeThreadMax，这样一台Consumer机器上的消费线程越多，消费的速度就越快。

**
**

***\**\*开启消费者的批量消费功能：\*\**\***也就是设置consumeMessageBatchMaxSize参数，该参数默认是1。可以设置该参数的值多一些，这样RocketMQ一次就会把一批消息交给消费者的回调函数进行处理。通过批量处理消息的方式，也可以大幅度提升消息消费的速度。

**
**

***\*(5)要不要消费历史消息\****

Consumer支持设置从哪里开始消费消息，常见的设置有两种：一是从Topic的第一条数据开始消费：CONSUME_FROM_LAST_OFFSET，二是从最后一次消费过的消息之后开始消费：CONSUME_FROM_FIRST_OFFSET。



一般会选择CONSUME_FROM_FIRST_OFFSET，这样消费者就会从Topic的第一条消息开始消费。但以后每次重启，消费者都会从上一次消费到的位置继续往后进行消费。

**
**

***\*16.如何处理RocketMQ的百万消息积压问题\****

***\*(1)直接丢弃消息来解决消息积压问题\****

如果这些消息是允许丢失的，那么此时可以紧急修改消费者的代码：在代码里对所有获取到的消息直接丢弃，不做任何处理。这样可以迅速让积压在RocketMQ里的百万消息被处理掉，只不过处理方式就是全部丢弃而已。

**
**

***\*(2)在旧Topic上扩容消费者来解决消息积压问题\****

如果这些消息是不允许丢失的，那么可以先等待消费者依赖的NoSQL数据库恢复，恢复后就可以根据线上Topic的MessageQueue数量来决定如何处理。



假设线上Topic有20个MessageQueue，然后只有4个消费者在消费，那么每个消费者会从5个MessageQueue里获取消息。此时如果仅仅依靠4个消费者来消费肯定是会继续积压消息的，毕竟RocketMQ里已经积压了百万消息了。



所以此时可以临时申请16台机器多部署16个消费者实例，然后让20个消费者同时消费。每个消费者消费一个MessageQueue的消息，此时消费的速度会提高5倍，积压的百万消息很快就会被处理完。



但是这里需要考虑消费者依赖的NoSQL数据库必须要能抗住临时增加5倍的读写压力，因为原来只有4个消费者在读写NoSQL，现在临时变成了20个消费者了。当处理完百万积压的消息后，就可以下线多余的16台机器了。



这是最常见的处理百万消息积压的办法。

**
**

***\*(3)通过新Topic扩容消费者来解决消息积压问题\****

如果Topic总共就只有4个MessageQueue，然后只有4个消费者呢？这时就没办法扩容消费者了，因为加再多的消费者，还是只有4个MessageQueue，没法降低原来消费者的消费压力了。



所以此时需要临时修改那4个消费者的代码，让它们获取到消息后不依赖NoSQL，直接把消息写入一个新的Topic，这时候的速度是很快的，因为仅仅是读写RocketMQ而已。然后新的Topic会有20个MessageQueue，于是部署20台临时增加的消费者去消费新的Topic，消费新的Topic时才依赖NoSQL。通过将积压的消息转移到一个新的Topic，来解决无法扩容消费者的问题。等到消息不积压之后，再恢复原来的消费者代码去依赖NoSQL。又或者修改生产者和消费者代码采用新的Topic，然后旧的Topic就用新增的临时机器来慢慢处理。

**
**

***\*(4)消息积压问题的处理总结\****

如果MessageQueue比较多，可以直接扩容消费者，那么就直接临时增加消费者实例来扩容消费者。如果MessageQueue比较少，不能直接扩容消费者，那么就把积压在原Topic的消息写入到新Topic。在消费新Topic的消息时，临时部署足够多的消费者实例，来实现间接扩容消费者。

**
**

***\*17.为RocketMQ增加消息限流功能保证其高可用\****

为什么要给RocketMQ增加限流功能保证其高可用性？因为限流功能可以对RocketMQ提供保护，避免因为Bug等原因导致短时间往RocketMQ写入大量数据而出现故障。



比如下面的代码，因为某些原因导致在while循环中向RocketMQ发消息。如果系统是部署在10多台机器上的，那么可能出现10多台机器都频繁往RocketMQ写消息，瞬间导致RocketMQ集群的TPS飙升，压垮RocketMQ集群。

- 
- 
- 
- 
- 
- 
- 
- 

```
try {    //业务代码    producer.send(message);} catch (Exception e) {    while (true) {        producer.send(message);    }}
```

所以针对这种情况，一般会改造RocketMQ的源码。在Broker接收消息时，引入限流机制。只允许一秒内写入多少条消息，避免因为一些异常情况，导致RocketMQ集群挂掉。

**
**

***\*18.从Kafka迁移到RocketMQ的双写双读方案\****

假设系统原来使用的MQ是Kafka，现在要从Kafka迁移到RocketMQ，那么这个迁移过程应该怎么做？



首先要做到双写，也就是在所有的Producer系统中，同时往Kafka和RocketMQ写入消息。一般会让双写持续1周左右，因为MQ里面数据也就最多保留一周。当双写持续一周后，Kafka和RocketMQ里的数据基本一模一样了。



但光是双写还不够，还需要同时进行双读。在双写的同时，所有Consumer消费者都要同时从Kafka和RocketMQ里获取消息，并且用一模一样的逻辑进行处理。只不过从Kafka里获取到的消息还是执行核心的逻辑进行处理，落入数据库或者其他存储。而从RocketMQ里获取到的消息，虽然也用同样逻辑进行处理，但不会把处理结果落入数据库或其他存储。



Consumer消费消息时，需要统计每天从Kafka和RocketMQ读取和处理的消息量，以及记录对应的消息处理结果到某个临时存储中。这样一段时间后，就可以对比从Kafka和RocketMQ读取和处理的消息量是否一致、处理的消息结果是否一致。如果是，那么就可以进行正式切换了。



基本上对于类似中间件的迁移，都会采取这种双写双读的方案。双写一段时间后，再观察结果是否都一致。如果是，那么再进行切换。

**
**

***\*19.Kafka利用零拷贝和页缓存实现高性能读取\****

Kafka在消费数据时，需要从磁盘文件里读取数据后通过网络发送出去，这时会怎么提升性能？



首先写入时利用了OS PageCache技术：由于Kafka写入数据到磁盘文件时是先写入OS PageCache的，这时没有直接发生磁盘IO。所以写入的数据大部分都会停留在OS PageCache里，而这个本质其实跟ElasticSearch的实现原理是类似的。



如果读取时没有零拷贝：Kafka的应用进程会先尝试从OS PageCache读，读不到才从磁盘IO读。从磁盘文件读到数据以后会先放在OS的Page Cache里，接着会发生上下文切换到OS，把OS PageCache的读缓存数据拷贝到Kafka应用进程的缓存里。接着再次发生上下文切换回Kafka应用进程，把应用进程缓存的数据拷贝到OS的Socket Cache缓存中，最后再把数据发送到网卡上。



这个过程里，发生了好几次上下文切换，而且还涉及到了好几次数据拷贝。如果不考虑跟硬件之间的交互，起码从OS PageCache到应用进程缓存、从应用进程缓存到OS SocketCache缓存，这两次拷贝是绝对没必要的。



如果读取时有零拷贝，即使用了Linux的Sendfile，那么就可以直接把操作交给OS。OS看Page Cache里是否有数据，如果没有就从磁盘上读取，如果有直接把OS Cache里的数据拷贝给网卡，中间就不用走那么多步骤了。所以通过零拷贝技术来读取磁盘上的数据，还有OS PageCahce的帮助，这个让Kafka的读性能就非常高。

**
**

***\*20.Kafka集群的分布式存储和高可用\******
**

***\*(1)Kafka如何实现TB级数据的分布式存储\****

Kafka是支持分布式存储的，一个Topic代表了逻辑上的一个数据集，可以认为一个业务上的数据集合。比如说用户行为日志是一个Topic、数据库里的每个表的数据分别又是一个Topic、订单表的增删改的变更记录又会进入一个Topic、促销表的增删改的变更记录又会进入一个Topic等。



每个Topic都可以有很多个Partition，这可以认为是数据分区，或者是数据分片。比如这个Topic假设有10TB的数据量需要存储在磁盘上，此时给它分配5个Partition，那么每个Partition会存放2TB的数据。然后每个Partition就可以放在不同的机器上，通过这个方式就可以实现数据的分布式存储了。每台机器上都运行一个Kafka的进程，叫做Broker。

**
**

***\*(2)Kafka基于多副本冗余机制保证高可用性\****

如果Kafka某台机器宕机了，那么放置在该机器的某个Topic的一个Partition数据就丢失了。所以需要对数据做多副本冗余，也就是每个Parttion都有副本：比如每个Partition做一个副本，副本放在另外一台机器上。然后Kafka会自动从一个Partition的多个副本中选举出一个Leader Partition。该Leader Partition就负责对外提供该Partiton的数据读和写，接收到写过来的数据就可把数据复制到Follower Partition上。



如果某个Leader Partition所在的机器宕机了，此时怎么办呢？Kafka集群会通过ZooKeeper来维持Kafka之间的会话：如果某个Kafka的某个Leader Partition所在的机器宕机了，那么Kafka集群就会在Follower Partition中重新选举一个Leader Partition，以此实现容错。



每个Partitino都有多个副本，其中一个是Leader Partitino，它是选举出来的，其他的都是Follower Partition。通过多副本冗余的机制，就可以实现Kafka高可用架构。

***\*
\****

***\*(3)Kafka的ISR机制可以保证写入的数据不丢失\****

依靠多副本机制能保证Kafka的高可用性，但不能保证数据不丢失。因为如果Leader宕机，但是Leader的数据还没同步到Follower上去，此时即使选举了Follower作为新的Leader，没同步的数据已经丢失了。



ISR的意思是in-sync replica，就是跟Leader Partition保持同步的Follower Partition的数量。只有处于ISR列表中的Follower才可以在Leader宕机之后被选举为新的Leader，因为在这个ISR列表里代表他的数据跟Leader是同步的。



如果要保证写入Kafka的数据不丢失，首先需要保证ISR中至少有一个Follower，其次就是在一条数据写入Leader Partition之后必须复制给ISR中所有的Follower Partition，才能表明这条数据已提交，这样才会绝对不丢失。

**
**

***\*(4)基于ZooKeeper实现Kafka无状态可伸缩的架构\****

Kafka有很多元数据：有哪些Partition、有哪些Topic、Partition相关的ISR列表等内部数据。如果Kafka把这些元数据放进自己Broker的内存中来管理，那么就表明Kafka是有状态的。一旦Kafka是有状态，则后面要对Kafka进行扩容比如给集群增加一台Broker机器，其他Broker则要进行内存数据同步。这时就不好扩容了，除了同步内存数据，还会导致将某个Broker数据分一部分到另一个Broker中，如Redis Cluster一样。所以Kafka进程尽量保证是无状态的，把Broker的元数据全都放在外部独立的ZooKeeper集群里去。

**
**

***\*(5)Kafka集群基于ZooKeeper实现节点发现与故障感知\****

比如给集群新增了一个Kafka的Broker，该Broker会把自己注册到ZooKeeper集群里，注册时会更新ZooKeeper的数据。更新ZooKeeper的数据后，ZooKeeper会通知其他各个Broker，这些Broker就会感知到集群里新增了一个Broker。



通过ZooKeeper就可以实现集群节点的上线下线的感知。比如Kafka集群的某个Broker挂掉了：由于其活着的时候会在ZooKeeper集群中创建一个临时会话，挂掉的时候会导致临时会话消失，临时会话消失后便能导致其他Broker感知到该Broker挂掉了。如果该Broker有Leader Partition，其他Broker就能从Follower Partition中选举一个新的Leader Partition出来。



Kafka核心原理简单总结：Topic可以分布式存储，写的时候往多台机器里去写Leader Partition，Leader Partition会将数据异步同步到Follower Partition。Kafka会维护一个ISR列表，通过ISR机制来保证数据同步。通过ZooKeeper集群来维护Broker的元数据，基于ZooKeeper进行集群节点的上下线和故障感知。

***\*
\****

***\*21.Kafka的LEO机制和高水位机制\******
**

***\*(1)Kafka的底层数据存储结构：日志文件以及offset\****

基本上可以认为每个Partition就是一个日志文件，存在于某台Kafka服务器上。然后这个日志里写入了很多消息，每个消息在Partition日志文件里都有一个序号叫做offset，offset代表这个消息是日志文件里的第几条消息。但在消费消息的时候也有一个offset，这个offset是代表消费者目前在Partition日志文件里消费到了第几条消息。

***\*
\****

***\*(2)Partition的高水位offset和LEO分别代表什么\****

每次Leader接收到一条消息都会更新自己的LEO，也就是Log End Offset，把最后一位offset + 1。比如有5条数据，第1条数据的offset = 0，第5条数据的offset = 4。此时LEO = 5，代表着最后一条数据后面的offset，也就是下一次将要写入的数据的offset。



接着各个Follower会向Leader发起请求来同步数据，这是持续进行的。并不是Leader主动推送数据给Follower，而是Follower主动向Leader获取数据，不断发送请求到Leader去fetch最新的数据。然后Follower同步到数据之后，就会更新自己的LEO。对于Leader接收到的某一条数据，所有Follower的LEO都更新之后，Leader才会把自己的HW高水位offset + 1。这个高水位offset表示的就是最新一条所有Follower都同步完成的消息。



LEO和HW分别是干什么的？LEO很重要的一个功能是负责用来更新HW的，就是如果Leader和Follower的LEO同步了，此时HW就可以更新。Partition中最开始的一条数据的offset是Base offset，消费者只能看到Base offset到HW offset之间的数据。因为只有这之间的数据才表明是所有Follower都同步完成的，这些数据叫做已提交，是可以被消费到的。HW offset到LEO之间的数据，是未提交，这时候消费者是看不到的。HW offset表示的是当前已经提交的数据offset，LEO表示的是下一个要写入的数据的offset。

**
**

***\*(3)Leader与Follower上的LEO是如何更新的\****

首先Leader接收到数据后就会更新自己的LEO值，接着Follower会不断向Leader发送fetch请求同步数据，然后每条数据同步到Follower之后，Follower的LEO就会更新。



Follower发送fetch请求给Leader时会带上自己的LEO值，Leader每次收到一个fetch请求就会更新自己维护的每个Follower的LEO值，所以Leader是会保存所有Follower的LEO值的。

**
**

***\*(4)Leader与Follower如果更新HW高水位offset\****

每次Leader发送数据给Follower时，都会发送自己的HW值。Follower获取到Leader的HW之后，就会与自己的LEO进行比较，然后取小的那个值作为自己的HW值。



Leader会在接收Follower的fetch请求时，更新自己维护的Follower的LEO后，同时判断当前维护的所有Follower的LEO的最小值是否大于当前的HW值，以此来决定是否更新自己的HW值。



Leader的HW值其实就是Partition的HW值，代表了这个Partition的哪个offset之前的数据是可以被消费的。

***\*
\****

***\*(5)Leader与Follower的LEO与高水位如何更新\****

假设Leader收到第一条数据，此时Leader LEO = 1，HW = 0，因为它发现其他Follower的LEO也是0，所以它的HW必须是0。



接着Follower发送fetch请求到Leader进行数据同步，这时会带上Follower的LEO = 0。所以Leader会更新自己维护的Follower LEO = 0，此时发现Follower的LEO还是0，与自己的LEO不同步，所以Leader的HW继续是0。



接着Leader发送一条数据给Follower，这时会带上Leader的HW = 0。因为Follower发现Leader的HW = 0，所以此时Follower的LEO更新为1，但是Follower的HW = 0，取Leader的HW。



接着下次Follower再次发送fetch请求给Leader时，就会带上Follower自己的LEO = 1。所以Leader会更新自己维护的Follower LEO = 1，此时发现Follower跟自己的LEO同步了，那么Leader的HW更新为1。



接着Leader发送给Follower的数据里包含了HW = 1，此时Follower发现Leader HW = 1，自己的LEO = 1，所以Follower的HW会更新为1。



总共5个数据：全部都要往前推进更新，需要2次fetch请求。第一次fetch请求是仅仅是更新两边的LEO，第二次fetch请求是更新Leader管理的Follower的LEO以及两个HW。

**
**

***\*(6)高水位机制可能导致Leader切换时数据丢失\****

上面说的高水位机制可能会导致数据丢失。如果生产者的min.insync.replicas为1，会导致生产者发消息给Leader，Leader写入成功后生产者就会认为写成功了。



当min.insync.replicas为1时，假设Leader的LEO = 1，HW = 0，因为Follower还没同步，HW肯定是0。



接着Follower发送fetch请求，此时Leader发现Follower LEO = 0，所以HW还是0，给Follower带回去的HW也是0。然后Follower开始同步数据，自己的LEO = 1，但是HW = 0，因为Leader的HW为0。



接着Follower再次发送fetch请求并带上自己的LEO = 1，Leader发现自己LEO = 1，Follower LEO = 1，所以HW更新为1。同时Leader会把HW = 1返回给Follower，此时Follower还没更新HW，HW还是0。



这时假如Follower机器宕机了，重启机器之后，Follower的LEO会自动被调整为0。因为重启后会依据HW来调整LEO，所以Follower的那条数据会从日志文件里删除，数据会没了。这时如果Leader又宕机，就会选举Follower为Leader，此时Follower的HW = 0。接着原Leader重启后会作为Follower，该Follower会从Leader同步得到HW = 0，此时会截断自己的日志，删除那条数据。于是在这种场景就会导致数据的丢失。



这是一个非常极端的一个场景，导致数据莫名其妙的丢失：Follower在第二次fetch后还没收到Leader的最新HW时重启，LEO被重置为HW丢失第一次fetch的数据。刚重启完Leader也重启，Follower与Leader身份互换，原Leader去fetch数据时发现返回的HW是最小的于是删除自己的数据。

**
**

***\*(7)高水位机制可能导致Leader切换时发生数据不一致问题\****

当min.insync.replicas = 1时，如果Leader写入了两条数据，但是Follower才同步了一条数据，第二条数据还没同步。这种情况是Follower同步完第二条数据后HW=1，再次fetch时带着LEO=2进行请求，Leader收到LEO=2后更新HW=2并返回。Follower还没收到这次返回就宕机重启，重启后由于之前LEO = 2且HW = 1，所以重置LEO = 1且HW = 1，变成只同步一条数据。这时会出现Leader的HW=2，Follower的HW=1，因为Follower的LEO小于Leader的HW，故Follower的HW设为自己的LEO。



这时如果Leader挂掉，Follower变成Leader，此时HW=1，然后生产者刚好发了一条数据给新Leader，此时HW变为2。但是第二条数据是新的数据，接着老Leader重启变为Follower，这个时候发现两者的HW都是2。这时第二条数据是不一致的，本来应该是新的Follower要删掉自己原来的第二条数据，跟新Leader同步的，让数据一致。但是因为HW发现一样，所以就不会截断数据了。

***\*
\****

***\*(8)引入Leader Epoch机制解决高水位机制弊端\****

所谓的Leader Epoch即为每个Leader的版本号，以及自己是从哪个offset开始写数据的，类似[epoch = 0, offset = 0]。按照上面的不一致场景：Leader的LEO = 2且HW = 2，Follower的LEO = 2且HW = 1，正在等第二次fetch的返回结果时，Follower宕机重启，不会像之前版本那样直接截断第二条数据，它会找Leader继续同步最新的数据，更新自己的LEO和HW。因为它会看自己有没有[epoch, offset]，如果有的话，除非是自己的offset大于了Leader的offset，才会截断自己的数据。



如果Leader宕机，切换到Follower上，此时新Leader就会更新自己的[epoch = 1, offset = 2]。意思是Leader版本号是epoch = 1，从offset = 2开始写数据。然后接着老Leader恢复变为Follower，从新Leader同步数据时会拿返回的Epoch跟自己对比：如果新Leader的offset = 2，自己的offset = 0，也不需要做任何数据截断，直接同步人家数据就可以了。如果新Leader的offset = 1，自己的offset = 1，这就是针对数据不一致的场景，此时就会截断掉自己的数据，保持一致。

**
**

***\*22.Kafka的ISR工作原理和机制\******
**

***\*(1)旧版Kafka是如何维护ISR列表的\****

什么样的Follower才有资格放到ISR列表里？在旧版本里，有一个核心的参数：replica.lag.max.messages，该参数规定了Follower如果落后Leader的消息数超过了该参数指定的数量后，就会认为该Follower是out-of-sync，就会从ISR列表里移除。



比如一个Partition有3个副本，其中一个Leader，两个Follower，然后replica.lag.max.messages = 3。刚开始时Leader和Follower都有3条数据，此时HW和LEO都是offset = 2的位置，大家都同步了。现在来了一条数据，Leader和其中一个Follower都写入了。但另外一个Follower因自身所在机器性能突然降低，导致没及时同步数据。这通常由于Follower所在机器的网络负载、内存负载、磁盘负载过高，导致整体性能下降导致的。



此时Leader Partition的HW还是offset = 2的位置，没变，但是LEO变成了offset = 3的位置。所以根据LEO来更新ISR的话，在每个Follower不断的发送fetch请求过来时，会判断Leader和Follower的LEO相差多少。如果差的数量超过了replica.lag.max.messages参数设置的一个阈值之后，就会把Follower给踢出ISR列表。



但是这时第二个Follower的LEO落后Leader才1个offset，还没到参数设定的3。所以第二个Follower实际上还在ISR列表里，只不过刚才那条消息没有算"提交的"，在HW外面，所以消费者是读不到的。而且这时生产者写数据，如果默认要求必须同步所有Follower才算写成功，可能会导致生产者一直卡着，认为没写成功。



当min.sync.replicas = 2，ack = -1，生产者会要求必须要有2个副本在ISR里才可以写。此外ISR里的副本全部都接受到数据才算写入成功，一旦ISR副本里少于2，还是会导致生产者被卡住的。比如一共有3个副本，1个Leaderr，2个Follower，此时其中一个Follower落后，被ISR踢掉了，ISR里还有2个副本。由于一个Leader和另外一个Follower都同步成功了，所以就不会卡住生产者了。



假设这时第二个Follower发生FGC持续了几百毫秒然后结束了，接着从Leader同步了那条数据，此时大家LEO都一样。而且Leader发现所有Follower都同步了这条数据，Leader就会把HW推进一位，HW变成offset = 3，这时消费者就可以读到这条在HW范围内的数据了。



但如果此时Follower发生FGC一直持续了好几秒钟，此时其他的生产者一直在发送数据过来。Leader和第一个Follower的LEO又推进了2位，LEO offset = 5，但HW还停留在offset = 2，这时HW后的数据是消费不了的。如果第二个Follower的LEO跟Leader的LEO差距超过3，就会触发阈值，该Follower被认为是out-of-sync，从ISR列表移除。



一旦第二个Follower从ISR列表里移除，此时ISR列表里就Leader和第一个Follower两个副本了。此时Leader和第一个Follower的LEO都是offset = 5，是同步的，Leader就会把HW推进到offset = 5。此时消费者就可以消费全部数据了，生产者也认为它们的写操作成功了。



如果第二个Follower后来FGC结束，开始追赶Leader数据，慢慢LEO又控制在replica.lag.max.messages限定范围内，此时Follower会重新加回到ISR列表里去。



这就是ISR的工作原理和机制。

**
**

***\*(2)一般导致Follower跟不上的情况主要是以下三种\****

情况一：Follower所在机器的性能变差

比如说网络负载过高，IO负载过高，CPU负载过高，机器负载过高，都可能导致机器性能变差，同步过慢。这时就可能导致某个Follower的LEO一直跟不上Leader，就从ISR列表里移除了。



我们生产环境遇到的一些问题，基本是Kafka机器层面。比如某台机器磁盘坏了也就是物理机的磁盘有故障导致写入性能特别差，CPU负载太高也就是线程间的切换太频繁导致CPU忙不过来了，网卡被其他的程序给打满导致网络传输的速度特别慢。replica.lag.max.messages主要是解决第一种情况。



情况二：Follower所在的Broker进程卡顿如FGC问题

Kafka自己本身对JVM的使用是很有限的。生产集群部署时，主要是接收到数据直接写本地磁盘，写入OS Page Cache。Kafka一般不怎么在自己的内存里维护过多的数据，主要是依托OS Page Cache(缓存)来提高读和写的性能的。replica.lag.time.max.ms是解决第二种情况，比如设置为500ms，那么在500ms内Follower没发送请求找Leader来同步数据，说明它可能在FGC，此时就移出ISR。



情况三：Kafka是支持动态调节副本数量的

如果动态增加了Partition的副本就会增加新的Follower，此时新的Follower会拼命从Leader上同步数据。但是这个是需要过程的，所以此时需要等待一段时间才能跟Leader同步。

**
**

***\*(3)旧版本Kafka的ISR机制在生产环境的缺陷\****

replica.lag.max.messages参数默认值是4000，也就是Follower落后4000条数据就认为是out-of-sync。



但是这里有一个问题，就是这个数字是固定死的。如果生产端突然之间涌入几万条数据，就有可能Leader瞬间刚接收到几万条消息。然后所有Follower还没来得及同步过去，此时所有Follower都会被踢出ISR列表，然后同步了之后，再回到ISR列表。



所以这种依靠固定参数判断的机制会导致在系统高峰期，Follower会频繁的踢出ISR列表再回到ISR列表。一般在Kafka  0.8.2.x系列版本上生产时，都会把这个ISR落后判定阈值设置的大一些，避免上述的情况出现。比如可以设置个几万、10万，如果公司里没那么大的高峰并发量，每秒就是几千的并发，那4000也没问题。

**
**

***\*(4)新版本Kafka的ISR机制做了哪些优化\****

kafka 0.9.x之后去掉了原来的replica.lag.max.messages参数，引入了一个replica.lag.time.max.ms参数，默认值是10秒。这个就不按照落后的条数来判断了，而是当某Follower的LEO一直落后Leader超过了10秒，那么才判定该Follower是out-of-sync的。这样即便出现流量洪峰，一下子导致几个Follower都落后了不少数据，但是只要尽快追上来，在10秒内别一直落后，就不会认为是out-of-sync，这个机制在线上实践会发现效果要好多了。
