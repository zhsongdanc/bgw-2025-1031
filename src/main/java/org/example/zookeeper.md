**1.ZAB协议和Raft协议的区别**

**2.分布式系统的理论**

**3.两阶段提交Two-Phase Commit(2PC)**

**4.三阶段提交Three-Phase Commit(3PC)**

**5.Paxos岛的故事来对应ZooKeeper**

**6.Paxos算法推导过程**

**7.Paxos协议的核心思想**

**8****.ZAB算法简述**

**9.为什么在分布式系统架构中需要使用zk集群**

**10.zk分布式系统具有哪些特点**

**11.zk集群机器的三种角色**

**12.客户端与zk之间的长连接和会话**

**13.zk的数据模型znode和节点类型**

**14.zk最核心的Watcher监听回调机制**

**15.ZAB协议的主从同步机制和崩溃恢复机制**

**16.ZAB协议流程之集群启动-数据同步-崩溃恢复**

**17.采用2PC两阶段提交思想的ZAB消息广播流程**

**18.zk到底是强一致性还是最终一致性**

**19.ZAB协议下两种可能存在的数据不一致问题**

**20.崩溃恢复时新Leader和Follower的数据同步**

**21.ZAB协议会如何处理需要丢弃的消息的**

**22.zk的Observer节点的作用**

**23.zk适合小集群部署 + 读多写少场景的原因**

**24.zk特性的总结**

**
**

**1.ZAB协议和Raft协议的区别**

**(1)‌ZAB协议和‌Raft协议的相同点‌**

***\*一.采用‌Quorum来确定整个系统的一致性\****‌

ZAB协议和Raft协议都使用Quorum机制来确保系统的数据一致性，通常实现为集群中半数以上的服务器。Zookeeper中的ZAB协议还提供了带权重的Quorum实现。

**
**

**二.由‌Leader发起写操作‌**

两者都由Leader节点发起写操作，并将写请求广播给所有Follower节点。

**
**

**三.使用心跳检测存活性‌**

两者都通过心跳机制来检测服务器的存活状态。

**
**

**四.领导者选举采用先到先得的投票方式‌**

两者在领导者选举时都采用先到先得的投票方式，即第一个收到投票请求的候选者成为领导者。

**
**

**(2)ZAB协议和RAFT协议的不同点**

**一.**事务编号的不同‌

ZAB协议‌使用‌epoch和count的组合来表示一个事务，其中Zxid是一个64位的数字，低32位是单调递增的计数器，高32位代表Leader周期epoch。Raft协议‌使用‌term和‌index的组合来表示一个事务，每个leader有一个任期，任期内的事务使用递增的索引。



二.日志一致性的不同‌

ZAB协议‌在选举新的Leader时，新的Leader需要将日志更新为Quorum中最新的日志，然后同步其他节点的日志，确保一致性。‌Raft协议‌的Follower在投票给Leader之前不需要与Leader的日志达成一致，而是通过数据同步来实现一致性。



三.数据同步的不同‌

‌ZAB协议‌采用两阶段提交过程，Leader将写请求封装成事务Proposal发送给所有Follower，等待超过半数的Follower响应后执行commit操作。Raft协议‌Leader将数据写入本地磁盘后，异步写入内存，然后遍历所有节点同步数据，若半数以上节点处理成功，则数据同步完成。

**
**

**2.分布式系统的理论**

**(1)CAP理论**

CAP指的是一致性(C)、可用性(A)、分区容错性(P)这三个词的缩写。CAP理论具体描述：一个分布式系统不可能同时满足一致性、可用性和分区容错性，最多只能同时满足其中的两项。注意：P是不能放弃的，不可能把所有应用全部放到一个节点上。A和C也不能全部放弃，因此要在A和C间寻求平衡，平衡的基础就是BASE理论。

**
**

**(2)BASE理论**

BASE理论：即使无法做到强一致性，但分布式系统可以根据自己的业务特点，采用适当的方式来使系统达到最终的一致性。BASE理论一句话概括：就是平时系统要基本可用，除了成功失败，运行时允许出现可容忍的延迟状态，但是无论如何经过一段时间的延迟后系统最终必须达到数据是一致的。

**
**

**(3)分布式一致性算法**

不管是CAP理论，还是BASE理论，这些理论都需要算法来实现。2PC、3PC、Paxos算法、ZAB算法就是用来实现CAP、BASE理论的。这些理论的前提一定是分布式，解决的问题都是：在分布式环境下，怎么让系统尽可能高可用，且数据能最终能达到一致。

**
**

**3.两阶段提交Two-Phase Commit(2PC)**

**(1)数据库事务通过undo和redo保证数据强一致性**

2PC即二阶段提交算法，是强一致性算法。它是数据库领域内，为了使基于分布式系统架构下的所有节点，在进行事务处理过程中能够保持原子性和一致性而设计的算法。所以很适合用作数据库的分布式事务，其实数据库经常用到的TCC本身就是一种2PC。



在InnoDB存储引擎中，对数据库的事务修改都会写undo日志和redo日志。其实不只是数据库，很多需要事务支持的都会用到undo和redo思路。一.对数据的修改操作首先写undo日志记录数据原值，二.然后执行事务修改操作，把修改写到redo日志里，三.万一事务失败了，可以从undo日志恢复数据。数据库通过undo和redo能保证数据强一致性。

**
**

**(2)分布式事务通过2PC保证数据强一致性**

解决分布式事务的前提就是节点是支持事务的。在这个前提下，2PC把整个分布式事务分两个阶段：投票阶段(Prepare)和执行阶段(Commit)。

**
**

**阶段一：投票阶段**

在阶段一中，各参与者投票表明是否要继续执行接下来的事务提交操作。



步骤一：协调者向参与者发起事务询问。协调者向所有的参与者发送事务内容，询问是否可以执行事务操作，并开始等待各参与者的响应。

步骤二：参与者收到协调者的询问后执行事务。各参与者节点执行事务操作，并将undo和redo信息记入事务日志中。

步骤三：参与者向协调者反馈事务询问的响应。如果参与者成功执行事务操作，就反馈协调者Yes响应，表示事务可执行。如果参与者没成功执行事务，就反馈协调者No响应，表示事务不可执行。

**
**

**阶段二：执行阶段**

在阶段二中，协调者会根据各参与者的反馈来决定是否可以进行事务提交。有两种提交可能：执行事务提交和中断事务。

**
**

**可能一：执行事务提交**

假如协调者从所有参与者获得的反馈都是Yes响应，那么就会执行事务提交。



步骤一：协调者向参与者发送提交请求，协调者向所有参与者节点发出Commit请求。

步骤二：参与者收到协调者的Commit请求后进行事务提交。参与者接收到Commit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。

步骤三：参与者向协调者反馈事务提交结果(Ack消息)。参与者在完成事务提交之后，向协调者发送Ack消息。

步骤四：协调者收到所有参与者反馈(Ack)后完成事务。协调者接收到所有参与者反馈的Ack消息后，完成事务。

**
**

**可能二：中断事务**

假如任何一个参与者向协调者反馈了No响应或者等待超时，那么协调者无法接收到所有参与者的反馈响应，就会中断事务。



步骤一：协调者向参与者发送回滚请求，协调者向所有参与者节点发出Rollback请求。

步骤二：参与者收到协调者的Rollback请求后进行事务回滚。参与者接收到Rollback请求后，会利用undo信息来执行事务回滚操作，并在完成回滚之后释放占用的事务资源。

步骤三：参与者向协调者反馈事务回滚结果(Ack消息)。参与者在完成事务回滚之后，向协调者发送Ack消息。

步骤四：协调者收到所有参与者反馈(Ack)后中断事务。协调者接收到所有参与者反馈的Ack消息后，完成事务中断。

**
**

**(3)2PC的优点和缺点**

**一.2PC优点**

优点一：原理简单

优点二：实现方便

**
**

**二.2PC缺点**

总结来说有四个缺点：同步阻塞、单点故障、数据不一致、容错机制不完善。



缺点一：同步阻塞。在二阶段提交过程中，所有节点都在等其他节点响应，无法进行其他操作，这种同步阻塞极大的限制了分布式系统的性能。

缺点二：单点问题。协调者在整个二阶段提交过程中很重要。如果协调者在提交阶段出现问题，那么整个流程将无法运转。而且其他参与者会处于一直锁定事务资源的状态中，无法完成事务操作。

缺点三：数据不一致。假设当协调者向所有参与者发送Commit请求后，发生了局部网络异常。或者是协调者在尚未发送完所有Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求，那么这会出现部分参与者的数据不一致问题。

缺点四：容错性不好。二阶段提交协议没有较为完善的容错机制，任意一个参与者或协调者故障都会导致整个事务的失败。

**
**

**4.三阶段提交Three-Phase Commit(3PC)**

**(1)第一阶段canCommit**

步骤一：协调者向参与者发起事务询问。协调者向所有参与者发送一个包含事务内容的canCommit请求。询问是否可以执行事务提交操作，并开始等待各参与者响应。

步骤二：参与者收到协调者的询问后反馈响应。参与者在接收到协调者的canCommit请求后，如果认为可以顺利执行事务，会反馈Yes响应并进入预备状态，否则反馈No响应。



这一阶段其实就是确认所有的资源是否都是健康、在线的。因为有了这一阶段，大大的减少了2PC提交的阻塞时间。因为这一阶段优化了以下这种情况：2PC提交时，如果有两个参与者，恰好参与者2出现问题，参与者1执行了耗时的事务操作，最后却发现参与者2连接不上，参与者1白执行耗时操作了。

**
**

**(2)第二阶段preCommit**

包含两种可能：执行事务预提交和中断事务。

**
**

**可能一：执行事务预提交**

假如协调者从所有参与者获得的反馈都是Yes响应，则执行事务预提交。



步骤一：协调者向参与者发送预提交请求，协调者向所有参与者发出preCommit请求，然后协调者会进入预提交状态。

步骤二：参与者收到协调者的preCommit请求后执行事务。参与者接收到协调者发出的preCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。

步骤三：参与者向协调者反馈事务执行的响应(Ack)。如果参与者成功执行了事务操作，那么就会反馈给协调者一个Ack响应。

**
**

**可能二：中断事务**

假如任何一个参与者向协调者反馈了No响应，或者等待超时。协调者无法接收到所有参与者的反馈响应，那么就会中断事务。



步骤一：协调者向参与者发送中断请求，协调者向所有参与者节点发出abort请求。

步骤二：参与者收到协调者abort请求则中断事务。无论是收到来自协调者的abort请求，或者在等待协调者请求过程中出现超时，参与者都会中断事务。

**
**

**(3)第三阶段doCommit**

包含两种可能：执行提交和中断事务。

**
**

**可能一：执行提交**

接收到来自所有参与者的Ack响应。



步骤一：协调者向参与者发送提交请求。协调者向所有参与者发出doCommit请求，然后协调者会由预提交状态进入提交状态。

步骤二：参与者收到协调者的doCommit请求后提交事务。参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源。

步骤三：参与者向协调者反馈事务提交结果。参与者在完成事务提交之后，向协调者发送Ack消息。

步骤四：协调者收到所有参与者反馈(Ack)完成事务。协调者接收到所有参与者反馈的Ack消息后，完成事务。

**
**

**可能二：中断事务**

假如任何一个参与者向协调者反馈了No响应，或者等待超时。协调者无法接收到所有参与者的反馈响应，那么就会中断事务。



步骤一：协调者向参与者发送回滚请求，协调者向所有参与者节点发出abort请求。

步骤二：参与者收到协调者的abort请求后进行事务回滚。参与者接收到协调者的abort请求后，会利用undo信息执行事务回滚操作，并在完成回滚之后释放占用的事务资源。

步骤三：参与者向协调者反馈事务回滚结果(Ack消息)。参与者在完成事务回滚之后，会向协调者发送Ack消息。

步骤四：协调者收到所有参与者反馈(Ack)后中断事务。协调者接收到所有参与者反馈的Ack消息后，完成事务中断。

**
**

***\*注意：\****一旦进入阶段三doCommit，无论出现哪一种故障：协调者出现了问题、协调者和参与者之间网络故障，最终都会导致参与者无法及时接收来自协调者的doCommit或abort请求。参与者都会在等待超时后，继续进行事务提交。

**
**

**(4)3PC的优缺点**

优点一：改善同步阻塞。与2PC相比，降低了参与者的阻塞范围。

优点二：改善单点故障。与2PC相比，出现单点故障后能继续达成一致。



缺点一：同步阻塞。相比2PC虽然降低阻塞范围，但依然存在阻塞。

缺点二：单点故障。虽然单点故障后能继续提交，但单点故障依然存在。

缺点三：数据不一致。正是因为出现单点故障后可以继续提交，所以可能会出现数据不一致。

缺点四：容错机制不完善。参与者或协调者节点失败会导致事务失败，所以数据库的分布式事务一般都是采用2PC，而3PC更多是被借鉴扩散成其他的算法。

**
**

**(5)3PC与2PC区别**

区别一：3PC第二阶段才写undo和redo事务日志。

区别二：3PC第三阶段协调者出现异常或网络超时参与者也会commit。

**
**

**5.Paxos岛的故事来对应ZooKeeper**

先说Paxos，它是一个基于消息传递的一致性算法。Chubby和ZooKeeper都是基于Paxos的理论来实现的，Paxos还被认为是到目前为止唯一的分布式一致性算法，其它的算法都是Paxos的改进或简化。但是Paxos有一个前提：没有拜占庭将军问题。就是Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。

**
**

**(1)Paxos岛的故事背景**

有一个叫做Paxos小岛(对应zk集群)，上面住了一批居民(对应zk的Client)。岛上面的事情由一些特殊的人决定，他们叫做议员(对应zk的Server)。议员(对应zk的Server)的总数是确定的，不能更改。



岛上的居民(对应zk的Client)每次进行事情变更都需要通过一个提议(Proposal)。每个提议都有一个编号(PID，对应zk的ZXID)。这个编号是一直增长的，不能倒退。每个提议都要超过半数议员同意才能生效。因为如果全部通过就是强一致性，会破坏可用性。



每个议员(zk的Server)只会同意大于当前编号(zk的ZXID)的提议(Proposal)，包括已生效和未生效的(分布式节点要排除由于网络延迟而晚收到的消息)。如果议员收到小于等于当前编号的提议，他会拒绝，并告知对方(对应zk的Client)：你的提议已经有人提过了(保证数据的版本性)。当前编号指的是每个议员在记事本上记录的编号，他会不断更新该编号。整个会议不能保证所有议员记事本上的编号总是相同的，因为网络延迟可能半数人已通过第10版本了，部分人还停留在第9版本。



现在会议有一个目标：保证所有议员(zk的Server)对提议(Proposal)都能达成一致的看法(完成所有数据的同步)。

**
**

**(2)Paxos岛的故事运作**

好，现在会议开始运作，所有议员一开始记事本上面记录的编号都是0。



说明一：有一个议员收到了居民的一个提议：将电费设定为1元/度。他首先看一下记事本，发现当前提议编号是0，则他的这个提议编号就是1。于是他给所有议员发起提议：1号提议，设定电费1元/度。



说明二：其他议员收到消息以后查了一下记事本，发现当前提议编号是0。所以接收到的这个提议是可以接受的，于是记录下这个提议并回复：我接受你的1号提议，同时在记事本上记录：当前提议编号为1。



说明三：发起提议的议员收到了超过半数的回复。于是他立即给所有人发通知：1号提议生效！收到该通知的议员会修改他的记事本，将1号提议由记录改成正式的法令。当有居民问他电费多少时，他会查看并告诉对方：1元/度。



上面这个议员发出第一个提议的阶段其实隐含了两阶段提交的概念，总的来说，过半通过 + 两阶段提交，可以非常好地解决消息的传递。



说明四：冲突的解决。假设总共有三个议员S1、S2、S3，S1和S2同时发起了一个提议：1号提议，设定电费。但S1想设为1元/度，S2想设为2元/度。此时S1和S2的提议各有一票了。结果S3先收到了S1的提议，于是他做了和前面同样的操作。紧接着他又收到了S2的提议，结果他一查记事本，发现这个提议的编号小于等于我的当前编号1，于是拒绝了这个提议：对不起，这个提议先前提过了。因此S2的提议被拒绝，S1的提议被S1和S3同意。所以S1可以正式发布提议：1号提议生效。S2向S1或S3打听并更新1号法令的内容，之后可选择继续发起2号提议。



至此，Paxos的核心已通过故事介绍完毕。

**
**

**6.Paxos算法推导过程**

**(1)Paxos的概念**

**一.Paxos的角色**

在Paxos算法中，有三种角色：Proposer、Acceptor、Learner。在具体的实现中，一个进程可能同时充当多种角色。比如一个进程可能既是Proposer又是Acceptor又是Learner。



还有一个重要概念叫提案(Proposal)，最终要达成一致的value就在提案里。



假如只有一个角色，那么其实就是分布式节点自己。各自认为自己的表达是正确的，这时候是无法达成一致的。所以需要引入多一个角色来处理各个节点的表达，最后还要引入一个角色将达成一致的结果同步给各分布式节点。

**
**

**二.Paxos的提案**

暂且认为提案(Proposal)只包含value，但在接下来的推导过程中会发现如果提案(Proposal)只包含value会有问题。



暂且认为Proposer可以直接提出提案，在接下来的推导过程中会发现：如果Proposer直接提出提案也会有问题，需要增加一个学习提案的过程。



Proposer可以提出(propose)提案，Acceptor可以批准(accept)提案。如果某个提案被选定(chosen)，那么该提案里的value就被选定了。

**
**

注意：批准和选定是不一样的，批准未必就代表选定。根据后面所述，多数Acceptor批准了某提案，才能认为该提案被选定了。

**
**

**三.Paxos角色对数据达成一致的情况**

对某个数据的值达成一致，指的是：Proposer、Acceptor、Learner都认为同一个value被选定(chosen)。



Proposer、Acceptor、Learner分别在什么情况才认为某个value被选定：

**
**

**情况一：Proposer**

只要Proposer发出的提案被Acceptor批准，那么Proposer就认为该提案里的value被选定了。

**
**

**情况二：Acceptor**

只要Acceptor批准了某个提案，那么Acceptor就认为该提案里的value被选定了。

**
**

**情况三：Learner**

Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定。



上面只有一个节点时是没问题的，但多个节点时批准就不能等于选定了。

**
**

**(2)问题描述**

**一.该一致性算法的基本实现目标**

假设有一组可以提出(propose)value的进程集合(value在提案Proposal里)，那么一个一致性算法需要做出如下保证。

保证一：在提出的这么多value中，只有一个value会被选定(chosen)

保证二：如果没有value被提出，那么就不应该有value被选定

保证三：如果一个value被选定，那么所有进程都应该能学习(learn)或者获取到这个被选定的value

**
**

**二.该一致性算法满足的安全性**

一个分布式算法有两个最重要的属性：安全性(Safety)和活性(Liveness)。安全性是指那些需要保证永远都不会发生的事情，活性是指那些最终一定会发生的事情。



对于一致性算法，安全性要求如下：

要求一：只有被提出的value才能被选定

要求二：只有一个value能被选定

要求三：如果某进程认为某value被选定，则该value必须是真的被选定



在对Paxos算法的介绍中，我们不去精确定义其活性需求，只需要确保：Paxos算法的目标是保证最终有一个提出的value被选定，当一个value被选定后，进程最终也能学习或者获取到这个value。

**
**

**三.推导该一致性算法的默认条件**

假设三个角色间可通过发送消息来进行通信，那么默认以下两个情况：



情况一：每个角色以任意的速度执行，可能因出错而停止，也可能会重启。同时即使一个value被选定后，所有的角色也可能失败然后重启。除非失败后重启的角色能记录某些信息，否则重启后无法确定被选定的值。



情况二：消息在传递过程中可能任意延迟、可能会重复、也可能丢失。但是消息不会被损坏，即消息内容不会被篡改，也就是无拜占庭将军问题。即各将军管理的军队被地理分割开，只能依靠通讯员传递信息，而通信员可能存在叛徒篡改信息欺骗将军。

**
**

**四.提案被选定的规定**

下面规定在存在多个Acceptor的情况下，如何判断选定一个提案。



规定：Proposer会向一个Acceptor集合发送提案。同样，集合中的每个Acceptor都可能会批准(Accept)该提案。当有足够多的Acceptor批准这个提案时，我们就认为该提案被选定了。



所以，批准和选定是不一样的，批准未必就代表选定。多数Acceptor批准了某提案，才能认为该提案被选定了。

**
**

足够多指的是：我们假定足够多的Acceptor其实是整个Acceptor集合的一个子集，并且让这个集合大得可以包含Acceptor集合中的大多数成员，因为任意两个包含大多数Acceptor的子集至少有一个公共成员。

**
**

**(3)推导过程**

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

```
约束P1：一个Acceptor必须批准它收到的第一个提案。规定R1：一个Proposer的提案能够发送给多个Acceptor(Acceptor集合)。规定R2：一个提案(value)被选定需要被半数以上的Acceptor批准。规定R3：每一个Acceptor必须能够批准不止一个提案(value)。
约束P2：如果提案[M0, V0]被选定了：那么所有比编号M0更高的且被选定的提案，其value值必须也是V0。
约束P2a：如果提案[M0, V0]被选定了：那么所有比编号M0更高的且被Acceptor批准的提案，其value值必须也是V0。
约束P2b：如果提案[M0, V0]被选定了：那么之后任何Proposer产生的编号更高的提案，其value值必须也是V0。
约束P2c：对于任意的Mn和Vn，如果提案[Mn, Vn]被提出：那么肯定存在一个半数以上的Acceptor组成的集合S，满足以下条件中的任意一个：
条件一：S中每个Acceptor都没有批准过编号小于Mn的提案。条件二：S中Acceptor批准过的编号小于Mn的且编号最大的提案的value为Vn。
```



**一.要选定一个唯一提案的最简单方案——只允许一个Acceptor存在**

要使得只有一个value被选定，最简单的方式莫过于只有一个Acceptor，当然可以有多个Proposer，这样Proposer只能发送提案给该Acceptor。



此时，Acceptor就可以选择它收到的第一个提案作为被选定的提案，这样就能够保证只有一个value会被选定。



但是，如果这个唯一的Acceptor宕机了，那么整个系统就无法工作。因此，必须要有多个Acceptor来避免Acceptor的单点问题。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYKPQ2ceEbjPsmpbaltzHwzgqVbvX7A5I5zFU6mzAQPteKQom7RYichTw/640?wx_fmt=png&from=appmsg)



**二.多个Acceptor和多个Proposer——如何使得只有一个value被选定**

如果希望即使只有一个Proposer提出一个value，该value也能被选定。那么，就得到下面的约束：



***\*约束P1：\****一个Acceptor必须要批准它收到的第一个提案。



但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据约束P1，每个Acceptor分别批准自己收到的第一个value，这就会导致不同的value被选定，出现value不一致。于是满足不了只有一个value会被选定的要求，如下图示：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYZSibtAV09xPWkHQ6B3L2BxUQua4WfPpILka4Kmgwdj11XibZ97RkibGHQ/640?wx_fmt=png&from=appmsg)

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYhVWsuoezgWiaJyLPVWiaLkGcdYJkUgUIQQzucrz0jwqZVxK2icibAduXUw/640?wx_fmt=png&from=appmsg)



上面是由于："一个提案只要被一个Acceptor批准，则该提案的value就被选定了"，以及"Acceptor和Proposer存在多个"，才导致value不一致问题。因此问题转化为：在存在多个Acceptor和多个Proposer情况下，如何进行提案的选定？



最简单的情况：如果一个Proposer的提案只发送给一个Acceptor，由上图可知必然会导致value不一致问题。因此，可以有以下规定：



***\*规定R1：\****一个Proposer的提案能够发送给多个Acceptor(Acceptor集合)。



既然一个Proposer的提案能够发送给多个Acceptor，当Proposer可以向Acceptor集合发送提案时，集合中的每个Acceptor都可能会批准该提案。当有足够多的Acceptor批准这个提案时，我们才可认为该提案被选定了。那么什么才是足够多呢？



我们假定足够多的Acceptor是整个Acceptor集合的一个子集，并且让该子集大得可以包含Acceptor集合中的大多数成员，因为任意两个包含大多数Acceptor的子集至少有一个公共成员。因此，有了如下规定：



***\*规定R2：\****一个提案(value)被选定需要被半数以上的Acceptor批准。



在约束P1(一个Acceptor必须要批准它收到的第一个提案)的基础上，再加上规定R2(一个提案(value)被选定需要被半数以上的Acceptor批准)，假如每个Acceptor最多只能批准一个提案，那么又回到了下图的问题：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYj1vZicZ1q1lxtzarxRhzWRZxYISJFzCrgywncq3hYNkyGwxibWTPe0rQ/640?wx_fmt=png&from=appmsg)



因此，有了如下规定：



***\*规定R3：\****每一个Acceptor必须能够批准不止一个提案(value)。



既然一个Proposer的提案能够发送给多个Acceptor，那么一个Acceptor就会收到多个提案。



考虑情形1：当多个Proposer将其提案发送给多个Acceptor后，突然大部分Acceptor挂了，只剩一个Acceptor存活，如何进行提案的批准。该存活的Acceptor收到多个提案，由规定R3，它可以批准多份提案，那么如何保证最后只有一个提案被选定保证value一致。



考虑情形2：有5个Acceptor，其中2个批准了提案v1，另外3个批准了提案v2。此时如果批准v2的3个Acceptor中有一个挂了，那么v1和v2的批准者都变成了2个，此时就没法选定最终的提案。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYTibT2LtqiaKPUIaEq413NM0ice55LDDicTwUWAr3W6MLPWqcd5gFx2F3SQ/640?wx_fmt=png&from=appmsg)



因此，可以引入全局唯一编号来唯一标识每一个被Acceptor批准的提案：当一个具有某value值的提案被半数以上的Acceptor批准后，我们就认为该value被选定了，也就是该提案被选定了。唯一编号的作用其实就是用来辅助：当出现多个value都被同一半数批准时，可以选定唯一的value，比如选择唯一编号最大的value。

**
**

**三.提案变成了一个由编号和value组成的组合体：****[编号, value]**

由上可知，选定其实认的只是value值。当提案[value]变成[编号, value]后，是允许多个提案[编号, value]被选定的。根据规定R2(一个提案被选定需要半数以上的Acceptor批准)，存在多个提案被半数以上的Acceptor批准，此时就有多个提案被选定。那么就必须保证所有被选定的提案[编号, value]都具有相同的value值。否则，如果被选定的多个提案[编号, value]其value不同，又会出现不一致。因此，可以得到如下约束：



***\*约束P2：\****如果提案[M0, V0]被选定了，那么所有比编号M0更高的且被选定的提案，它的value值也必须是V0。

**
**

编号可理解为提案被提出的时间，比编号M0更高可理解为晚提出。所以提案[M0,V0]被选定后，所有比该提案晚提出的提案被选定时。为了value一致，那么这些晚提出的被选定的提案的value也要是V0。而所有比该提案早提出的提案，可以忽视它不进行选定即可。

**
**

一个提案[编号, value]只有被Acceptor批准才可能被选定，因此我们可以把约束P2改写成对"Acceptor批准的提案"的约束P2a：



***\*约束P2a：\****如果提案[M0, V0]被选定了，那么所有比编号M0更高的且被批准的提案，它的value值也必须是V0。

**
**

所以提案[M0,V0]被选定后，所有比该提案晚提出的提案被批准时。为了value一致，那么这些晚提出的被批准的提案的value也要是V0。而所有比该提案早提出的提案，可以忽视它不进行批准即可。因此只要满足了P2a，就能满足P2。



由于通信是异步的，一个提案可能会在某个Acceptor还未收到任何提案时就被选定了。假设有5个Acceptor和2个提案，在Acceptor1没收到任何提案情况下，其他4个Acceptor已经批准了来自Proposer2的提案。而此时Proposer1产生了一个具有其他value值的且编号更高的提案，并发送给了Acceptor1。那么根据P1，Acceptor1要批准该提案，但这与约束P2a矛盾。



因此，如果要同时满足P1和P2a，需要对P2a进行强化。因为P2a只是说晚提出的提案被Acceptor批准时value才为V0，需要对P2a强化为晚提出的提案不管是否被批准其value都是V0。



***\*约束P2b：\****如果提案[M0, V0]被选定了，那么之后任何Proposer产生的编号更高的提案，其value值必须也是V0。

**
**

否则一个提案得到多数批准被选定后，再提出一个值不同的新提案，这个提案会作为第一个提案发到某个Acceptor然后被批准，与P2a矛盾。



因为一个提案必须在被Proposer提出后才能被Acceptor批准，所以P2b可以推出P2a，P2a可以推出P2。

- 
- 
- 
- 
- 
- 
- 
- 
- 

```
P2：提案[M,V]被选定后，晚提出的提案如果被选定，那么其value也是V；(否则会出现选定多个value了)P2a：提案[M,V]被选定后，晚提出的提案如果被批准，那么其value也是V；(P2a可以推出P2)P2b：提案[M,V]被选定后，晚提出的提案不管是否被批准，那么其value也是V；(否则根据P1可能会出现与P2a矛盾的情况)而对于比提案[M,V]早提出的提案，可以采取忽略无视处理；(P2b可以推出P2a)
P1：一个Acceptor必须批准它收到的第一个提案；(否则只有一个提案被提出时就无法选定一个value了)R1：一个Proposer的提案能够发送给多个Acceptor(Acceptor集合)；(否则根据P1, 就会出现选定多个value了)R2：一个提案(value)被选定需要被半数以上的Acceptor批准；(选定提案时对足够多的规定)R3：每一个Acceptor必须能够批准不止一个提案(value)；(否则一个提案就没法做到被半数以上的Acceptor批准了)
```



**四.如何证明P2b**

即提案[M0,V0]被选定后，Proposer提出的编号更高的提案的value都为V0。如果要证明P2b成立，则具体是要证明：假设提案[M0,V0]已被选定，则编号大于M0的提案Mn，其value值都是V0。



通过对Mn使用第二数学归纳法来证明，也就是说需要证明结论：假设编号在M0到Mn-1之间的提案，其value值都是V0，那么编号为Mn的提案的value值也为V0。

- 
- 
- 

```
第二数学归纳法：要证明n时的value值为v，则先假设0~n-1时的value值都为v；然后再推导出n时的value值为v；
```



**证明：**

根据第二数学归纳法，当提案[M0,V0]被选定时，要证明编号大于M0的提案Mn，其value值都是V0。也就是假设编号M0到Mn-1的提案的value值都是V0时，证明Mn的value值为V0。



因为编号为M0的提案已经被选定了，这意味着存在一个由半数以上的Acceptor组成的集合C，C中的每个Acceptor都批准了编号为M0的提案。



根据归纳假设，编号为M0的提案被选定意味着：C中的每个Acceptor都批准了一个编号在M0到Mn-1范围内的提案，并且每个编号在M0到Mn-1范围内的被Acceptor批准的提案，其value为V0。

**
**

根据归纳假设，因为编号M0到Mn-1的提案的value值都是V0，所以C中的Acceptor1可以批准M0提案，Acceptor2可以批准M0 + M1提案，Acceptor3可以批准M0 + M1 + M2 + M3提案......也就是C中每个Acceptor批准的一个M0到Mn-1范围内的提案的value都是V0。

**
**

因为任何包含半数以上Acceptor的集合S都至少包含C中的一个成员，所以S中必然存在一个Acceptor，它批准的M0到Mn-1提案的value都是V0。这是根据归纳假设得出的结论，因此可以根据此而进一步加强到P2c。



因此只要满足如下P2c，就能让编号为Mn的提案的value值也为V0，也就是只要满足P2c，就可以证明P2b。只要满足如下P2c约束 +  上述归纳假设得出的结论，就能证明Mn也为V0。





***\*约束P2c：\****对于任意的Mn和Vn，如果提案[Mn, Vn]被提出，则存在一个半数以上的Acceptor组成的集合S，满足以下条件中的任一个。

条件一：S中每个Acceptor都没有批准过编号小于Mn的提案

条件二：S中Acceptor批准过的编号小于Mn且编号最大的提案的value为Vn

**
**

**五.如何根据P2c去证明P2b**

从P1到P2c的过程其实是对一系列条件的逐步加强。如果需要证明这些条件可以保证一致性，那么就要反向推导：P2c => P2b => P2a => P2，然后通过P1和P2来保证一致性。



实际上，P2c规定了每个Propeser应该如何产生一个提案(P2c规定的提案生成规则)。对于产生的每个提案[Mn, Vn]，需要满足：存在一个由超过半数的Acceptor组成的集合S满足以条件的任意一个：

条件一：要么S中没有Acceptor批准过编号小于Mn的任何提案

条件二：要么S中所有Acceptor批准的所有编号小于Mn的提案中，编号最大的那个提案的value值为Vn

**
**

当每个Proposer都按照这个规则来产生提案时，就可以保证满足P2b了。



下面在P2c的生成规则下证明P2b：

首先假设提案[M0,V0]被选定了，设比该提案编号M0大的提案为[Mn,Vn]，那么在P2c的生成提案规则前提下，证明Vn = V0；

**
**

**数学归纳法第一步：****验证某个初始值成立**

当Mn = M0 + 1时，如果有这样一个编号为Mn的提案，根据P2c的提案生成规则可知，一定存在一个超半数Acceptor的子集S，由于提案[M0, V0]已被选定，所以S中必然有Acceptor批准过编号小于Mn的提案，也就是M0提案，即此时P2c的提案生成规则的条件一不成立，进入条件二来生成Vn。



所以，由于S中有Acceptor已经批准了编号小于Mn的提案(即M0提案)。于是，Vn只能是多数集S中编号小于Mn但为最大编号的那个提案的值。而此时因为Mn = M0 + 1，因此理论上编号小于Mn但为最大编号的那个提案肯定是[M0, V0]，同时由于S和选定[M0, V0]的Acceptor集合都是多数集，故两者肯定有交集，也就是说由于两者都是多数集，所以S中必然存在一个Acceptor批准了M0。根据Mn = M0 + 1，M0其实就是编号小于Mn但是编号是最大的。这样Proposer在确定Vn取值的时候，就一定会选择V0(根据Vn只能是多数集S中编号小于Mn但为最大编号的那个提案的值)。

**
**

**数学归纳法第二步：假设编号在M****0** **+ 1到M****n** **- 1内成立，推导编号M****n****也成立**

根据假设，编号在M0 + 1到Mn - 1区间内的所有提案的value值为V0，需要证明的是编号为Mn的提案的value值也为V0。



由于编号在M0 + 1到Mn - 1区间内的所有提案都是按P2c的规则生成的，所以一定存在一个超半数Accepter的子集S，而且S中有Acceptor已经批准了编号小于Mn的提案(P2c条件一不成立)。于是，Vn只能是多数集S中编号小于Mn但为最大编号的那个提案的值。如果这个最大编号落在M0 + 1到Mn - 1区间内，那么Vn肯定是V0。如果不落在M0 + 1到Mn - 1区间内，则它的编号不可能比M0小，肯定是M0。这时因为S肯定会与批准[M0, V0]这个提案的Acceptor集合S'有交集，也就是说S中肯定存在一个Acceptor是批准了[M0, V0]的。又由于此时S中编号最大的提案其编号就是M0，根据上述，Vn只能是多数集S中编号小于Mn但为最大编号的那个提案的值。从而可知，此时Vn也是V0，因此得证。

**
**

**(4)Proposer生成提案**

在P2c的基础上，如何进行提案的生成？



对于一个Proposer来说，获取那些已经被通过的提案远比预测未来可能会被通过的提案简单。所以Proposer产生一个编号为M的提案时，必须要知道：当前某个已被半数以上Acceptor批准的编号小于M但为最大编号的提案，必须要求：所有的Acceptor都不要再批准任何编号小于M的提案。于是就引出了如下Proposer生成提案的算法：



***\*步骤一：\****Proposer选择一个新的编号M向某Acceptor集合的成员发送请求，即编号为M的提案的Prepare请求，要求集合中的Acceptor做出两个回应。回应一是：向Proposer承诺，保证不再批准任何编号小于M的提案。回应二是：如果Acceptor已经批准过任何提案，那么就向Proposer反馈当前其已批准的、编号小于M但为最大编号的提案。

**
**

***\*步骤二：\****如果Proposer收到了来自半数以上的Acceptor的响应结果，那么就可产生提案[M, V]，这里V取收到响应的编号最大的提案的value值。当然还存在另外一种情况，就是半数以上的Acceptor都没有批准任何提案。也就是响应中不包含任何提案，那么此时V就可以由Proposer任意选择。Proposer确定好生成的提案[M, V]后，会将该提案再次发送给某个Acceptor集合，并期望获得它们的批准，此请求称为编号为M的提案的Accept请求。



***\*注意：\****此时接收Accept请求的Acceptor集合，不一定是之前响应Prepare请求的Acceptor集合。

**
**

**(5)Acceptor批准提案**

根据Proposer生成提案的算法，一个Acceptor可能会收到来自Proposer的两种请求：编号为M的提案的Prepare请求和编号为M的提案的Accept请求。一个Acceptor会对Prepare请求做出响应的条件是：Acceptor可以在任何时候响应一个Prepare请求。一个Acceptor会对Accept请求做出响应的条件是：在不违背Acceptor现有承诺前提下，可响应任意Accept请求。



Acceptor可以忽略任何请求而不用担心破坏算法的安全性。因此，我们这里要讨论什么时候Acceptor可以响应一个请求。我们对Acceptor批准提案给出如下约束：



***\*约束P1a：\****一个Acceptor只要尚未响应过任何编号大于M的Prepare请求，那么它就可以批准这个编号为M的提案。



可见，P1a包含了P1(一个Acceptor必须批准它收到的第一个提案)。



假设一个Acceptor收到一个编号为M的Prepare请求，在此之前它已经响应过编号大于M的Prepare请求。根据P1a，该Acceptor不可能再批准任何新的编号为M的提案，Acceptor也就没有必有对这个Prepare请求做出响应。因此，该Acceptor可以忽略编号为M的Prepare请求。



因此，每个Acceptor只需记住：它已批准提案的最大编号 + 它已响应Prepare请求的最大编号。这样即便Acceptor出现故障或者重启，也能保证满足P2c生成提案的规则。



而对于Proposer来说，只要它可以保证不会产生具有相同编号的提案，那么就可以丢弃任意的提案以及它所有的运行时状态信息。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDY9MAdMUlH1b5XT6IUMs27AjvtXicLXPPic1CGGmPs0z6qg0e2hNXkiaTaA/640?wx_fmt=png&from=appmsg)



**(6)Paxos算法描述**

结合Proposer和Acceptor对提案的处理逻辑；可以得出类似于两阶段提交的算法执行过程。

**
**

**阶段一：****(Prepare请求)**

一.Proposer选择一个提案编号M，然后向半数以上的Acceptor发送编号为M的Prepare请求。



二.如果一个Acceptor收到一个编号为M的Prepare请求，且M大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经批准过的编号最大的提案作为响应反馈给Proposer，同时该Acceptor承诺不再批准任何编号小于M的提案。

**
**

**阶段二：****(Accept请求)**

一.如果Proposer收到半数以上Acceptor，对其发出的编号为M的Prepare请求的响应，那么它就会发送一个针对[M, V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value。如果响应中不包含任何提案，那么V就由Proposer自己决定。



二.如果Acceptor收到一个针对[M, V]提案的Accept请求，只要该Acceptor没有对编号大于M的Prepare请求做出过响应，它就批准该提案。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYb8SzfOgfZeWwf1icNmeEOKHWOfFOgUF6AexSUYXybtCkHurcTYvZSNQ/640?wx_fmt=png&from=appmsg)



**(7)Learner学习被选定的value**

Learner获取提案，有三种方案：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYHqt1go2Wwu1KPzI2yTcMISTAvYKGCk7WfrOzseb4TmxT5fSFbEiax7w/640?wx_fmt=png&from=appmsg)



**(8)如何保证Paxos算法的活性**

一个极端的活锁场景：

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYJJaBSD0lElKz4vibOa5icpgPpUZRlSzT5A3uV8NeoibyibCIb4AApBSmsw/640?wx_fmt=png&from=appmsg)



**(9)总结**

二阶段提交协议解决了分布式事务的原子性问题，保证了分布式事务的多个参与者要么都执行成功，要么都执行失败。但是在二阶段解决部分分布式事务问题的同时，依然存在一些难以解决的诸如同步阻塞、无限期等待和脑裂等问题。



三阶段提交协议则是在二阶段提交协议的基础上，添加了PreCommit过程，从而避免了二阶段提交协议中的无限期等待问题。



Paxos算法引入过半的理念，也就是少数服从多数的原则。Paxos算法支持分布式节点角色之间的轮换，极大避免了分布式单点故障。因此Paxos算法既解决了无限期等待问题，也解决了脑裂问题。



整个Paxos算法就是想说明：每个Proposer生成的提案都去争取大多数Acceptor的批准。一旦有某个Proposer生成的提案[M0, V0]被大多数批准了，即便后面发现还有更多其他Proposer生成的提案[Mn, Vn]也被大多数Acceptor批准，那么这些提案的value值其实都是一样的，都为V0。因此就可以让每个Proposer都统一为一个value值为V0的提案，从而保证一致性。

**
**

**7.Paxos协议的核心思想**

**(1)Paxos协议的核心思想**

"与其预测未来，不如限制未来"，这应该是Paxos协议的核心思想。Paxos协议本身是比较简单的，如何将Paxos协议工程化才是真正的难题。

**
**

**(2)Paxos协议的基本概念**

Proposal Value：提议的值

Proposal Number：提议编号，编号不能冲突

Proposal：提议 = 提议的值 + 提议编号

Proposer：提议发起者

Acceptor：提议接受者

Learner：提议学习者



***\*说明一：\****协议中的Proposer有两个行为，一个是向Acceptor发Prepare请求，另一个是向Acceptor发Accept请求。



***\*说明二：\****协议中的Acceptor则会根据协议规则，对Proposer的请求作出应答。



***\*说明三：\****最后Learner可以根据Acceptor的状态，学习最终被确定的值。



为方便讨论，记[n, v]为提议编号为n、提议值为v的提议，记(m, [n, v])为承诺了Prepare(m)请求编号不再比m小，并接受过提议[n, v]。

**
**

**(3)Paxos协议过程**

**一.第一阶段A**

Proposer选择一个提议编号n，向所有的Acceptor广播Prepare(n)请求。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYiafu6miccqnobClMg7pOR6ibEHYhqOI82QZ1quy84wib9XLTrCf25S4xCQ/640?wx_fmt=png&from=appmsg)



**二.第一阶段B**

Acceptor接收到Prepare(n)请求，若提议编号n比之前接收的Prepare请求都要大，则返回承诺将不会接收提议编号比n小的提议，并且带上之前Accept的提议中编号小于n的最大的提议，否则不予理会。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYEL3AaYjHDEBozC8vqRxvwAa2u23k7mzv9YWCywReHnyom1Io4ObZjA/640?wx_fmt=png&from=appmsg)

**三.第二阶段A**

Proposer得到了多数Acceptor的承诺后，如果没有发现有一个Acceptor接受过一个值，那么向所有的Acceptor发起自己的值和提议编号n。否则，从所有接受过的值中选择对应的提议编号最大的，作为提议的值，此时提议编号仍然为n。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYGvbCsfDKEw7bB0zeWYp0lOMTvykcfOowpAfkfyhFicP4nnRShkd1SDQ/640?wx_fmt=png&from=appmsg)



**四.第二阶段B**

Acceptor接收到提议后，如果该提议编号不违反自己做过的承诺，则接受该提议。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYQmh26moNp5ZoQNFV6yrwum7XyGquu37O6M7icicmYSINIbwu7ITqeErg/640?wx_fmt=png&from=appmsg)



需要注意的是，Proposer发出Prepare(n)请求后，得到多数派的应答。然后可以随便再选择一个多数派广播Accept请求，这时不一定要将Accept请求发给有应答的Acceptor。

**
**

**五.协议过程总结**

上面的图例中：首先P1广播了Prepare请求，但是给A3的Prepare请求丢失了。不过A1、A2成功返回了，即该Prepare请求得到多数派的应答。然后P1可以广播Accept请求，但是给A1的Accept请求丢失了。不过A2、A3成功接受了这个提议，因为这个提议被多数派(A2、A3形成多数派)接受，所以我们称被多数派接受的提议对应的值被Chosen。



***\*情况一\**\**：\****如果三个Acceptor之前都没有接受过提议(即Accept请求)，那么在第一阶段B中，就不用返回接受过的提议。

**
**

***\*情况二：\****如果三个Acceptor之前接受过提议(即Accept请求)，那么就需要在第一阶段B中，带上之前Accept的提议中编号小于n的最大的提议值，进行返回。



如下图示：Proposer广播Prepare请求之后，收到了A1和A2的应答，应答中携带了它们之前接受过的[n1, v1]和[n2, v2]。Proposer则根据n1、n2的大小关系，选择较大的那个提议对应的值。比如n1 > n2，那么就选择v1作为提议值，最后向Acceptor广播提议[n, v1]。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYDfGOMXF54WIPyib2XCjnS0gGDNfqJLia8rfI6NbX6GnsfOtIpng5q7ZA/640?wx_fmt=png&from=appmsg)



**(4)Paxos协议最终解决什么问题**

当一个提议被多数派接受后，这个提议对应的值会被Chosen(选定)。一旦有一个值被Chosen(选定)，那么只要按照协议的规则继续交互。后续被Chosen的值都是同一个值，也就是保持了这个Chosen值的一致性。

**
**

**(5)Paxos协议证明**

上文就是基本Paxos协议的全部内容，其实是一个非常确定的数学问题。下面用数学语言表达，进而用严谨的数学语言加以证明。



一.Paxos原命题

如果一个提议[n0, v0]被大多数Acceptor接受，那么不存在提议[n1, v1]被大多数Acceptor接受，其中n0 < n1，v0 != v1。



二.Paxos原命题加强

如果一个提议[n0, v0]被大多数Acceptor接受，那么不存在Acceptor接受提议[n1, v1]，其中n0 < n1，v0 != v1。



三.Paxos原命题进一步加强

如果一个提议[n0, v0]被大多数Acceptor接受，那么不存在Proposer发出提议[n1, v1]，其中n0 < n1，v0 != v1。



如果"Paxos原命题进一步加强"成立，那么"Paxos原命题"显然成立。下面通过证明"Paxos原命题进一步加强"，从而证明"Paxos原命题"。



四.归纳法证明

假设提议[m, v](简称提议m)被多数派接受，那么提议m到n(如果存在)对应的值都为v，其中n不小于m(m <= n)。



这里对n进行归纳假设，当n = m时，结论显然成立。设n = k时结论成立，即如果提议[m, v]被多数派接受，那么提议m到k对应的值都为v，其中k不小于m(m <= k)。



当n = k + 1时，若提议k + 1不存在，那么结论成立。若提议k + 1存在，对应的值为v1。



因为提议m已经被多数派接受，又k + 1的Prepare被多数派承诺并返回结果。基于两个多数派必有交集，易知提议k + 1的第一阶段B有带提议回来。那么v1是从返回的提议中选出来的，不妨设这个值是选自提议[t, v1]。



根据第二阶段B，因为t是返回的提议中编号最大的，所以t >= m。又由第一阶段A，知道t < n，即t < k + 1，也就是m <= t < k + 1。所以根据假设可知，提议m到k对应的值都为v。所以再根据m <= t < k + 1，可得出t对应的值就为v，即有v1 = v。因此由假设的n = k结论成立，可以推出n = k + 1成立。



于是对于任意的提议编号不小于m的提议n，对应的值都为v，所以命题成立。



五.反证法证明

要证明的是：如果一个提议[n0, v0]被大多数Acceptor接受，那么不存在Proposer发出提议[n1, v1]，其中n0 < n1，v0 != v1。



假设存在，不妨设n1是满足条件的最小提议编号。

即存在提议[n1, v1]，其中n0 < n1，v0 != v1。-----------------------(A)

那么提议n0、n0 + 1、n0 + 2、...、n1 - 1对应的值为v0。-------------(B)



由于存在提议[n1, v1]，则说明大多数Acceptor已经接收n1的Prepare，并承诺将不会接受提议编号比n1小的提议。



又因为[n0, v0]被大多数Acceptor接受，所以存在一个Acceptor既对n1的Prepare进行了承诺，又接受了提议n0。



由协议的第二阶段B可知，这个Acceptor先接受了[n0, v0]。所以发出[n1, v1]提议的Proposer会从大多数的Acceptor返回中得知，至少某个编号不小于n0而且值为v0的提议已经被接受。----------(C)



由协议的第二阶段A知，该Proposer会从已经被接受的值中选择一个提议编号最大的值，作为提议的值。由(C)知可该提议编号不小于n0，由协议第二阶段B可知，该提议编号小于n1。于是由(B)知v1 == v0，与(A)矛盾。



所以命题成立。

**
**

**(6)为什么要被多数派接受**

因为两个多数派之间必有交集，所以Paxos协议一般是2F + 1个Acceptor。然后允许最多F个Acceptor停机，而保证协议依然能够正常进行，最终得到一个确定的值。

**
**

**(7)为什么需要做一个承诺**

可以保证第二阶段A中Proposer的选择不会受到未来变化的干扰。另外，对于一个Acceptor而言，这个承诺决定了：它回应提议编号较大的Prepare请求和接受提议编号较小的Accept请求的先后顺序。

**
**

**(8)为什么第二阶段A要从返回的提议中选择一个编号最大的**

这样选出来的提议编号一定不小于已经被多数派接受的提议编号，进而可以根据假设得到该提议编号对应的值是Chosen的那个值。

**
**

**(9)Paxos协议的学习过程**

如果一个提议被多数Acceptor接受，则这个提议对应的值被选定。一个简单直接的学习方法就是：获取所有Acceptor接受过的提议，然后看哪个提议被多数的Acceptor接受，那么该提议对应的值就是被选定的。



另外一个学习方法是：把Learner看作一个Proposer，根据协议流程，发起一个正常的提议，然后看这个提议是否被多数Acceptor接受。



注意：这里强调"一个提议被多数Acceptor接受"，而不是"一个值被多数Acceptor接受"。

![img](https://mmbiz.qpic.cn/sz_mmbiz_png/DXGTicJyJ8QBk82OPicib1ibrSGia40rhCJDYLGiauWD8LxTV3C5k7o0H533cNoRpclTM7YWAP9kAiagdqfIB9iaLZlARA/640?wx_fmt=png&from=appmsg)

上图中，提议[3, v3]，[5, v3]分别被B、C接受。虽然出现了v3被多数派接受，但不能说明v3被选定(Chosen)。只有提议[7, v1]被多数派(A和C组成)接受，才能说v1被选定，而这个选定的值随着协议继续进行不会改变。

**
**

**8****.ZAB算法简述**

zk的ZAB协议是Paxos协议的一个精简版。ZAB协议即Zookeeper Atomic Broadcast，zk原子广播协议。ZAB协议是用来保证zk各个节点之间数据的一致性的。



ZAB协议包括以下特色：

***\*特色一：\****Follower节点上全部的写请求都转发给Leader

***\*特色二：\****写操作严格有序

***\*特色三：\****使用改编的两阶段提交协议来保证各个节点的事务一致性，半数以上的参与者回复yes即可



广播模式：

广播模式就是指zk正常工作的模式。正常状况下，一个写入命令会通过以下步骤被执行：



***\*步骤一：\****Leader从客户端或者Follower那里收到一个写请求

***\*步骤二：\****Leader生成一个新的事务并为这个事务生成一个唯一的ZXID

***\*步骤三：\****Leader将这个事务以Proposal形式发送给全部的Follower节点

**步骤四：****Follower节点将收到的事务请求加入队列，并发送ACK给Leader**

***\*步骤五：\****当Leader收到大多数Follower的ACK消息，会发送Commit请求

***\*步骤六：\****当Follower收到Commit请求时，会判断该事务的ZXID是否是比队列中任何事务的ZXID都小来决定Commit



恢复模式：

当第一次启动集群时，先启动的过半机器中ZXID、myid最大的为Leader。当Leader故障时，zk集群进入恢复模式，此时zk集群不能对外提供服务。此时必须选出一个新的Leader完成数据一致后才能重新对外提供服务，zk官方宣称集群能够在200毫秒内选出一个新Leader。



正常模式下的几个步骤，每一个步骤都有可能由于Leader故障而中断，可是恢复过程只与Leader有没有Commit有关。



首先看前三个步骤，只做了一件事，把事务发送出去。若是事务没有发出去，全部Follower都没有收到这个事务，Leader故障了，全部的Follower都不知道这个事务的存在。



根据心跳检测机制，Follower发现Leader故障，需要重新选出一个Leader。此时会根据每一个节点ZXID来选择。谁的ZXID最大，表示谁的数据最新，就会被选举成新的Leader。若是ZXID都同样，那么就表示在Follower故障以前，全部的Follower节点数据一致，此时选择myid最大的节点成为新的Leader。



所以由于有一个固定的选举标准会加快选举流程，新的Leader选出来后，全部节点的数据同步一致后就能够对外提供服务。



假设新的Leader选出来以后，原来的Leader又恢复了，此时原来的Leader会自动成为Follower。



以前的事务即便发送给新的Leader，由于新的Leader已经开启了新的纪元，而原先的Leader中ZXID仍是旧的纪元，所以该事务就会被丢弃，而且该节点的ZXID也会更新成新的纪元。纪元就是标识当前Leader是第几任Leader，至关于改朝换代时候的年号。



若是在Leader故障以前已经Commit，zk会根据ZXID或者myid选出数据最新的那个Follower作为新的Leader。



新Leader会为Follower创建FIFO的队列，首先将自身有而Follower缺失的事务发送给该队列，然后再将这些事务的Commit命令发送给Follower，这样便保证了全部的Follower都保存了全部的事务数据。



ZAB协议确保那些已经在Leader提交的事务最终会被全部服务器提交，ZAB协议确保丢弃那些只在Leader提出或复制，但是没有提交的事务。

**
**

**9.****为什么****在分布式系统架构中需要使用zk集群**

当需要对分布式集群进行：集中式存储元数据、Master选举实现HA、分布式协调和通知时，可以自研一个类似zk的系统，也可以使用zk集群。

**
**

**(1)自研一个类似zk的系统**

如果是单机版本，只部署在一台机器上，里面提供了一些功能。虽然已实现存储一些元数据、支持Master选举、支持分布式协调和通知。但是对于单机版本的系统，万一挂掉了怎么办？



所以需要集群部署，通过多台机器保证高可用，即便挂掉一台机器，都可以继续运行下去。假设现在有3台机器，要进行元数据的存储。已经向机器1写了一条数据，那么机器1应该怎么把数据同步给其他的机器？所以自研一个类似的zk系统一旦集群部署后，数据一致性应该怎么保证？

**
**

**(2)使用zk集群**

久经考验的zk，bug很少且功能全面，已用在很多工业级的分布式系统中，所以直接使用zk集群即可。

**
**

**10.zk分布式系统具有哪些特点**

**(1)集群化部署**

3~5台机器组成一个集群，每台机器都在内存保存zk的全部数据。机器间互相通信同步数据，客户端可连接任何一台机器。

**
**

**(2)树形结构的数据模型**

znode的数据结构跟文件系统类似，是有层级关系的树形数据结构。znode的数据结构是树形结构，纯内存保存，znode可以认为是一个节点而已。

**
**

**(3)顺序写**

集群中只有一台机器可以写，所有机器都可以读。所有写请求都会分配一个zk集群全局的唯一递增编号ZXID，ZXID用于保证客户端发起的写请求都是有顺序的。

**
**

**(4)数据一致性**

任何一台zk机器收到了写请求后都会同步给其他机器，保证数据一致。客户端连接到任何一台zk机器看到的数据都是一致的。

**
**

**(5)高性能**

每台zk机器都在内存维护数据，所以zk集群绝对是高并发高性能的。如果让zk部署在高配置机器上，3台机器的zk集群能抗下每秒几万请求。

**
**

**(6)高可用**

哪怕集群中挂掉不超过一半的机器，都能保证集群可用，数据不会丢失。3台机器可以挂1台，5台机器可以挂2台。

**
**

**(7)高并发**

高并发是由高性能决定的，只要基于纯内存数据结构来处理，那么并发能力是很高的。使用高配置的物理机器进行写，比如1台16核32G可以支持几万QPS，3台16核32G可以支持十几万QPS。

**
**

**11.zk集群机器的三种角色**

通常来说zk集群里有三种角色的机器，分别是Leader、Follower、Observer。集群启动后会自动选举一个Leader出来，只有Leader可以写，Follower只能同步数据和提供数据的读取。如果Leader挂了，那么Follower会继续选举出新的Leader。Observer只能读，而且Observer不参与选举。

**
**

**12.客户端与zk之间的长连接和会话**

zk集群启动后，集群中的各个节点会自己分配好角色。之后客户端跟zk集群建立的连接，是TCP长连接。也就建立了一个会话Session，会通过心跳来感知会话是否存在。

**
**

**13.zk的数据模型znode和节点类型**

zk的核心数据模型就是znode树，往zk写数据就是创建树形结构的znode，里面可以写入值，存放在zk的内存中。



有两种节点：持久节点和临时节点。持久节点，就是哪怕客户端断开连接，也一直存在。临时节点，就是只要客户端断开连接，节点就没了。



顺序节点，就是创建节点时自增加全局递增的编号。Curator中关于zk分布式锁的实现就是基于zk的临时顺序节点来实现的，加锁的时候是创建一个临时顺序节点。zk会自动给临时节点加上一个后缀，也就是全局递增的一个编号。如果客户端断开连接，就自动销毁这个客户端加的锁，此时其他客户端就会感知到而尝试去加锁。



如果进行元数据存储，则需要使用持久节点。如果进行分布式协调和通知，则通常使用临时节点。如果实现分布式锁，则通常使用临时顺序节点。

**
**

**14.zk最核心的Watcher监听回调机制**

zk最核心的机制是：一个客户端可以对znode进行Watcher监听，当znode发生改变时zk会回调该客户端进行通知。



这是非常有用的一个功能，在分布式系统的协调中是非常有必要的。如果zk只支持写和读，那只能实现元数据存储、Master选举和部分功能，对于分布式系统的协调需求就没办法实现了。如系统A监听一个数据的变化，如果系统B更新了该数据，zk需要能通知系统A该数据的变化。



通过对zk内存数据模型(不同节点类型)进行这两种操作：写数据和读数据、监听数据变化(更新数据时反向通知数据变化)，就可以实现：存储集群元数据、分布式锁、Master选举、分布式协调监听等功能。

**
**

**15.ZAB协议的主从同步机制和崩溃恢复机制**

ZAB协议使用的是主从架构，需要划分集群角色，有Leader和Follower两种角色。其中Leader和Follower都可以处理读请求，但只有Leader可以处理写请求。



Leader收到事务请求后，会往本地磁盘日志文件写数据，然后转换为Proposal提议并同步给所有的Follower。Follower收到Leader的Proposal提议后，也会往本地磁盘日志文件写数据。



当Leader发现超过半数Follower都收到Proposal提议时，Leader会给所有Follower发送Commit消息提交事务写数据到内存。



如果Leader崩溃了，Follower会重新选举新的Leader保证服务运行。



所以ZAB协议涉及：角色划分、2PC(两阶段)、过半写机制。

**
**

**16.ZAB协议流程之集群启动-数据同步-崩溃恢复**

**(1)zk集群启动时会进入数据恢复模式**

集群启动时会选举一个Leader。只要有过半机器认可某台机器是Leader，那么该机器就可以被选举为Leader。



Leader选举出来后，Leader会等待集群中过半Follower与它进行数据同步。只要过半Follower完成数据同步，集群就会退出恢复模式，就可以对外提供服务。当然还没完成数据同步的Follower会继续与Leader进行数据同步。

**
**

**(2)zk启动完后会进入消息广播模式**

客户端既可以连接Leader，也可以连接Follower，但要注意只有Leader可以处理写请求。



如果客户端发送了一个写请求给Follower，那么Follower会把写请求转发给Leader。Leader收到写请求后，会把写请求以Proposal提议的形式同步给所有Follower。过半Follower都收到Proposal后，Leader再发送Commit消息让Follower提交写请求。

**
**

**(3)Leader宕机时会进入数据恢复模式**

当Leader宕机时，Follower会重新选举一个Leader。只要过半Follower都承认一个Follower成为Leader，那么就可以完成选举。



所以在zk集群中，只要宕机的机器数小于一半，那么集群就还可以正常工作。因为还有过半机器存活下来进行重新选举，此时还可以重新选举出新的Leader。新的Leader选举出来后再重新等待过半Follower跟它进行数据同步，过半Follower完成数据同步后集群就会重新进入消息广播模式。

**
**

**(4)总结**

一.集群启动时的数据恢复模式

Leader选举(过半机器选举机制) + (剩余机器)进行数据同步。

二.消息写入时的消息广播模式

Leader采用2PC模式的过半写机制，来给Follower进行同步。

三.Leader宕机时的数据恢复模式

Leader宕机时，只要剩余存活机器超过一半，那么就还可以选举出新的Leader。选举出新的Leader后，Follower会重新进行数据同步。

**
**

**17.采用2PC两阶段提交思想的ZAB消息广播流程**

对每一条消息进行广播时，都是通过2PC实现的。首先Leader广播Proposal提议，然后各个Follower返回ACK响应，Leader收到过半Follower的ACK响应后再广播Commit消息让Follower进行提交。

**
**

**(1)Leader发起一个事务Proposal之前**

Leader会分配一个全局唯一递增的ZXID来严格保证事务处理的顺序，而且Leader会为每个Follower创建一个FIFO队列。队列里会顺序放入发送给Follower的Proposal，从而保证事务处理的顺序。

**
**

**(2)每个Follower收到一个事务Proposal后**

Follower会立即写入本地磁盘日志，写入成功后就可以保证数据不丢失。然后Follower会返回一个ACK给Leader，当过半Follower都返回ACK时，Leader就会发送Commit消息给全部Follower。

**
**

**(3)Leader自己也会进行Commit操作**

Leader和Follower进行Commit之后，就意味这个数据可以被读取到了。

**
**

**18.zk到底是强一致性还是最终一致性**

**(1)强一致性**

只要写入一条数据，无论从zk哪台机器上都可以马上读取到这条数据。强一致性的写入操作卡住时，直到Leader和全部Follower都进行了Commit，才能让写入操作返回，才能认为写入成功。所以只要写入成功，无论从哪个zk机器查询都能查到，这就是强一致性。很明显，ZAB协议机制下的zk不是强一致性。

**
**

**(2)最终一致性**

写入一条数据，方法返回写入成功。此时马上去其他zk机器上查有可能是查不到的，可能会出现短暂时间的数据不一致。但是过一会儿，一定会让其他机器同步到这条数据，最终一定可以查到写入的数据。



在ZAB协议中：当过半Follower对Proposal提议返回ACK时，Leader就会发送Commit消息给所有Follower。只要Follower或者Leader进行了Commit，那么这个数据就会被客户端读取到。



那么就有可能出现：有的Follower已经Commit了，但有的Follower还没有Commit。某个客户端连接到某个Follower时可以读取到刚刚Commit的数据，但有的客户端连接到另一个Follower时却没法读取到还没有Commit的数据。



所以zk不是强一致性的。Leader不会保证一条数据被全部Follower都Commit后，才让客户端读取到。在Commit的过程中，可能会出现在不同的Follower上读取到的数据是不一致的情况。但完成Commit后，最终一定会让客户端读取到一致的数据。

**
**

**(3)顺序一致性**

zk官方给自己的定义是顺序一致性，当然也属于最终一致性。但zk的顺序一致性比最终一致性更好一点，因为Leader会保证所有的Proposal提议同步到Follower时是按顺序进行同步的。



如果要求zk是强一致性，那么可以手动调用zk的sync()方法。

**
**

**19.ZAB协议下两种可能存在的数据不一致问题**

**(1)Leader还没发送Commit消息就崩溃**

Leader收到了过半的Follower的ACK，接着Leader自己Commit了，但还没来得及发送Commit消息给所有Follower自己却宕机了。此时相当于Leader的数据跟所有Follower是不一致的，所以得保证全部Follower最终都完成Commit。



因此Leader崩溃后，就会选举一个拥有最大ZXID的Follower作为Leader，这个Leader会检查事务日志。如果发现自己事务日志里有一个还没进行提交的Proposal提议，那么旧说明旧Leader没来得及发送Commit消息就崩溃了。此时它作为新Leader会为这个Proposal提议向Follower发送Commit消息，从而保证旧Leader提交的事务最终可以被提交到所有Follower中。



同理，如果Leader收到过半Follower的ACK响应后，在发送Commit消息的过程中，出现Leader宕机或者Leader和Follower的网络问题。那么Follower只要与它能找到的Leader(新选举的ZXID最大的Follower或者原来的Leader)进行数据同步，就可以保证新数据在该Follower中完成Commit。

**
**

**(2)Leader还没广播Proposal提议就崩溃**

Leader没来得及发送Proposal提议给所有Follower的时候就宕机了，此时Leader上的这个请求应该是要被丢弃的。对于这种情况：如果旧Leader日志里有一个事务Proposal提议，它重启后跟新Leader同步，发现这个事务Proposal提议其实是不应该存在的，那么就会直接丢弃。

**
**

**20.崩溃恢复时新Leader和Follower的数据同步**

**(1)先发送Proposal提议再发送Commit消息**

新选举出一个Leader后，其他Follower就会跟它进行数据同步。Leader会给每个Follower准备一个队列，然后把所有的Proposal提议都发送给Follower，并紧接着会发送Commit消息给那个Follower。

**
**

**(2)Commit操作会把数据写入到内存中的znode**

Commit操作就是把这条数据加入内存中的znode树形数据结构里，这样客户端就能访问到该数据，当然也会去通知监听这个znode的客户端。而收到Proposal提议时，会将数据以事务日志的形式顺序写入到磁盘。

**
**

**(3)通过已同步Follower列表判断数据可用**

如果一个Follower跟新Leader完成数据同步后，就会加入新Leader的已同步Follower列表中。当这个已同步Follower列表中有过半的Follower时，那么新Leader就可以对外继续提供服务了。

**
**

**(4)选择ZXID最大的Follower作为新Leader的原因**

在选举新Leader时，会挑选拥有最大ZXID的那个Follower作为新Leader。比如5个机器，1Leader + 4个Follower。1个Leader把Proposal发送给4个Follower，其中3个Follower(过半)都收到了Proposal返回ACK了，但是第四个Follower没收到Proposal。此时Leader执行Commit后挂了，Commit消息没法发送给其他Follower。这时剩余4个Follower，只要其中3个投票某一个当Leader，那它就可成为Leader。假设收到Proposal的那3个Follower都投票给没收到Proposal的Follower，那么这条数据就会永久丢失，所以需要选择一个拥有最大ZXID的Follower作为新Leader。

**
**

**21.ZAB协议会如何处理需要丢弃的消息的**

**(1)ZAB协议根据epoch处理需要丢弃的消息**

每一个事务的ZXID是64位的，高32位是Leader的epoch(表示选举轮次)，低32位是自增长的序列号。



如果一个Leader刚把一个Proposal提议写入本地磁盘日志，还没来得及广播Proposal提议给全部Follower就崩溃了。那么当新Leader选举出来后，事务的epoch会自增长一位。然后当旧Leader重启后重新加入集群成为Follower时，会发现自己比新Leader多出一条Proposal提议，但该Proposal提议的epoch比新Leader的epoch低，所以会丢弃这条数据。

**
**

**(2)ZAB协议的简单总结**

一.启动时：过半机器选举Leader + 数据同步

二.对外提供服务时：2PC + 过半写机制，顺序一致性(最终一致性)

三.崩溃恢复时：重新选举Leader + 针对两种数据不一致的情况进行处理

**
**

**22.zk的Observer节点的作用**

Observer节点不参与Leader选举，不参与广播提议时过半Follower进行ACK的环节，只处理客户端读请求和同步数据。

**
**

**(1)对于写请求**

由于Leader在进行数据同步时，Observer不会参与到过半写机制里。所以zk集群无论多少台机器，只能是由一个Leader进行写。Leader单机写入最多每秒上万QPS，这是没法扩展的。



因此zk是适合写少的场景。Redis的写QPS可以达10万，zk的写QPS才上万。虽然Redis和zk都是内存级的，而且对写请求的处理都是单线程处理的。但是由于Redis没有过半写机制，所以它的写性能更高。

**
**

**(2)对于读请求**

Follower通常会有2个或4个，这样处理读请求时就可以达到每秒几万QPS。如果引入更多的Follower，那么在过半写机制中会有更多的Follower参与。这会影响Leader写的效率，因此就有了Observer。



所以为了进一步提升zk集群对读请求的处理能力，可以引入Observer节点。由于它只同步数据提供读服务，所以可以无限扩展节点而不影响写效率。

**
**

**23.zk适合小集群部署 + 读多写少场景的原因**

**(1)zk集群通常采用三五台机器小集群部署**

假设有1个Leader + 20个Follower，总共21台机器。此时由于Follower要参与到ZAB的写请求的过半ACK，所以一个写请求要等至少10个Follower返回ACK才能发送Commit消息。Leader发送Commit消息后才能告诉客户端写请求成功，所以性能会比较差。所以ZAB协议决定了zk集群通常采用1个Leader + 2个Follower的小集群。

**
**

**(2)zk集群适合读多写少的场景**

zk集群的写请求的处理能力是无法扩展的。如果读请求量大，可以加Observer机器。因此zk只适合读多写少的场景，所以zk主要是用来处理分布式系统的一些协调工作。

**
**

**24.zk特性的总结**

**(1)集群模式部署**

一般奇数节点部署，不能超过一半的机器挂掉。因为5台机器可以挂2台，6台机器也最多只能挂2台。所以5台和6台效果一致，所以奇数节点可以减少机器开销。而且zk集群是小集群部署，适用于读多写少的场景。

**
**

**(2)主从架构**

Leader、Follower、Observer。

**
**

**(3)内存数据模型**

znode，多种节点类型，支持Watcher机制实现监听回调通知。

**
**

**(4)顺序一致性**

消息按顺序同步，但是最终才会一致，不是强一致。ZXID的高32位epoch，低32位是自增长的序列号。

**
**

**(5)高性能**

2PC中的过半写机制，纯内存的数据结构，znode。ZAB协议：2PC、过半ACK + 写事务日志，Commit + 写内存数据。

**
**

**(6)高可用**

Follower宕机没影响，Leader宕机有数据不一致问题。新选举的Leader会自动处理，正常运行。但是在恢复模式期间，可能有一小段时间是没法写入zk的。客户端跟zk进行TCP长连接，通过心跳机制维持Session。

**
**

**(7)高并发**

单机Leader写，Observer节点可以线性扩展读QPS。
